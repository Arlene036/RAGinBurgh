{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_file_name(url):\n",
    "    return 'raw_documents/'+'-'.join(url.split(\"/\")[2:]).replace('.','-') + \".json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "\n",
    "def scrape_wikipedia_to_json(url, output_file):\n",
    "    # Send request to the URL\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    # Parse the page content\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    # Find the correct div based on the updated structure\n",
    "    content = soup.find('div', {'id': 'mw-content-text'})\n",
    "\n",
    "    # Dictionary to store the result\n",
    "    data = {}\n",
    "    current_h1 = None\n",
    "    current_h2 = None\n",
    "    current_h3 = None\n",
    "    current_h4 = None\n",
    "    current_h5 = None\n",
    "    current_h6 = None\n",
    "\n",
    "    # Iterate over headings and paragraphs\n",
    "    for tag in content.find_all(['h2', 'h3', 'h4', 'h5', 'h6', 'p']):\n",
    "        if tag.name == 'h2':\n",
    "            current_h2 = tag.get_text(strip=True)\n",
    "            data[current_h2] = {}\n",
    "            current_h3 = current_h4 = current_h5 = current_h6 = None\n",
    "        elif tag.name == 'h3' and current_h2:\n",
    "            current_h3 = tag.get_text(strip=True)\n",
    "            data[current_h2][current_h3] = {}\n",
    "            current_h4 = current_h5 = current_h6 = None\n",
    "        elif tag.name == 'h4' and current_h3:\n",
    "            current_h4 = tag.get_text(strip=True)\n",
    "            data[current_h2][current_h3][current_h4] = {}\n",
    "            current_h5 = current_h6 = None\n",
    "        elif tag.name == 'h5' and current_h4:\n",
    "            current_h5 = tag.get_text(strip=True)\n",
    "            data[current_h2][current_h3][current_h4][current_h5] = {}\n",
    "            current_h6 = None\n",
    "        elif tag.name == 'h6' and current_h5:\n",
    "            current_h6 = tag.get_text(strip=True)\n",
    "            data[current_h2][current_h3][current_h4][current_h5][current_h6] = {}\n",
    "        elif tag.name == 'p':\n",
    "            if current_h6:\n",
    "                data[current_h2][current_h3][current_h4][current_h5][current_h6].setdefault('content', []).append(tag.get_text(strip=True))\n",
    "            elif current_h5:\n",
    "                data[current_h2][current_h3][current_h4][current_h5].setdefault('content', []).append(tag.get_text(strip=True))\n",
    "            elif current_h4:\n",
    "                data[current_h2][current_h3][current_h4].setdefault('content', []).append(tag.get_text(strip=True))\n",
    "            elif current_h3:\n",
    "                data[current_h2][current_h3].setdefault('content', []).append(tag.get_text(strip=True))\n",
    "            elif current_h2:\n",
    "                data[current_h2].setdefault('content', []).append(tag.get_text(strip=True))\n",
    "    \n",
    "    # Save the dictionary as a JSON file\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=4)\n",
    "    \n",
    "    print(f\"Content successfully written to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Content successfully written to Pittsburgh_Wikipedia.json\n",
      "Content successfully written to History_of_Pittsburgh_Wikipedia.json\n",
      "Content successfully written to Carnegie_Mellon_University_Wikipedia.json\n"
     ]
    }
   ],
   "source": [
    "url_list = [\"https://en.wikipedia.org/wiki/Pittsburgh\", \"https://en.wikipedia.org/wiki/History_of_Pittsburgh\",\n",
    "\"https://en.wikipedia.org/wiki/Carnegie_Mellon_University\"]\n",
    "output_file_list = [\"Pittsburgh_Wikipedia.json\", \"History_of_Pittsburgh_Wikipedia.json\", \"Carnegie_Mellon_University_Wikipedia.json\"]\n",
    "for url, output_file in zip(url_list, output_file_list):\n",
    "    scrape_wikipedia_to_json(url, output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ---- scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bs4.element.Tag"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = soup.find('div', {'id': 'bodyContent'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tag in content.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'p']):\n",
    "    print(tag.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to store the result\n",
    "data = {}\n",
    "current_h1 = None\n",
    "current_h2 = None\n",
    "current_h3 = None\n",
    "current_h4 = None\n",
    "current_h5 = None\n",
    "current_h6 = None\n",
    "\n",
    "# Iterate over headings and paragraphs\n",
    "for tag in content.find_all(['h2', 'h3', 'h4', 'h5', 'h6', 'p']):\n",
    "    if tag.name == 'h2':\n",
    "        current_h2 = tag.get_text(strip=True)\n",
    "        data[current_h2] = {}\n",
    "        current_h3 = current_h4 = current_h5 = current_h6 = None\n",
    "    elif tag.name == 'h3' and current_h2:\n",
    "        current_h3 = tag.get_text(strip=True)\n",
    "        data[current_h2][current_h3] = {}\n",
    "        current_h4 = current_h5 = current_h6 = None\n",
    "    elif tag.name == 'h4' and current_h3:\n",
    "        current_h4 = tag.get_text(strip=True)\n",
    "        data[current_h2][current_h3][current_h4] = {}\n",
    "        current_h5 = current_h6 = None\n",
    "    elif tag.name == 'h5' and current_h4:\n",
    "        current_h5 = tag.get_text(strip=True)\n",
    "        data[current_h2][current_h3][current_h4][current_h5] = {}\n",
    "        current_h6 = None\n",
    "    elif tag.name == 'h6' and current_h5:\n",
    "        current_h6 = tag.get_text(strip=True)\n",
    "        data[current_h2][current_h3][current_h4][current_h5][current_h6] = {}\n",
    "    elif tag.name == 'p':\n",
    "        if current_h6:\n",
    "            data[current_h2][current_h3][current_h4][current_h5][current_h6].setdefault('content', []).append(tag.get_text(strip=True))\n",
    "        elif current_h5:\n",
    "            data[current_h2][current_h3][current_h4][current_h5].setdefault('content', []).append(tag.get_text(strip=True))\n",
    "        elif current_h4:\n",
    "            data[current_h2][current_h3][current_h4].setdefault('content', []).append(tag.get_text(strip=True))\n",
    "        elif current_h3:\n",
    "            data[current_h2][current_h3].setdefault('content', []).append(tag.get_text(strip=True))\n",
    "        elif current_h2:\n",
    "            data[current_h2].setdefault('content', []).append(tag.get_text(strip=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pittsburghpa.gov"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get subpages' url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def get_subpage_links(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    # Find all <a> tags\n",
    "    links = []\n",
    "    for a_tag in soup.find_all('a', href=True):\n",
    "        link = a_tag['href']\n",
    "        if link.startswith('/'):  # If it's a relative link, prepend the base URL\n",
    "            link = f\"{url.rstrip('/')}{link}\"\n",
    "        links.append(link)\n",
    "    \n",
    "    return links\n",
    "\n",
    "# Example usage\n",
    "url = \"https://pittsburghpa.gov/index.html\"  # Replace with the base URL\n",
    "subpage_links = get_subpage_links(url)\n",
    "print(f\"Found {len(subpage_links)} subpage links:\")\n",
    "for link in subpage_links:\n",
    "    print(link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get('https://pittsburghpa.gov/finance/tax-forms')\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "div = soup.find('div', class_='opened-for-codepen')\n",
    "\n",
    "gov_finance_urls = []\n",
    "if div:\n",
    "    links = div.find_all('a')\n",
    "    for link in links:\n",
    "        gov_finance_urls.append(link['href'])\n",
    "else:\n",
    "    print(\"未找到class='opened-for-codepen'的div标签\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get('https://pittsburghpa.gov/events/index.html')\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "div = soup.find('div', class_ = 'accordion')\n",
    "\n",
    "gov_events_urls = []\n",
    "if div:\n",
    "    links = div.find_all('a')\n",
    "    for link in links:\n",
    "        gov_events_urls.append(link['href'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://pittsburghpa.gov/city-info/press-releases',\n",
       " 'https://pittsburghpa.gov/city-info/socialmedia',\n",
       " 'https://pittsburghpa.gov/city-info/frequent-numbers',\n",
       " 'https://pittsburghpa.gov/city-info/executive-orders',\n",
       " 'https://pittsburghpa.gov/city-info/policies']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = requests.get('https://pittsburghpa.gov/city-info/policies')\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "div = soup.find('div', class_ = 'accordion')\n",
    "\n",
    "gov_policy_urls = []\n",
    "if div:\n",
    "    links = div.find_all('a')\n",
    "    for link in links:\n",
    "        gov_policy_urls.append(link['href'])\n",
    "gov_policy_urls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### build documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "\n",
    "def scrape_gov_to_json(url, output_file):\n",
    "    # Send request to the URL\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    # Parse the page content\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    # Find the correct div based on the updated structure\n",
    "    content = soup.find('div', {'class': 'col-md-12'})\n",
    "\n",
    "    # Dictionary to store the result\n",
    "    data = {}\n",
    "    for passage in content.find_all('p'):\n",
    "        data.setdefault('content', []).append(passage.get_text(strip=True))\n",
    "    \n",
    "    # Save the dictionary as a JSON file\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=4)\n",
    "    \n",
    "    print(f\"Content successfully written to {output_file}\")\n",
    "\n",
    "def gov_url_list_to_docuemnts(url_list):\n",
    "    for url in url_list:\n",
    "        if url.endswith('pdf'):\n",
    "            print(f\"Skipping PDF file: {url}\")\n",
    "            continue\n",
    "        try:\n",
    "            output_file = build_file_name(url)\n",
    "            scrape_gov_to_json(url, output_file)\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to scrape {url}: {e}\")\n",
    "            continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://pittsburghpa.gov/pittsburgh/pgh-about\n",
    "gov_urls = [\"https://pittsburghpa.gov/pittsburgh/pgh-about\", \"https://pittsburghpa.gov/pittsburgh/pgh-sports\", \n",
    "\"https://pittsburghpa.gov/pittsburgh/cultural-activities\", \"https://pittsburghpa.gov/pittsburgh/flag-seal\", \n",
    "\"https://pittsburghpa.gov/mayor/pghmayors\"]\n",
    "\n",
    "gov_url_list_to_docuemnts(gov_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gov_url_list_to_docuemnts(gov_finance_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gov_url_list_to_docuemnts(gov_events_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Content successfully written to raw_documents/pittsburghpa-gov-city-info-press-releases.json\n",
      "Content successfully written to raw_documents/pittsburghpa-gov-city-info-socialmedia.json\n",
      "Content successfully written to raw_documents/pittsburghpa-gov-city-info-frequent-numbers.json\n",
      "Content successfully written to raw_documents/pittsburghpa-gov-city-info-executive-orders.json\n",
      "Content successfully written to raw_documents/pittsburghpa-gov-city-info-policies.json\n"
     ]
    }
   ],
   "source": [
    "gov_url_list_to_docuemnts(gov_policy_urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Page:1>\n",
      "<Page:2>\n",
      "<Page:3>\n",
      "<Page:4>\n",
      "<Page:5>\n",
      "<Page:6>\n",
      "<Page:7>\n",
      "<Page:8>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pdfplumber\n",
    "\n",
    "# 1. 通过 URL 下载 PDF\n",
    "def download_pdf(url, output_path):\n",
    "    response = requests.get(url)\n",
    "    with open(output_path, 'wb') as file:\n",
    "        file.write(response.content)\n",
    "\n",
    "# 2. 将 PDF 转换为文字\n",
    "def pdf_to_text(pdf_path):\n",
    "    text = ''\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        for page in pdf.pages:\n",
    "            print(page)\n",
    "            text += page.extract_text()\n",
    "    return text\n",
    "\n",
    "# 示例使用\n",
    "url = \"https://apps.pittsburghpa.gov/redtail/images/25073_SummerGuide-2024-WEB.pdf\"\n",
    "pdf_path = \"raw_documents/gov_summer_event_guide.pdf\"\n",
    "\n",
    "# 下载 PDF\n",
    "download_pdf(url, pdf_path)\n",
    "\n",
    "# PDF 转文字\n",
    "pdf_text = pdf_to_text(pdf_path)\n",
    "print(pdf_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting pymupdf\n",
      "  Downloading PyMuPDF-1.24.11-cp38-abi3-macosx_11_0_arm64.whl (18.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 18.2 MB 2.5 MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: pymupdf\n",
      "Successfully installed pymupdf-1.24.11\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 24.2 is available.\n",
      "You should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pymupdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import fitz  # PyMuPDF\n",
    "\n",
    "def pdf_to_text_pymupdf(pdf_path):\n",
    "    text = ''\n",
    "    pdf_document = fitz.open(pdf_path)\n",
    "    for page_num in range(pdf_document.page_count):\n",
    "        page = pdf_document.load_page(page_num)\n",
    "        text += page.get_text(\"text\")  # 使用 \"text\" 模式提取纯文本\n",
    "    return text\n",
    "\n",
    "pdf_text = pdf_to_text_pymupdf(pdf_path)\n",
    "print(pdf_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## britannica.com/place/Pittsburgh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.britannica.com/place/Pittsburgh\n",
    "\n",
    "response = requests.get('https://www.britannica.com/place/Pittsburgh')\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "content = soup.find('div', class_ = 'reading-channel')\n",
    "\n",
    "data = {}\n",
    "for passage in content.find_all('p'):\n",
    "    data.setdefault('content', []).append(passage.get_text(strip=True))\n",
    "\n",
    "with open('raw_documents/Pittsburgh_Britannica.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(data, f, ensure_ascii=False, indent=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## visitPittsburgh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.visitpittsburgh.com/events-festivals/annual-events/\n",
    "# 从class_='content--primary'中提取subpages\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "visit_pitt_event_url = 'https://www.visitpittsburgh.com/events-festivals/annual-events/'\n",
    "response = requests.get(visit_pitt_event_url)\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "# Find all <a> tags\n",
    "visit_even_links = []\n",
    "div = soup.find('main', class_='content--primary')\n",
    "for a_tag in div.find_all('a', href=True):\n",
    "    link = a_tag['href']\n",
    "    if link.startswith('/'):  # If it's a relative link, prepend the base URL\n",
    "        link = f\"{url.rstrip('/')}{link}\"\n",
    "    if len(link.split('/')) > 4:\n",
    "        visit_even_links.append(link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_visit_to_json(url, output_file):\n",
    "    # Send request to the URL\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    # Parse the page content\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    # Find the correct div based on the updated structure\n",
    "    content = soup.find('main', {'class': 'content--primary'})\n",
    "\n",
    "    # Dictionary to store the result\n",
    "    data = {}\n",
    "    current_h1 = None\n",
    "    current_h2 = None\n",
    "    current_h3 = None\n",
    "    current_h4 = None\n",
    "    current_h5 = None\n",
    "    current_h6 = None\n",
    "\n",
    "    # Iterate over headings and paragraphs\n",
    "    for tag in content.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'p']):\n",
    "        if tag.name == 'h1':\n",
    "            current_h1 = tag.get_text(strip=True)\n",
    "            data[current_h1] = {}\n",
    "            current_h2 = current_h3 = current_h4 = current_h5 = current_h6 = None\n",
    "        elif tag.name == 'h2' and current_h1:\n",
    "            current_h2 = tag.get_text(strip=True)\n",
    "            data[current_h1][current_h2] = {}\n",
    "            current_h3 = current_h4 = current_h5 = current_h6 = None\n",
    "        elif tag.name == 'h3' and current_h2:\n",
    "            current_h3 = tag.get_text(strip=True)\n",
    "            data[current_h1][current_h2][current_h3] = {}\n",
    "            current_h4 = current_h5 = current_h6 = None\n",
    "        elif tag.name == 'h4' and current_h3:\n",
    "            current_h4 = tag.get_text(strip=True)\n",
    "            data[current_h1][current_h2][current_h3][current_h4] = {}\n",
    "            current_h5 = current_h6 = None\n",
    "        elif tag.name == 'h5' and current_h4:\n",
    "            current_h5 = tag.get_text(strip=True)\n",
    "            data[current_h1][current_h2][current_h3][current_h4][current_h5] = {}\n",
    "            current_h6 = None\n",
    "        elif tag.name == 'h6' and current_h5:\n",
    "            current_h6 = tag.get_text(strip=True)\n",
    "            data[current_h1][current_h2][current_h3][current_h4][current_h5][current_h6] = {}\n",
    "        elif tag.name == 'p':\n",
    "            if current_h6:\n",
    "                data[current_h1][current_h2][current_h3][current_h4][current_h5][current_h6].setdefault('content', []).append(tag.get_text(strip=True))\n",
    "            elif current_h5:\n",
    "                data[current_h1][current_h2][current_h3][current_h4][current_h5].setdefault('content', []).append(tag.get_text(strip=True))\n",
    "            elif current_h4:\n",
    "                data[current_h1][current_h2][current_h3][current_h4].setdefault('content', []).append(tag.get_text(strip=True))\n",
    "            elif current_h3:\n",
    "                data[current_h1][current_h2][current_h3].setdefault('content', []).append(tag.get_text(strip=True))\n",
    "            elif current_h2:\n",
    "                data[current_h1][current_h2].setdefault('content', []).append(tag.get_text(strip=True))\n",
    "            elif current_h1:\n",
    "                data[current_h1].setdefault('content', []).append(tag.get_text(strip=True))\n",
    "\n",
    "    \n",
    "    # Save the dictionary as a JSON file\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=4)\n",
    "    \n",
    "    print(f\"Content successfully written to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visit_pitt_event_url_to_docuemnts(url_list):\n",
    "    for url in url_list:\n",
    "        if url.endswith('pdf'):\n",
    "            print(f\"Skipping PDF file: {url}\")\n",
    "            continue\n",
    "        try:\n",
    "            output_file = build_file_name(url)\n",
    "            scrape_visit_to_json(url, output_file)\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to scrape {url}: {e}\")\n",
    "            continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visit_pitt_event_url_to_docuemnts(visit_even_links)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "visit_pitt_sports_url = 'https://www.visitpittsburgh.com/things-to-do/pittsburgh-sports-teams/'\n",
    "response = requests.get(visit_pitt_sports_url)\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "# Find all <a> tags\n",
    "visit_sports_links = []\n",
    "div = soup.find('main', class_='content--primary')\n",
    "for a_tag in div.find_all('a', href=True):\n",
    "    link = a_tag['href']\n",
    "    if link.startswith('/'):  # If it's a relative link, prepend the base URL\n",
    "        link = f\"{url.rstrip('/')}{link}\"\n",
    "    if len(link.split('/')) > 4:\n",
    "        visit_sports_links.append(link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "visit_sports_links.extend(['https://www.visitpittsburgh.com/things-to-do/pittsburgh-sports-teams/'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "61"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(visit_sports_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_visit_sport_to_json(url, output_file):\n",
    "    # Send request to the URL\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    # Parse the page content\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    # Find the correct div based on the updated structure\n",
    "    if 'blog' in url:\n",
    "        content = soup.find('article',class_= 'detail__inner')\n",
    "    elif 'directory' in url:\n",
    "        content = soup.find('div', class_='detail__primary-inner row')\n",
    "    else:\n",
    "        content = soup.find('main', {'class': 'content--primary'})\n",
    "\n",
    "    # Dictionary to store the result\n",
    "    data = {}\n",
    "    current_h1 = None\n",
    "    current_h2 = None\n",
    "    current_h3 = None\n",
    "    current_h4 = None\n",
    "    current_h5 = None\n",
    "    current_h6 = None\n",
    "\n",
    "    # Iterate over headings and paragraphs\n",
    "    for tag in content.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'p']):\n",
    "        if tag.name == 'h1':\n",
    "            current_h1 = tag.get_text(strip=True)\n",
    "            data[current_h1] = {}\n",
    "            current_h2 = current_h3 = current_h4 = current_h5 = current_h6 = None\n",
    "        elif tag.name == 'h2' and current_h1:\n",
    "            current_h2 = tag.get_text(strip=True)\n",
    "            data[current_h1][current_h2] = {}\n",
    "            current_h3 = current_h4 = current_h5 = current_h6 = None\n",
    "        elif tag.name == 'h3' and current_h2:\n",
    "            current_h3 = tag.get_text(strip=True)\n",
    "            data[current_h1][current_h2][current_h3] = {}\n",
    "            current_h4 = current_h5 = current_h6 = None\n",
    "        elif tag.name == 'h4' and current_h3:\n",
    "            current_h4 = tag.get_text(strip=True)\n",
    "            data[current_h1][current_h2][current_h3][current_h4] = {}\n",
    "            current_h5 = current_h6 = None\n",
    "        elif tag.name == 'h5' and current_h4:\n",
    "            current_h5 = tag.get_text(strip=True)\n",
    "            data[current_h1][current_h2][current_h3][current_h4][current_h5] = {}\n",
    "            current_h6 = None\n",
    "        elif tag.name == 'h6' and current_h5:\n",
    "            current_h6 = tag.get_text(strip=True)\n",
    "            data[current_h1][current_h2][current_h3][current_h4][current_h5][current_h6] = {}\n",
    "        elif tag.name == 'p':\n",
    "            if current_h6:\n",
    "                data[current_h1][current_h2][current_h3][current_h4][current_h5][current_h6].setdefault('content', []).append(tag.get_text(strip=True))\n",
    "            elif current_h5:\n",
    "                data[current_h1][current_h2][current_h3][current_h4][current_h5].setdefault('content', []).append(tag.get_text(strip=True))\n",
    "            elif current_h4:\n",
    "                data[current_h1][current_h2][current_h3][current_h4].setdefault('content', []).append(tag.get_text(strip=True))\n",
    "            elif current_h3:\n",
    "                data[current_h1][current_h2][current_h3].setdefault('content', []).append(tag.get_text(strip=True))\n",
    "            elif current_h2:\n",
    "                data[current_h1][current_h2].setdefault('content', []).append(tag.get_text(strip=True))\n",
    "            elif current_h1:\n",
    "                data[current_h1].setdefault('content', []).append(tag.get_text(strip=True))\n",
    "\n",
    "    \n",
    "    # Save the dictionary as a JSON file\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=4)\n",
    "    \n",
    "    print(f\"Content successfully written to {output_file}\")\n",
    "\n",
    "def visit_pitt_sport_url_to_docuemnts(url_list):\n",
    "    for url in url_list:\n",
    "        if url.endswith('pdf'):\n",
    "            print(f\"Skipping PDF file: {url}\")\n",
    "            continue\n",
    "        try:\n",
    "            output_file = build_file_name(url)\n",
    "            scrape_visit_sport_to_json(url, output_file)\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to scrape {url}: {e}\")\n",
    "            continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visit_pitt_sport_url_to_docuemnts(visit_sports_links)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "food"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "visit_pitt_food_url = 'https://www.visitpittsburgh.com/restaurants-culinary/'\n",
    "response = requests.get(visit_pitt_food_url)\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "# Find all <a> tags\n",
    "visit_food_links = []\n",
    "div = soup.find('main', class_='content--primary')\n",
    "for a_tag in div.find_all('a', href=True):\n",
    "    link = a_tag['href']\n",
    "    if link.startswith('/'):  # If it's a relative link, prepend the base URL\n",
    "        link = f\"{url.rstrip('/')}{link}\"\n",
    "    if len(link.split('/')) > 4:\n",
    "        visit_food_links.append(link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visit_pitt_sport_url_to_docuemnts(visit_food_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Send request to the URL\n",
    "url = 'https://www.visitpittsburgh.com/blog/unique-pittsburgh-pierogi-joints/'\n",
    "response = requests.get(url)\n",
    "\n",
    "# Parse the page content\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# Find the correct div based on the updated structure\n",
    "if 'blog' in url:\n",
    "    content = soup.find('article',class_= 'detail__inner')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "main pages / other pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "visit_other_url = [\n",
    "    'https://www.visitpittsburgh.com/',\n",
    "    'https://www.visitpittsburgh.com/nfl-draft-pittsburgh/',\n",
    "    'https://www.visitpittsburgh.com/blog/fall-fairs-festivals-and-events/',\n",
    "    'https://www.visitpittsburgh.com/blog/pittsburgh-fall-date-ideas/',\n",
    "    'https://www.visitpittsburgh.com/blog/guide-to-football-in-pittsburgh/',\n",
    "    'https://www.visitpittsburgh.com/blog/upcoming-concerts-in-pittsburgh/',\n",
    "    'https://www.visitpittsburgh.com/blog/how-to-ride-the-pittsburgh-inclines/',\n",
    "    'https://www.visitpittsburgh.com/blog/pittsburgh-theatre-upcoming-performances/',\n",
    "    'https://www.visitpittsburgh.com/blog/penguins-penguins-penguins/',\n",
    "    'https://www.visitpittsburgh.com/blog/halloween-pop-ups-bars-and-restaurants-in-pittsburgh/',\n",
    "    'https://www.visitpittsburgh.com/blog/jackworth-ginger-beer-pittsburgh/',\n",
    "    'https://www.visitpittsburgh.com/restaurants-culinary/farms-farmers-markets/guide-to-pittsburgh-farmers-markets/',\n",
    "    'https://www.visitpittsburgh.com/restaurants-culinary/farms-farmers-markets/pittsburgh-farm-fresh-meats/',\n",
    "    'https://www.visitpittsburgh.com/restaurants-culinary/farms-farmers-markets/pittsburghs-csas/',\n",
    "    'https://www.visitpittsburgh.com/restaurants-culinary/farms-farmers-markets/u-pick-farms/'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visit_pitt_sport_url_to_docuemnts(visit_other_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gov Tax Forms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "参考如何把structure data和RAG结合：https://medium.com/intel-tech/tabular-data-rag-llms-improve-results-through-data-table-prompting-bcb42678914b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://pittsburghpa.gov/finance/tax-forms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2024 Operating Budget\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://apps.pittsburghpa.gov/redtail/images/23255_2024_Operating_Budget.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CMU about"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.cmu.edu/about/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
