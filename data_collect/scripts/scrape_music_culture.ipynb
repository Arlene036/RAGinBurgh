{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Music\n",
    "## Symphony\n",
    "### https://www.pittsburghsymphony.org/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping page 1...\n",
      "Scraping page 2...\n",
      "Scraping page 3...\n",
      "Scraping page 4...\n",
      "Scraping page 5...\n",
      "Pittsburgh Symphony Scraping completed.\n"
     ]
    }
   ],
   "source": [
    "###################################### Events ######################################\n",
    "\n",
    "import json\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "\n",
    "class PittsburghSymphonyScraper:\n",
    "    def __init__(self):\n",
    "        chrome_options = Options()\n",
    "        chrome_options.add_argument(\"--headless\")\n",
    "        chrome_options.add_argument(\"--disable-gpu\")\n",
    "        chrome_options.add_argument(\"--no-sandbox\")\n",
    "        chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "        self.driver = webdriver.Chrome(options=chrome_options)\n",
    "        self.base_url = \"https://www.pittsburghsymphony.org/calendar?page=\"\n",
    "        self.result_file = \"../raw_documents/Pittsburgh_Symphony.json\"\n",
    "\n",
    "    def fetch_page(self, page_num):\n",
    "        url = self.base_url + str(page_num)\n",
    "        try:\n",
    "            self.driver.get(url)\n",
    "            time.sleep(3)  # wait the page to load\n",
    "            page_source = self.driver.page_source\n",
    "            soup = BeautifulSoup(page_source, \"html.parser\")\n",
    "            return soup\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching page {page_num}: {e}\")\n",
    "            return None\n",
    "\n",
    "    def extract_event_info(self, soup):\n",
    "        events = []\n",
    "        event_list = soup.find_all('article', class_='event')\n",
    "        for event in event_list:\n",
    "            try:\n",
    "                title = event.find('h3', class_='title').get_text(strip=True)\n",
    "                time = event.find('time', class_='range').get_text(strip=True)\n",
    "                venue = event.find('div', class_='venue').get_text(strip=True)\n",
    "                organization = event.find('div', class_='organization').get_text(strip=True)\n",
    "\n",
    "                event_info = {\n",
    "                    \"event_name\": title,\n",
    "                    \"event_time\": time,\n",
    "                    \"venue\": venue,\n",
    "                    \"organization\": organization\n",
    "                }\n",
    "                events.append(event_info)\n",
    "            except Exception as e:\n",
    "                print(f\"Error extracting event info: {e}\")\n",
    "        return events\n",
    "\n",
    "    def append_to_json(self, events):    \n",
    "        # write in time\n",
    "        try:\n",
    "            with open(self.result_file, 'a') as f:\n",
    "                for event in events:\n",
    "                    json.dump(event, f, indent=4)\n",
    "                    f.write(\"\\n\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error writing to JSON file: {e}\")\n",
    "\n",
    "    def scrape(self):\n",
    "        for page_num in range(1, 6):\n",
    "            print(f\"Scraping page {page_num}...\")\n",
    "            soup = self.fetch_page(page_num)\n",
    "            if soup:\n",
    "                events = self.extract_event_info(soup)\n",
    "                self.append_to_json(events)\n",
    "\n",
    "    def close(self):\n",
    "        self.driver.quit()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    scraper = PittsburghSymphonyScraper()\n",
    "    scraper.scrape()\n",
    "    scraper.close()\n",
    "    print(\"Pittsburgh Symphony Scraping completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Musicians' data has been appended to the JSON file.\n"
     ]
    }
   ],
   "source": [
    "###################################### Musicians ######################################\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Pittsburgh Symphony Orchestra musicians page URL\n",
    "url = \"https://www.pittsburghsymphony.org/pso_home/web/musicians\"\n",
    "\n",
    "# Send a GET request to the page\n",
    "response = requests.get(url)\n",
    "\n",
    "# Parse the page content using BeautifulSoup\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "# Initialize a dictionary to store musician data\n",
    "musicians_data = {}\n",
    "\n",
    "# Define the path of the existing JSON file\n",
    "json_file_path = \"../raw_documents/Pittsburgh_Symphony.json\"\n",
    "\n",
    "# If the file exists, load the existing data\n",
    "if os.path.exists(json_file_path):\n",
    "    with open(json_file_path, \"r\") as json_file:\n",
    "        try:\n",
    "            musicians_data = json.load(json_file)\n",
    "        except json.JSONDecodeError:\n",
    "            musicians_data = {}\n",
    "\n",
    "# Define a helper function to get musician introduction from subpages\n",
    "def get_musician_introduction(subpage_url):\n",
    "    try:\n",
    "        subpage_response = requests.get(subpage_url)\n",
    "        subpage_soup = BeautifulSoup(subpage_response.content, \"html.parser\")\n",
    "        bio_text_div = subpage_soup.find(\"div\", class_=\"bio-text\")\n",
    "        if bio_text_div:\n",
    "            return bio_text_div.get_text(strip=True, separator=\" \")\n",
    "    except Exception as e:\n",
    "        print(f\"Error accessing {subpage_url}: {e}\")\n",
    "    return None\n",
    "\n",
    "# Loop over sections such as First Violin, Second Violin, etc.\n",
    "for section in soup.find_all(\"h3\"):\n",
    "    section_name = section.get_text().strip()\n",
    "    # If this section already exists in the JSON, skip it to avoid duplicates\n",
    "    if section_name in musicians_data:\n",
    "        continue\n",
    "    \n",
    "    musicians_data[section_name] = []\n",
    "    \n",
    "    # Find the <p> tag containing musician list under each section\n",
    "    musician_list = section.find_next(\"p\")\n",
    "    \n",
    "    # Check if musician_list exists\n",
    "    if musician_list:\n",
    "        # Get all musician names and titles within the <p> tag\n",
    "        for musician in musician_list.find_all(\"a\"):\n",
    "            musician_name = musician.get_text(strip=True)\n",
    "            musician_title = musician_list.get_text(strip=True).split('|')[1].strip() if '|' in musician_list.get_text() else \"\"\n",
    "            musician_data = {\n",
    "                \"name\": musician_name,\n",
    "                \"title\": musician_title\n",
    "            }\n",
    "            \n",
    "            # If musician has a subpage, fetch their introduction\n",
    "            musician_subpage_url = musician.get(\"href\")\n",
    "            if musician_subpage_url:\n",
    "                full_subpage_url = f\"https://www.pittsburghsymphony.org{musician_subpage_url}\"\n",
    "                introduction = get_musician_introduction(full_subpage_url)\n",
    "                if introduction:\n",
    "                    musician_data[\"introduction\"] = introduction\n",
    "            \n",
    "            # Append each musician's data to the list under the section\n",
    "            musicians_data[section_name].append(musician_data)\n",
    "\n",
    "# Append new data to the JSON file\n",
    "with open(json_file_path, \"w\") as json_file:\n",
    "    json.dump(musicians_data, json_file, indent=4)\n",
    "    json_file.write(\"\\n\")  # Optional newline for better readability\n",
    "\n",
    "print(\"Musicians' data has been appended to the JSON file.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Opera\n",
    "### https://pittsburghopera.org/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching: https://pittsburghopera.org/\n",
      "Fetching: https://pittsburghopera.org/about/mission-history\n",
      "Fetching: https://pittsburghopera.org/about/inclusion-diversity-equity-accessibility-idea/\n",
      "Fetching: https://pittsburghopera.org/calendar?timequery=week&prev=71+&start=1722484800000&end=1725145140000\n",
      "Fetching: https://pittsburghopera.org/calendar?timequery=week&prev=55+&start=1722484800000&end=1725145140000\n",
      "Fetching: https://pittsburghopera.org/calendar?timequery=week&prev=47+&start=1722484800000&end=1725145140000\n",
      "Fetching: https://pittsburghopera.org/calendar?timequery=week&prev=39+&start=1722484800000&end=1725145140000\n",
      "Fetching: https://pittsburghopera.org/calendar?timequery=week&prev=31+&start=1722484800000&end=1725145140000\n",
      "Fetching: https://pittsburghopera.org/calendar?timequery=week&prev=23+&start=1722484800000&end=1725145140000\n",
      "Fetching: https://pittsburghopera.org/calendar?timequery=week&prev=15+&start=1722484800000&end=1725145140000\n",
      "Fetching: https://pittsburghopera.org/calendar?timequery=week&prev=7+&start=1722484800000&end=1725145140000\n",
      "Fetching: https://pittsburghopera.org/calendar?timequery=week&prev=-1+&start=1722484800000&end=1725145140000\n",
      "Fetching: https://pittsburghopera.org/calendar?timequery=week&prev=-9+&start=1722484800000&end=1725145140000\n",
      "Fetching: https://pittsburghopera.org/calendar?timequery=week&prev=-17+&start=1722484800000&end=1725145140000\n",
      "Fetching: https://pittsburghopera.org/calendar?timequery=week&prev=-25+&start=1722484800000&end=1725145140000\n",
      "Fetching: https://pittsburghopera.org/calendar?timequery=week&prev=-33+&start=1722484800000&end=1725145140000\n",
      "Fetching: https://pittsburghopera.org/calendar?timequery=week&prev=-41+&start=1722484800000&end=17251451400000\n",
      "Fetching: https://pittsburghopera.org/calendar?timequery=week&prev=-57+&start=1722484800000&end=1725145140000\n",
      "Fetching: https://pittsburghopera.org/calendar?timequery=week&prev=-97+&start=1722484800000&end=1725145140000\n",
      "Fetching: https://pittsburghopera.org/calendar?timequery=week&prev=-105+&start=1722484800000&end=1725145140000\n",
      "Fetching: https://pittsburghopera.org/calendar?timequery=week&prev=-113+&start=1722484800000&end=1725145140000\n",
      "Fetching: https://pittsburghopera.org/calendar?timequery=week&prev=-121+&start=1722484800000&end=1725145140000\n",
      "Fetching: https://pittsburghopera.org/calendar?timequery=week&prev=-129+&start=1722484800000&end=1725145140000\n",
      "Fetching: https://pittsburghopera.org/calendar?timequery=week&prev=-145+&start=1722484800000&end=1725145140000\n",
      "Fetching: https://pittsburghopera.org/calendar?timequery=week&prev=-153+&start=1722484800000&end=1725145140000\n",
      "Fetching: https://pittsburghopera.org/calendar?timequery=week&prev=-161+&start=1722484800000&end=1725145140000\n",
      "Fetching: https://pittsburghopera.org/calendar?timequery=week&prev=-169+&start=1722484800000&end=1725145140000\n",
      "Fetching: https://pittsburghopera.org/calendar?timequery=month&prev=-169+&start=1743480000000&end=1746053940000\n",
      "Fetching: https://pittsburghopera.org/calendar?timequery=month&prev=-169+&start=1746072000000&end=1748732340000\n",
      "Fetching: https://pittsburghopera.org/season/tosca\n",
      "Fetching: https://pittsburghopera.org/season/cavalleria-rusticana-pagliacci\n",
      "Fetching: https://pittsburghopera.org/season/armida\n",
      "Fetching: https://pittsburghopera.org/season/madama-butterflyhttps://pittsburghopera.org/season/woman-with-eyes-closed\n",
      "Fetching: https://pittsburghopera.org/season/past-seasons\n",
      "Fetching: https://pittsburghopera.org/season/the-barber-of-seville?hsLang=en\n",
      "Fetching: https://pittsburghopera.org/season/the-flying-dutchman?hsLang=en\n",
      "Fetching: https://pittsburghopera.org/season/iphigenie-en-tauride?hsLang=en\n",
      "Fetching: https://pittsburghopera.org/season/proving-up?hsLang=en\n",
      "Fetching: https://pittsburghopera.org/season/la-traviata?hsLang=en\n",
      "Fetching: https://pittsburghopera.org/season/the-passion-of-mary-cardwell-dawson?hsLang=en\n",
      "Fetching: https://pittsburghopera.org/season/past-seasons/rusalka?hsLang=en\n",
      "Fetching: https://pittsburghopera.org/season/past-seasons/the-marriage-of-figaro?hsLang=en\n",
      "Fetching: https://pittsburghopera.org/season/past-seasons/ariodante?hsLang=en\n",
      "Fetching: https://pittsburghopera.org/season/past-seasons/il-trovatore?hsLang=en\n",
      "Fetching: https://pittsburghopera.org/season/past-seasons/denis-katya?hsLang=en\n",
      "Fetching: https://pittsburghopera.org/season/past-seasons/we-shall-not-be-moved?hsLang=en\n",
      "Fetching: https://pittsburghopera.org/season/past-seasons/the-magic-flute?hsLang=en\n",
      "Fetching: https://pittsburghopera.org/season/past-seasons/the-rose-elf?hsLang=en\n",
      "Fetching: https://pittsburghopera.org/season/past-seasons/in-a-grove?hsLang=en\n",
      "Fetching: https://pittsburghopera.org/season/past-seasons/carmen?hsLang=en\n",
      "Fetching: https://pittsburghopera.org/season/past-seasons/blue?hsLang=enhttps://pittsburghopera.org/season/past-seasons/semele?hsLang=en\n",
      "Fetching: https://pittsburghopera.org/season/past-seasons/cosi-fan-tutte?hsLang=en\n",
      "Fetching: https://pittsburghopera.org/season/past-seasons/soldier-songs?hsLang=en\n",
      "Fetching: https://pittsburghopera.org/season/past-seasons/charlie-parkers-yardbird?hsLang=en\n",
      "Fetching: https://pittsburghopera.org/season/past-seasons/last-american-hammer?hsLang=en\n",
      "Fetching: https://pittsburghopera.org/season/past-seasons/alcina?hsLang=en\n",
      "Fetching: https://pittsburghopera.org/season/past-seasons/florencia-en-el-amazonas?hsLang=en\n",
      "Fetching: https://pittsburghopera.org/season/past-seasons/don-giovanni?hsLang=en\n",
      "Fetching: https://pittsburghopera.org/season/past-seasons/madama-butterfly?hsLang=en\n",
      "Fetching: https://pittsburghopera.org/season/past-seasons/afterwards?hsLang=en\n",
      "Fetching: https://pittsburghopera.org/season/past-seasons/glory-denied?hsLang=en\n",
      "Fetching: https://pittsburghopera.org/season/past-seasons/la-boheme?hsLang=en\n",
      "Fetching: https://pittsburghopera.org/season/past-seasons/don-pasquale?hsLang=en\n",
      "Fetching: https://pittsburghopera.org/season/past-seasons/hansel-gretel?hsLang=en\n",
      "Fetching: https://pittsburghopera.org/season/past-seasons/savage-winter?hsLang=en\n",
      "Fetching: https://pittsburghopera.org/season/past-seasons/the-long-walk?hsLang=en\n",
      "Fetching: https://pittsburghopera.org/season/past-seasons/the-elixir-of-love?hsLang=en\n",
      "Fetching: https://pittsburghopera.org/season/past-seasons/moby-dick?hsLang=en\n",
      "Fetching: https://pittsburghopera.org/season/past-seasons/the-marriage-of-figaro?hsLang=en\n",
      "Fetching: https://pittsburghopera.org/season/past-seasons/tosca-2017?hsLang=en\n",
      "Fetching: https://pittsburghopera.org/season/past-seasons/the-summer-king?hsLang=en\n",
      "Fetching: https://pittsburghopera.org/season/past-seasons/turandot?hsLang=en\n",
      "Fetching: https://pittsburghopera.org/season/past-seasons/as-one?hsLang=en\n",
      "Fetching: https://pittsburghopera.org/season/past-seasons/richard-the-lionheart?hsLang=en\n",
      "Fetching: https://pittsburghopera.org/season/past-seasons/salome?hsLang=en\n",
      "Fetching: https://pittsburghopera.org/season/past-seasons/la-traviata-2016?hsLang=en\n",
      "Fetching: https://pittsburghopera.org/season/past-seasons/the-rakes-progress?hsLang=en\n",
      "Fetching: https://pittsburghopera.org/season/past-seasons/twenty-seven?hsLang=en\n",
      "Fetching: https://pittsburghopera.org/season/past-seasons/the-barber-of-seville-2016?hsLang=en\n",
      "Fetching: https://pittsburghopera.org/season/past-seasons/little-women?hsLang=en\n",
      "Fetching: https://pittsburghopera.org/season/past-seasons/cos%C3%AC-fan-tutte-2015?hsLang=en\n",
      "Fetching: https://pittsburghopera.org/season/past-seasons/nabucco?hsLang=en\n",
      "Fetching: https://pittsburghopera.org/season/free-low-cost-events\n",
      "Fetching: https://pittsburghopera.org/season/free-low-cost-events/songshop\n",
      "Fetching: https://pittsburghopera.org/season/free-low-cost-events/opera-up-close\n",
      "Fetching: https://pittsburghopera.org/season/free-low-cost-events/pre-opera-talks\n",
      "Fetching: https://pittsburghopera.org/season/free-low-cost-events/meet-the-artists\n",
      "Fetching: https://pittsburghopera.org/season/free-low-cost-events/community-concerts\n",
      "Fetching: https://pittsburghopera.org/season/free-low-cost-events/allegheny-county-summer-concert-series\n",
      "Fetching: https://pittsburghopera.org/season/free-low-cost-events/wqed-broadcasts\n",
      "Fetching: https://pittsburghopera.org/season/special-events\n",
      "Fetching: https://pittsburghopera.org/season/special-events/diamond-horseshoe-ball\n",
      "Fetching: https://pittsburghopera.org/season/special-events/pittsburgh-opera-fashion-event\n",
      "Fetching: https://pittsburghopera.org/season/special-events/maecenas\n",
      "Fetching: https://pittsburghopera.org/resident-artists/2024-25resident-artists\n",
      "Fetching: https://pittsburghopera.org/resident-artists/faculty-administration/\n",
      "Fetching: https://pittsburghopera.org/resident-artists/auditions/\n",
      "Fetching: https://pittsburghopera.org/resident-artists/history-alumni/\n",
      "Fetching: https://pittsburghopera.org/our-team/orchestra\n",
      "Fetching: https://pittsburghopera.org/our-team/chorus\n",
      "Fetching: https://pittsburghopera.org/facilities/pittsburgh-opera-headquarters/\n",
      "Fetching: https://pittsburghopera.org/facilities/office-hours\n",
      "Fetching: https://pittsburghopera.org/facilities/production-rentals\n",
      "Fetching: https://pittsburghopera.org/facilities/hold-your-event-at-pittsburgh-opera/\n",
      "Data saved to ../raw_documents/Pittsburgh_Opera.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "\n",
    "class Scraper:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.chrome_options = Options()\n",
    "        self.chrome_options.add_argument(\"--headless\")\n",
    "        self.chrome_options.add_argument(\"--disable-gpu\")\n",
    "        self.chrome_options.add_argument(\"--no-sandbox\")\n",
    "        self.chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "        self.driver = webdriver.Chrome(options=self.chrome_options)\n",
    "        self.driver.set_page_load_timeout(10)  # Adjusted timeout for slower pages\n",
    "        self.current_url = None\n",
    "        self.current_domain = None\n",
    "\n",
    "    def set_domain(self, url: str):\n",
    "        \"\"\"set domain name and URL\"\"\"\n",
    "        self.current_domain = url.split(\"/\")[2]  # 例如 'www.example.com'\n",
    "        self.current_url = url\n",
    "\n",
    "    def fetch(self, url: str, raw_html: bool = False):\n",
    "        \"\"\"capture the webpage and choose whether to remove JS or CSS\"\"\"\n",
    "        soup = self.get_soup(url)\n",
    "        if not raw_html:\n",
    "            soup = self.remove_js_css(soup)\n",
    "        return soup, self.get_links(soup), self.get_title(soup)\n",
    "\n",
    "    def get_html(self, url: str):\n",
    "        \"\"\"get HTML\"\"\"\n",
    "        self.set_domain(url)  # 调用 set_domain 方法\n",
    "        self.driver.get(url)\n",
    "        return self.driver.page_source\n",
    "\n",
    "    def get_soup(self, url: str):\n",
    "        \"\"\"parse and get BeautifulSoup object\"\"\"\n",
    "        html = self.get_html(url)\n",
    "        return BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "    def get_links(self, soup: BeautifulSoup):\n",
    "        \"\"\"get external links\"\"\"\n",
    "        links = [link.get(\"href\") for link in soup.find_all(\"a\")]\n",
    "        return self.filter_links(links)\n",
    "\n",
    "    def get_title(self, soup: BeautifulSoup):\n",
    "        \"\"\"title\"\"\"\n",
    "        if soup.title is None:\n",
    "            return f\"untitled_{self.get_timestamp()}\"\n",
    "        title = soup.title.string.replace(\" \", \"_\").replace(\"/\", \"__\")\n",
    "        return title.replace(\"\\n\", \"\")\n",
    "\n",
    "    def remove_js_css(self, soup: BeautifulSoup):\n",
    "        \"\"\"remove JS or CSS\"\"\"\n",
    "        for script in soup([\"script\", \"style\"]):\n",
    "            script.extract()  # 移除所有的 <script> 和 <style> 标签\n",
    "        return soup\n",
    "\n",
    "    def remove_unnecessary_sections(self, soup: BeautifulSoup):\n",
    "        \"\"\"remove header and footer\"\"\"\n",
    "        for tag in [\"header\", \"footer\", \"nav\", \"aside\"]:\n",
    "            for element in soup.find_all(tag):\n",
    "                element.decompose()\n",
    "        return soup\n",
    "\n",
    "    def filter_links(self, links: list):\n",
    "        filtered_links = []\n",
    "        for link in links:\n",
    "            if link is None:\n",
    "                continue\n",
    "            elif link.startswith(\"#\"):\n",
    "                continue \n",
    "            elif link.startswith(\"http\"):\n",
    "                filtered_links.append(link)\n",
    "            elif link.startswith(\"//\"):\n",
    "                filtered_links.append(f\"https:{link}\")\n",
    "            elif link.startswith(\"/\"):\n",
    "                filtered_links.append(f\"https://{self.current_domain}{link}\") \n",
    "            else:\n",
    "                filtered_links.append(f\"{self.current_url}/{link}\")\n",
    "        return filtered_links\n",
    "\n",
    "    def get_timestamp(self):\n",
    "        return datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "    def close(self):\n",
    "        self.driver.quit()\n",
    "\n",
    "    def __del__(self):\n",
    "        self.close()\n",
    "\n",
    "\n",
    "def organize_content_by_heading(soup: BeautifulSoup):\n",
    "    data = {}\n",
    "    current_h2, current_h3, current_h4, current_h5, current_h6 = None, None, None, None, None\n",
    "\n",
    "    for tag in soup.find_all(['h2', 'h3', 'h4', 'h5', 'h6', 'p']):\n",
    "        if tag.name == 'h2':\n",
    "            current_h2 = tag.get_text(strip=True)\n",
    "            data[current_h2] = {}\n",
    "            current_h3 = current_h4 = current_h5 = current_h6 = None\n",
    "        elif tag.name == 'h3' and current_h2:\n",
    "            current_h3 = tag.get_text(strip=True)\n",
    "            data[current_h2][current_h3] = {}\n",
    "            current_h4 = current_h5 = current_h6 = None\n",
    "        elif tag.name == 'h4' and current_h3:\n",
    "            current_h4 = tag.get_text(strip=True)\n",
    "            data[current_h2][current_h3][current_h4] = {}\n",
    "            current_h5 = current_h6 = None\n",
    "        elif tag.name == 'h5' and current_h4:\n",
    "            current_h5 = tag.get_text(strip=True)\n",
    "            data[current_h2][current_h3][current_h4][current_h5] = {}\n",
    "            current_h6 = None\n",
    "        elif tag.name == 'h6' and current_h5:\n",
    "            current_h6 = tag.get_text(strip=True)\n",
    "            data[current_h2][current_h3][current_h4][current_h5][current_h6] = {}\n",
    "        elif tag.name == 'p':\n",
    "            if current_h6:\n",
    "                data[current_h2][current_h3][current_h4][current_h5][current_h6].setdefault('content', []).append(tag.get_text(strip=True))\n",
    "            elif current_h5:\n",
    "                data[current_h2][current_h3][current_h4][current_h5].setdefault('content', []).append(tag.get_text(strip=True))\n",
    "            elif current_h4:\n",
    "                data[current_h2][current_h3][current_h4].setdefault('content', []).append(tag.get_text(strip=True))\n",
    "            elif current_h3:\n",
    "                data[current_h2][current_h3].setdefault('content', []).append(tag.get_text(strip=True))\n",
    "            elif current_h2:\n",
    "                data[current_h2].setdefault('content', []).append(tag.get_text(strip=True))\n",
    "    \n",
    "    return data\n",
    "\n",
    "\n",
    "def save_to_json(data, filename):\n",
    "    try:\n",
    "        with open(filename, 'w', encoding='utf-8') as f:\n",
    "            json.dump(data, f, ensure_ascii=False, indent=4)\n",
    "        print(f\"Data saved to {filename}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error writing to JSON file: {e}\")\n",
    "\n",
    "\n",
    "def scrape_links(scraper_, links):\n",
    "    all_data = []\n",
    "    for link in links:\n",
    "        try:\n",
    "            print(f\"Fetching: {link}\")\n",
    "            soup = scraper_.get_soup(link)\n",
    "            \n",
    "            soup = scraper_.remove_js_css(soup)\n",
    "            soup = scraper_.remove_unnecessary_sections(soup)\n",
    "\n",
    "            page_data = organize_content_by_heading(soup)\n",
    "            page_data[\"url\"] = link  \n",
    "\n",
    "            all_data.append(page_data)\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching {link}: {e}\")\n",
    "    \n",
    "    return all_data\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    scraper_ = Scraper()\n",
    "\n",
    "    links = [\n",
    "        \"https://pittsburghopera.org/\",\n",
    "        \"https://pittsburghopera.org/about/mission-history\",\n",
    "        \"https://pittsburghopera.org/about/inclusion-diversity-equity-accessibility-idea/\",\n",
    "        \"https://pittsburghopera.org/calendar?timequery=week&prev=71+&start=1722484800000&end=1725145140000\",\n",
    "        \"https://pittsburghopera.org/calendar?timequery=week&prev=55+&start=1722484800000&end=1725145140000\",\n",
    "        \"https://pittsburghopera.org/calendar?timequery=week&prev=47+&start=1722484800000&end=1725145140000\",\n",
    "        \"https://pittsburghopera.org/calendar?timequery=week&prev=39+&start=1722484800000&end=1725145140000\",\n",
    "        \"https://pittsburghopera.org/calendar?timequery=week&prev=31+&start=1722484800000&end=1725145140000\",\n",
    "        \"https://pittsburghopera.org/calendar?timequery=week&prev=23+&start=1722484800000&end=1725145140000\",\n",
    "        \"https://pittsburghopera.org/calendar?timequery=week&prev=15+&start=1722484800000&end=1725145140000\",\n",
    "        \"https://pittsburghopera.org/calendar?timequery=week&prev=7+&start=1722484800000&end=1725145140000\",\n",
    "        \"https://pittsburghopera.org/calendar?timequery=week&prev=-1+&start=1722484800000&end=1725145140000\",\n",
    "        \"https://pittsburghopera.org/calendar?timequery=week&prev=-9+&start=1722484800000&end=1725145140000\",\n",
    "        \"https://pittsburghopera.org/calendar?timequery=week&prev=-17+&start=1722484800000&end=1725145140000\",\n",
    "        \"https://pittsburghopera.org/calendar?timequery=week&prev=-25+&start=1722484800000&end=1725145140000\",\n",
    "        \"https://pittsburghopera.org/calendar?timequery=week&prev=-33+&start=1722484800000&end=1725145140000\",\n",
    "        \"https://pittsburghopera.org/calendar?timequery=week&prev=-41+&start=1722484800000&end=17251451400000\",\n",
    "        \"https://pittsburghopera.org/calendar?timequery=week&prev=-57+&start=1722484800000&end=1725145140000\",\n",
    "        \"https://pittsburghopera.org/calendar?timequery=week&prev=-97+&start=1722484800000&end=1725145140000\",\n",
    "        \"https://pittsburghopera.org/calendar?timequery=week&prev=-105+&start=1722484800000&end=1725145140000\",\n",
    "        \"https://pittsburghopera.org/calendar?timequery=week&prev=-113+&start=1722484800000&end=1725145140000\",\n",
    "        \"https://pittsburghopera.org/calendar?timequery=week&prev=-121+&start=1722484800000&end=1725145140000\",\n",
    "        \"https://pittsburghopera.org/calendar?timequery=week&prev=-129+&start=1722484800000&end=1725145140000\",\n",
    "        \"https://pittsburghopera.org/calendar?timequery=week&prev=-145+&start=1722484800000&end=1725145140000\",\n",
    "        \"https://pittsburghopera.org/calendar?timequery=week&prev=-153+&start=1722484800000&end=1725145140000\",\n",
    "        \"https://pittsburghopera.org/calendar?timequery=week&prev=-161+&start=1722484800000&end=1725145140000\",\n",
    "        \"https://pittsburghopera.org/calendar?timequery=week&prev=-169+&start=1722484800000&end=1725145140000\",\n",
    "        \"https://pittsburghopera.org/calendar?timequery=month&prev=-169+&start=1743480000000&end=1746053940000\",\n",
    "        \"https://pittsburghopera.org/calendar?timequery=month&prev=-169+&start=1746072000000&end=1748732340000\",\n",
    "        \"https://pittsburghopera.org/season/tosca\",\n",
    "        \"https://pittsburghopera.org/season/cavalleria-rusticana-pagliacci\",\n",
    "        \"https://pittsburghopera.org/season/armida\",\n",
    "        \"https://pittsburghopera.org/season/madama-butterfly\"\n",
    "        \"https://pittsburghopera.org/season/woman-with-eyes-closed\",\n",
    "        \"https://pittsburghopera.org/season/past-seasons\",\n",
    "        \"https://pittsburghopera.org/season/the-barber-of-seville?hsLang=en\",\n",
    "        \"https://pittsburghopera.org/season/the-flying-dutchman?hsLang=en\",\n",
    "        \"https://pittsburghopera.org/season/iphigenie-en-tauride?hsLang=en\",\n",
    "        \"https://pittsburghopera.org/season/proving-up?hsLang=en\",\n",
    "        \"https://pittsburghopera.org/season/la-traviata?hsLang=en\",\n",
    "        \"https://pittsburghopera.org/season/the-passion-of-mary-cardwell-dawson?hsLang=en\",\n",
    "        \"https://pittsburghopera.org/season/past-seasons/rusalka?hsLang=en\",\n",
    "        \"https://pittsburghopera.org/season/past-seasons/the-marriage-of-figaro?hsLang=en\",\n",
    "        \"https://pittsburghopera.org/season/past-seasons/ariodante?hsLang=en\",\n",
    "        \"https://pittsburghopera.org/season/past-seasons/il-trovatore?hsLang=en\",\n",
    "        \"https://pittsburghopera.org/season/past-seasons/denis-katya?hsLang=en\",\n",
    "        \"https://pittsburghopera.org/season/past-seasons/we-shall-not-be-moved?hsLang=en\",\n",
    "        \"https://pittsburghopera.org/season/past-seasons/the-magic-flute?hsLang=en\",\n",
    "        \"https://pittsburghopera.org/season/past-seasons/the-rose-elf?hsLang=en\",\n",
    "        \"https://pittsburghopera.org/season/past-seasons/in-a-grove?hsLang=en\",\n",
    "        \"https://pittsburghopera.org/season/past-seasons/carmen?hsLang=en\",\n",
    "        \"https://pittsburghopera.org/season/past-seasons/blue?hsLang=en\"\n",
    "        \"https://pittsburghopera.org/season/past-seasons/semele?hsLang=en\",\n",
    "        \"https://pittsburghopera.org/season/past-seasons/cosi-fan-tutte?hsLang=en\",\n",
    "        \"https://pittsburghopera.org/season/past-seasons/soldier-songs?hsLang=en\",\n",
    "        \"https://pittsburghopera.org/season/past-seasons/charlie-parkers-yardbird?hsLang=en\",\n",
    "        \"https://pittsburghopera.org/season/past-seasons/last-american-hammer?hsLang=en\",\n",
    "        \"https://pittsburghopera.org/season/past-seasons/alcina?hsLang=en\",\n",
    "        \"https://pittsburghopera.org/season/past-seasons/florencia-en-el-amazonas?hsLang=en\",\n",
    "        \"https://pittsburghopera.org/season/past-seasons/don-giovanni?hsLang=en\",\n",
    "        \"https://pittsburghopera.org/season/past-seasons/madama-butterfly?hsLang=en\",\n",
    "        \"https://pittsburghopera.org/season/past-seasons/afterwards?hsLang=en\",\n",
    "        \"https://pittsburghopera.org/season/past-seasons/glory-denied?hsLang=en\",\n",
    "        \"https://pittsburghopera.org/season/past-seasons/la-boheme?hsLang=en\",\n",
    "        \"https://pittsburghopera.org/season/past-seasons/don-pasquale?hsLang=en\",\n",
    "        \"https://pittsburghopera.org/season/past-seasons/hansel-gretel?hsLang=en\",\n",
    "        \"https://pittsburghopera.org/season/past-seasons/savage-winter?hsLang=en\",\n",
    "        \"https://pittsburghopera.org/season/past-seasons/the-long-walk?hsLang=en\",\n",
    "        \"https://pittsburghopera.org/season/past-seasons/the-elixir-of-love?hsLang=en\",\n",
    "        \"https://pittsburghopera.org/season/past-seasons/moby-dick?hsLang=en\",\n",
    "        \"https://pittsburghopera.org/season/past-seasons/the-marriage-of-figaro?hsLang=en\",\n",
    "        \"https://pittsburghopera.org/season/past-seasons/tosca-2017?hsLang=en\",\n",
    "        \"https://pittsburghopera.org/season/past-seasons/the-summer-king?hsLang=en\",\n",
    "        \"https://pittsburghopera.org/season/past-seasons/turandot?hsLang=en\",\n",
    "        \"https://pittsburghopera.org/season/past-seasons/as-one?hsLang=en\",\n",
    "        \"https://pittsburghopera.org/season/past-seasons/richard-the-lionheart?hsLang=en\",\n",
    "        \"https://pittsburghopera.org/season/past-seasons/salome?hsLang=en\",\n",
    "        \"https://pittsburghopera.org/season/past-seasons/la-traviata-2016?hsLang=en\",\n",
    "        \"https://pittsburghopera.org/season/past-seasons/the-rakes-progress?hsLang=en\",\n",
    "        \"https://pittsburghopera.org/season/past-seasons/twenty-seven?hsLang=en\",\n",
    "        \"https://pittsburghopera.org/season/past-seasons/the-barber-of-seville-2016?hsLang=en\",\n",
    "        \"https://pittsburghopera.org/season/past-seasons/little-women?hsLang=en\",\n",
    "        \"https://pittsburghopera.org/season/past-seasons/cos%C3%AC-fan-tutte-2015?hsLang=en\",\n",
    "        \"https://pittsburghopera.org/season/past-seasons/nabucco?hsLang=en\",\n",
    "        \"https://pittsburghopera.org/season/free-low-cost-events\",\n",
    "        \"https://pittsburghopera.org/season/free-low-cost-events/songshop\",\n",
    "        \"https://pittsburghopera.org/season/free-low-cost-events/opera-up-close\",\n",
    "        \"https://pittsburghopera.org/season/free-low-cost-events/pre-opera-talks\",\n",
    "        \"https://pittsburghopera.org/season/free-low-cost-events/meet-the-artists\",\n",
    "        \"https://pittsburghopera.org/season/free-low-cost-events/community-concerts\",\n",
    "        \"https://pittsburghopera.org/season/free-low-cost-events/allegheny-county-summer-concert-series\",\n",
    "        \"https://pittsburghopera.org/season/free-low-cost-events/wqed-broadcasts\",\n",
    "        \"https://pittsburghopera.org/season/special-events\",\n",
    "        \"https://pittsburghopera.org/season/special-events/diamond-horseshoe-ball\",\n",
    "        \"https://pittsburghopera.org/season/special-events/pittsburgh-opera-fashion-event\",\n",
    "        \"https://pittsburghopera.org/season/special-events/maecenas\",\n",
    "        \"https://pittsburghopera.org/resident-artists/2024-25resident-artists\",\n",
    "        \"https://pittsburghopera.org/resident-artists/faculty-administration/\",\n",
    "        \"https://pittsburghopera.org/resident-artists/auditions/\",\n",
    "        \"https://pittsburghopera.org/resident-artists/history-alumni/\",\n",
    "        \"https://pittsburghopera.org/our-team/orchestra\",\n",
    "        \"https://pittsburghopera.org/our-team/chorus\",\n",
    "        \"https://pittsburghopera.org/facilities/pittsburgh-opera-headquarters/\",\n",
    "        \"https://pittsburghopera.org/facilities/office-hours\",\n",
    "        \"https://pittsburghopera.org/facilities/production-rentals\",\n",
    "        \"https://pittsburghopera.org/facilities/hold-your-event-at-pittsburgh-opera/\"\n",
    "    ]\n",
    "\n",
    "    scraped_data = scrape_links(scraper_, links)\n",
    "    \n",
    "    save_to_json(scraped_data, \"../raw_documents/Pittsburgh_Opera.json\")\n",
    "\n",
    "    scraper_.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching: https://pittsburghopera.org/\n",
      "Fetching: https://pittsburghopera.org/about/mission-history\n",
      "Fetching: https://pittsburghopera.org/about/inclusion-diversity-equity-accessibility-idea/\n",
      "Fetching: https://pittsburghopera.org/calendar?timequery=week&prev=71+&start=1722484800000&end=1725145140000\n",
      "Fetching: https://pittsburghopera.org/calendar?timequery=week&prev=55+&start=1722484800000&end=1725145140000https://pittsburghopera.org/calendar?timequery=week&prev=47+&start=1722484800000&end=1725145140000\n",
      "Fetching: https://pittsburghopera.org/calendar?timequery=week&prev=39+&start=1722484800000&end=1725145140000\n",
      "Fetching: https://pittsburghopera.org/calendar?timequery=week&prev=31+&start=1722484800000&end=1725145140000\n",
      "Fetching: https://pittsburghopera.org/calendar?timequery=week&prev=23+&start=1722484800000&end=1725145140000\n",
      "Fetching: https://pittsburghopera.org/calendar?timequery=week&prev=15+&start=1722484800000&end=1725145140000\n",
      "Fetching: https://pittsburghopera.org/calendar?timequery=week&prev=7+&start=1722484800000&end=1725145140000\n",
      "Fetching: https://pittsburghopera.org/calendar?timequery=week&prev=-1+&start=1722484800000&end=1725145140000\n",
      "Fetching: https://pittsburghopera.org/calendar?timequery=week&prev=-9+&start=1722484800000&end=1725145140000\n",
      "Fetching: https://pittsburghopera.org/calendar?timequery=week&prev=-17+&start=1722484800000&end=1725145140000\n",
      "Fetching: https://pittsburghopera.org/calendar?timequery=week&prev=-25+&start=1722484800000&end=1725145140000\n",
      "Fetching: https://pittsburghopera.org/calendar?timequery=week&prev=-33+&start=1722484800000&end=1725145140000\n",
      "Fetching: https://pittsburghopera.org/calendar?timequery=week&prev=-41+&start=1722484800000&end=17251451400000\n",
      "Fetching: https://pittsburghopera.org/calendar?timequery=week&prev=-57+&start=1722484800000&end=1725145140000\n",
      "Fetching: https://pittsburghopera.org/calendar?timequery=week&prev=-97+&start=1722484800000&end=1725145140000\n",
      "Fetching: https://pittsburghopera.org/calendar?timequery=week&prev=-105+&start=1722484800000&end=1725145140000\n",
      "Fetching: https://pittsburghopera.org/calendar?timequery=week&prev=-113+&start=1722484800000&end=1725145140000\n",
      "Fetching: https://pittsburghopera.org/calendar?timequery=week&prev=-121+&start=1722484800000&end=1725145140000\n",
      "Fetching: https://pittsburghopera.org/calendar?timequery=week&prev=-129+&start=1722484800000&end=1725145140000\n",
      "Fetching: https://pittsburghopera.org/calendar?timequery=week&prev=-145+&start=1722484800000&end=1725145140000\n",
      "Fetching: https://pittsburghopera.org/calendar?timequery=week&prev=-153+&start=1722484800000&end=1725145140000\n",
      "Fetching: https://pittsburghopera.org/calendar?timequery=week&prev=-161+&start=1722484800000&end=1725145140000\n",
      "Fetching: https://pittsburghopera.org/calendar?timequery=week&prev=-169+&start=1722484800000&end=1725145140000\n",
      "Fetching: https://pittsburghopera.org/calendar?timequery=month&prev=-169+&start=1743480000000&end=1746053940000\n",
      "Fetching: https://pittsburghopera.org/calendar?timequery=month&prev=-169+&start=1746072000000&end=1748732340000\n",
      "Fetching: https://pittsburghopera.org/season/tosca\n",
      "Fetching: https://pittsburghopera.org/season/cavalleria-rusticana-pagliacci\n",
      "Fetching: https://pittsburghopera.org/season/armida\n",
      "Fetching: https://pittsburghopera.org/season/madama-butterflyhttps://pittsburghopera.org/season/woman-with-eyes-closed\n",
      "Fetching: https://pittsburghopera.org/season/past-seasons\n",
      "Fetching: https://pittsburghopera.org/season/the-barber-of-seville?hsLang=en\n",
      "Fetching: https://pittsburghopera.org/season/the-flying-dutchman?hsLang=en\n",
      "Fetching: https://pittsburghopera.org/season/iphigenie-en-tauride?hsLang=en\n",
      "Fetching: https://pittsburghopera.org/season/proving-up?hsLang=en\n",
      "Fetching: https://pittsburghopera.org/season/la-traviata?hsLang=en\n",
      "Fetching: https://pittsburghopera.org/season/the-passion-of-mary-cardwell-dawson?hsLang=en\n",
      "Fetching: https://pittsburghopera.org/season/past-seasons/rusalka?hsLang=en\n",
      "Fetching: https://pittsburghopera.org/season/past-seasons/the-marriage-of-figaro?hsLang=en\n",
      "Fetching: https://pittsburghopera.org/season/past-seasons/ariodante?hsLang=en\n",
      "Fetching: https://pittsburghopera.org/season/past-seasons/il-trovatore?hsLang=en\n",
      "Fetching: https://pittsburghopera.org/season/past-seasons/denis-katya?hsLang=en\n",
      "Fetching: https://pittsburghopera.org/season/past-seasons/we-shall-not-be-moved?hsLang=en\n",
      "Fetching: https://pittsburghopera.org/season/past-seasons/the-magic-flute?hsLang=en\n",
      "Fetching: https://pittsburghopera.org/season/past-seasons/the-rose-elf?hsLang=en\n",
      "Fetching: https://pittsburghopera.org/season/past-seasons/in-a-grove?hsLang=en\n",
      "Fetching: https://pittsburghopera.org/season/past-seasons/carmen?hsLang=en\n",
      "Fetching: https://pittsburghopera.org/season/past-seasons/blue?hsLang=enhttps://pittsburghopera.org/season/past-seasons/semele?hsLang=en\n",
      "Fetching: https://pittsburghopera.org/season/past-seasons/cosi-fan-tutte?hsLang=en\n",
      "Fetching: https://pittsburghopera.org/season/past-seasons/soldier-songs?hsLang=en\n",
      "Fetching: https://pittsburghopera.org/season/past-seasons/charlie-parkers-yardbird?hsLang=en\n",
      "Fetching: https://pittsburghopera.org/season/past-seasons/last-american-hammer?hsLang=en\n",
      "Fetching: https://pittsburghopera.org/season/past-seasons/alcina?hsLang=en\n",
      "Fetching: https://pittsburghopera.org/season/past-seasons/florencia-en-el-amazonas?hsLang=en\n",
      "Fetching: https://pittsburghopera.org/season/past-seasons/don-giovanni?hsLang=en\n",
      "Fetching: https://pittsburghopera.org/season/past-seasons/madama-butterfly?hsLang=en\n",
      "Fetching: https://pittsburghopera.org/season/past-seasons/afterwards?hsLang=en\n",
      "Fetching: https://pittsburghopera.org/season/past-seasons/glory-denied?hsLang=en\n",
      "Fetching: https://pittsburghopera.org/season/past-seasons/la-boheme?hsLang=en\n",
      "Fetching: https://pittsburghopera.org/season/past-seasons/don-pasquale?hsLang=en\n",
      "Fetching: https://pittsburghopera.org/season/past-seasons/hansel-gretel?hsLang=en\n",
      "Fetching: https://pittsburghopera.org/season/past-seasons/savage-winter?hsLang=en\n",
      "Fetching: https://pittsburghopera.org/season/past-seasons/the-long-walk?hsLang=en\n",
      "Fetching: https://pittsburghopera.org/season/past-seasons/the-elixir-of-love?hsLang=en\n",
      "Fetching: https://pittsburghopera.org/season/past-seasons/moby-dick?hsLang=en\n",
      "Fetching: https://pittsburghopera.org/season/past-seasons/the-marriage-of-figaro?hsLang=en\n",
      "Fetching: https://pittsburghopera.org/season/past-seasons/tosca-2017?hsLang=en\n",
      "Fetching: https://pittsburghopera.org/season/past-seasons/the-summer-king?hsLang=en\n",
      "Fetching: https://pittsburghopera.org/season/past-seasons/turandot?hsLang=en\n",
      "Fetching: https://pittsburghopera.org/season/past-seasons/as-one?hsLang=en\n",
      "Fetching: https://pittsburghopera.org/season/past-seasons/richard-the-lionheart?hsLang=en\n",
      "Fetching: https://pittsburghopera.org/season/past-seasons/salome?hsLang=en\n",
      "Fetching: https://pittsburghopera.org/season/past-seasons/la-traviata-2016?hsLang=en\n",
      "Fetching: https://pittsburghopera.org/season/past-seasons/the-rakes-progress?hsLang=en\n",
      "Fetching: https://pittsburghopera.org/season/past-seasons/twenty-seven?hsLang=en\n",
      "Fetching: https://pittsburghopera.org/season/past-seasons/the-barber-of-seville-2016?hsLang=en\n",
      "Fetching: https://pittsburghopera.org/season/past-seasons/little-women?hsLang=en\n",
      "Fetching: https://pittsburghopera.org/season/past-seasons/cos%C3%AC-fan-tutte-2015?hsLang=en\n",
      "Fetching: https://pittsburghopera.org/season/past-seasons/nabucco?hsLang=en\n",
      "Fetching: https://pittsburghopera.org/season/free-low-cost-events\n",
      "Fetching: https://pittsburghopera.org/season/free-low-cost-events/songshop\n",
      "Fetching: https://pittsburghopera.org/season/free-low-cost-events/opera-up-close\n",
      "Fetching: https://pittsburghopera.org/season/free-low-cost-events/pre-opera-talks\n",
      "Fetching: https://pittsburghopera.org/season/free-low-cost-events/meet-the-artists\n",
      "Fetching: https://pittsburghopera.org/season/free-low-cost-events/community-concerts\n",
      "Fetching: https://pittsburghopera.org/season/free-low-cost-events/allegheny-county-summer-concert-series\n",
      "Fetching: https://pittsburghopera.org/season/free-low-cost-events/wqed-broadcasts\n",
      "Fetching: https://pittsburghopera.org/season/special-events\n",
      "Fetching: https://pittsburghopera.org/season/special-events/diamond-horseshoe-ball\n",
      "Fetching: https://pittsburghopera.org/season/special-events/pittsburgh-opera-fashion-event\n",
      "Fetching: https://pittsburghopera.org/season/special-events/maecenas\n",
      "Fetching: https://pittsburghopera.org/resident-artists/2024-25resident-artists\n",
      "Fetching: https://pittsburghopera.org/resident-artists/faculty-administration/\n",
      "Fetching: https://pittsburghopera.org/resident-artists/auditions/\n",
      "Fetching: https://pittsburghopera.org/resident-artists/history-alumni/\n",
      "Fetching: https://pittsburghopera.org/our-team/orchestra\n",
      "Fetching: https://pittsburghopera.org/our-team/chorus\n",
      "Fetching: https://pittsburghopera.org/facilities/pittsburgh-opera-headquarters/\n",
      "Fetching: https://pittsburghopera.org/facilities/office-hours\n",
      "Fetching: https://pittsburghopera.org/facilities/production-rentals\n",
      "Fetching: https://pittsburghopera.org/facilities/hold-your-event-at-pittsburgh-opera/\n",
      "Data saved to ../raw_documents/Pittsburgh_Opera_New.json\n"
     ]
    }
   ],
   "source": [
    "######### Add Event Name #########\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "\n",
    "class Scraper:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.chrome_options = Options()\n",
    "        self.chrome_options.add_argument(\"--headless\")\n",
    "        self.chrome_options.add_argument(\"--disable-gpu\")\n",
    "        self.chrome_options.add_argument(\"--no-sandbox\")\n",
    "        self.chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "        self.driver = webdriver.Chrome(options=self.chrome_options)\n",
    "        self.driver.set_page_load_timeout(20)  # Adjusted timeout for slower pages\n",
    "        self.current_url = None\n",
    "        self.current_domain = None\n",
    "\n",
    "    def set_domain(self, url: str):\n",
    "        \"\"\"set domain name and URL\"\"\"\n",
    "        self.current_domain = url.split(\"/\")[2]  # 例如 'www.example.com'\n",
    "        self.current_url = url\n",
    "\n",
    "    def fetch(self, url: str, raw_html: bool = False):\n",
    "        \"\"\"capture the webpage and choose whether to remove JS or CSS\"\"\"\n",
    "        soup = self.get_soup(url)\n",
    "        if not raw_html:\n",
    "            soup = self.remove_js_css(soup)\n",
    "        return soup, self.get_links(soup), self.get_title(soup)\n",
    "\n",
    "    def get_html(self, url: str):\n",
    "        \"\"\"get HTML\"\"\"\n",
    "        self.set_domain(url)  # 调用 set_domain 方法\n",
    "        self.driver.get(url)\n",
    "        return self.driver.page_source\n",
    "\n",
    "    def get_soup(self, url: str):\n",
    "        \"\"\"parse and get BeautifulSoup object\"\"\"\n",
    "        html = self.get_html(url)\n",
    "        return BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "    def get_links(self, soup: BeautifulSoup):\n",
    "        \"\"\"get external links\"\"\"\n",
    "        links = [link.get(\"href\") for link in soup.find_all(\"a\")]\n",
    "        return self.filter_links(links)\n",
    "\n",
    "    def get_title(self, soup: BeautifulSoup):\n",
    "        \"\"\"title\"\"\"\n",
    "        if soup.title is None:\n",
    "            return f\"untitled_{self.get_timestamp()}\"\n",
    "        title = soup.title.string.replace(\" \", \"_\").replace(\"/\", \"__\")\n",
    "        return title.replace(\"\\n\", \"\")\n",
    "\n",
    "    def remove_js_css(self, soup: BeautifulSoup):\n",
    "        \"\"\"remove JS or CSS\"\"\"\n",
    "        for script in soup([\"script\", \"style\"]):\n",
    "            script.extract()  # 移除所有的 <script> 和 <style> 标签\n",
    "        return soup\n",
    "\n",
    "    def remove_unnecessary_sections(self, soup: BeautifulSoup):\n",
    "        \"\"\"remove header and footer\"\"\"\n",
    "        for tag in [\"header\", \"footer\", \"nav\", \"aside\"]:\n",
    "            for element in soup.find_all(tag):\n",
    "                element.decompose()\n",
    "        return soup\n",
    "\n",
    "    def filter_links(self, links: list):\n",
    "        filtered_links = []\n",
    "        for link in links:\n",
    "            if link is None:\n",
    "                continue\n",
    "            elif link.startswith(\"#\"):\n",
    "                continue \n",
    "            elif link.startswith(\"http\"):\n",
    "                filtered_links.append(link)\n",
    "            elif link.startswith(\"//\"):\n",
    "                filtered_links.append(f\"https:{link}\")\n",
    "            elif link.startswith(\"/\"):\n",
    "                filtered_links.append(f\"https://{self.current_domain}{link}\") \n",
    "            else:\n",
    "                filtered_links.append(f\"{self.current_url}/{link}\")\n",
    "        return filtered_links\n",
    "\n",
    "    def get_timestamp(self):\n",
    "        return datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "    def close(self):\n",
    "        self.driver.quit()\n",
    "\n",
    "    def __del__(self):\n",
    "        self.close()\n",
    "\n",
    "def save_to_json(data, filename):\n",
    "    try:\n",
    "        with open(filename, 'w', encoding='utf-8') as f:\n",
    "            json.dump(data, f, ensure_ascii=False, indent=4)\n",
    "        print(f\"Data saved to {filename}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error writing to JSON file: {e}\")\n",
    "\n",
    "\n",
    "def scrape_links(scraper_, links):\n",
    "    all_data = []\n",
    "    for link in links:\n",
    "        try:\n",
    "            print(f\"Fetching: {link}\")\n",
    "            soup = scraper_.get_soup(link)\n",
    "            \n",
    "            page_data = {}\n",
    "            event_name_tag = soup.find(\"h4\")\n",
    "            event_name = event_name_tag.text.strip() if event_name_tag else \"No Event Name\"\n",
    "            page_data[\"event_name\"] = event_name  # 存储事件名称\n",
    "\n",
    "            soup = scraper_.remove_js_css(soup)\n",
    "            soup = scraper_.remove_unnecessary_sections(soup)\n",
    "\n",
    "            content_data = organize_content_by_heading(soup)\n",
    "            page_data.update(content_data)\n",
    "            page_data[\"url\"] = link  \n",
    "\n",
    "            all_data.append(page_data)\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching {link}: {e}\")\n",
    "    \n",
    "    return all_data\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    scraper_ = Scraper()\n",
    "\n",
    "    links = [\n",
    "        \"https://pittsburghopera.org/\",\n",
    "        \"https://pittsburghopera.org/about/mission-history\",\n",
    "        \"https://pittsburghopera.org/about/inclusion-diversity-equity-accessibility-idea/\",\n",
    "        \"https://pittsburghopera.org/calendar?timequery=week&prev=71+&start=1722484800000&end=1725145140000\",\n",
    "        \"https://pittsburghopera.org/calendar?timequery=week&prev=55+&start=1722484800000&end=1725145140000\"\n",
    "        \"https://pittsburghopera.org/calendar?timequery=week&prev=47+&start=1722484800000&end=1725145140000\",\n",
    "        \"https://pittsburghopera.org/calendar?timequery=week&prev=39+&start=1722484800000&end=1725145140000\",\n",
    "        \"https://pittsburghopera.org/calendar?timequery=week&prev=31+&start=1722484800000&end=1725145140000\",\n",
    "        \"https://pittsburghopera.org/calendar?timequery=week&prev=23+&start=1722484800000&end=1725145140000\",\n",
    "        \"https://pittsburghopera.org/calendar?timequery=week&prev=15+&start=1722484800000&end=1725145140000\",\n",
    "        \"https://pittsburghopera.org/calendar?timequery=week&prev=7+&start=1722484800000&end=1725145140000\",\n",
    "        \"https://pittsburghopera.org/calendar?timequery=week&prev=-1+&start=1722484800000&end=1725145140000\",\n",
    "        \"https://pittsburghopera.org/calendar?timequery=week&prev=-9+&start=1722484800000&end=1725145140000\",\n",
    "        \"https://pittsburghopera.org/calendar?timequery=week&prev=-17+&start=1722484800000&end=1725145140000\",\n",
    "        \"https://pittsburghopera.org/calendar?timequery=week&prev=-25+&start=1722484800000&end=1725145140000\",\n",
    "        \"https://pittsburghopera.org/calendar?timequery=week&prev=-33+&start=1722484800000&end=1725145140000\",\n",
    "        \"https://pittsburghopera.org/calendar?timequery=week&prev=-41+&start=1722484800000&end=17251451400000\",\n",
    "        \"https://pittsburghopera.org/calendar?timequery=week&prev=-57+&start=1722484800000&end=1725145140000\",\n",
    "        \"https://pittsburghopera.org/calendar?timequery=week&prev=-97+&start=1722484800000&end=1725145140000\",\n",
    "        \"https://pittsburghopera.org/calendar?timequery=week&prev=-105+&start=1722484800000&end=1725145140000\",\n",
    "        \"https://pittsburghopera.org/calendar?timequery=week&prev=-113+&start=1722484800000&end=1725145140000\",\n",
    "        \"https://pittsburghopera.org/calendar?timequery=week&prev=-121+&start=1722484800000&end=1725145140000\",\n",
    "        \"https://pittsburghopera.org/calendar?timequery=week&prev=-129+&start=1722484800000&end=1725145140000\",\n",
    "        \"https://pittsburghopera.org/calendar?timequery=week&prev=-145+&start=1722484800000&end=1725145140000\",\n",
    "        \"https://pittsburghopera.org/calendar?timequery=week&prev=-153+&start=1722484800000&end=1725145140000\",\n",
    "        \"https://pittsburghopera.org/calendar?timequery=week&prev=-161+&start=1722484800000&end=1725145140000\",\n",
    "        \"https://pittsburghopera.org/calendar?timequery=week&prev=-169+&start=1722484800000&end=1725145140000\",\n",
    "        \"https://pittsburghopera.org/calendar?timequery=month&prev=-169+&start=1743480000000&end=1746053940000\",\n",
    "        \"https://pittsburghopera.org/calendar?timequery=month&prev=-169+&start=1746072000000&end=1748732340000\",\n",
    "        \"https://pittsburghopera.org/season/tosca\",\n",
    "        \"https://pittsburghopera.org/season/cavalleria-rusticana-pagliacci\",\n",
    "        \"https://pittsburghopera.org/season/armida\",\n",
    "        \"https://pittsburghopera.org/season/madama-butterfly\"\n",
    "        \"https://pittsburghopera.org/season/woman-with-eyes-closed\",\n",
    "        \"https://pittsburghopera.org/season/past-seasons\",\n",
    "        \"https://pittsburghopera.org/season/the-barber-of-seville?hsLang=en\",\n",
    "        \"https://pittsburghopera.org/season/the-flying-dutchman?hsLang=en\",\n",
    "        \"https://pittsburghopera.org/season/iphigenie-en-tauride?hsLang=en\",\n",
    "        \"https://pittsburghopera.org/season/proving-up?hsLang=en\",\n",
    "        \"https://pittsburghopera.org/season/la-traviata?hsLang=en\",\n",
    "        \"https://pittsburghopera.org/season/the-passion-of-mary-cardwell-dawson?hsLang=en\",\n",
    "        \"https://pittsburghopera.org/season/past-seasons/rusalka?hsLang=en\",\n",
    "        \"https://pittsburghopera.org/season/past-seasons/the-marriage-of-figaro?hsLang=en\",\n",
    "        \"https://pittsburghopera.org/season/past-seasons/ariodante?hsLang=en\",\n",
    "        \"https://pittsburghopera.org/season/past-seasons/il-trovatore?hsLang=en\",\n",
    "        \"https://pittsburghopera.org/season/past-seasons/denis-katya?hsLang=en\",\n",
    "        \"https://pittsburghopera.org/season/past-seasons/we-shall-not-be-moved?hsLang=en\",\n",
    "        \"https://pittsburghopera.org/season/past-seasons/the-magic-flute?hsLang=en\",\n",
    "        \"https://pittsburghopera.org/season/past-seasons/the-rose-elf?hsLang=en\",\n",
    "        \"https://pittsburghopera.org/season/past-seasons/in-a-grove?hsLang=en\",\n",
    "        \"https://pittsburghopera.org/season/past-seasons/carmen?hsLang=en\",\n",
    "        \"https://pittsburghopera.org/season/past-seasons/blue?hsLang=en\"\n",
    "        \"https://pittsburghopera.org/season/past-seasons/semele?hsLang=en\",\n",
    "        \"https://pittsburghopera.org/season/past-seasons/cosi-fan-tutte?hsLang=en\",\n",
    "        \"https://pittsburghopera.org/season/past-seasons/soldier-songs?hsLang=en\",\n",
    "        \"https://pittsburghopera.org/season/past-seasons/charlie-parkers-yardbird?hsLang=en\",\n",
    "        \"https://pittsburghopera.org/season/past-seasons/last-american-hammer?hsLang=en\",\n",
    "        \"https://pittsburghopera.org/season/past-seasons/alcina?hsLang=en\",\n",
    "        \"https://pittsburghopera.org/season/past-seasons/florencia-en-el-amazonas?hsLang=en\",\n",
    "        \"https://pittsburghopera.org/season/past-seasons/don-giovanni?hsLang=en\",\n",
    "        \"https://pittsburghopera.org/season/past-seasons/madama-butterfly?hsLang=en\",\n",
    "        \"https://pittsburghopera.org/season/past-seasons/afterwards?hsLang=en\",\n",
    "        \"https://pittsburghopera.org/season/past-seasons/glory-denied?hsLang=en\",\n",
    "        \"https://pittsburghopera.org/season/past-seasons/la-boheme?hsLang=en\",\n",
    "        \"https://pittsburghopera.org/season/past-seasons/don-pasquale?hsLang=en\",\n",
    "        \"https://pittsburghopera.org/season/past-seasons/hansel-gretel?hsLang=en\",\n",
    "        \"https://pittsburghopera.org/season/past-seasons/savage-winter?hsLang=en\",\n",
    "        \"https://pittsburghopera.org/season/past-seasons/the-long-walk?hsLang=en\",\n",
    "        \"https://pittsburghopera.org/season/past-seasons/the-elixir-of-love?hsLang=en\",\n",
    "        \"https://pittsburghopera.org/season/past-seasons/moby-dick?hsLang=en\",\n",
    "        \"https://pittsburghopera.org/season/past-seasons/the-marriage-of-figaro?hsLang=en\",\n",
    "        \"https://pittsburghopera.org/season/past-seasons/tosca-2017?hsLang=en\",\n",
    "        \"https://pittsburghopera.org/season/past-seasons/the-summer-king?hsLang=en\",\n",
    "        \"https://pittsburghopera.org/season/past-seasons/turandot?hsLang=en\",\n",
    "        \"https://pittsburghopera.org/season/past-seasons/as-one?hsLang=en\",\n",
    "        \"https://pittsburghopera.org/season/past-seasons/richard-the-lionheart?hsLang=en\",\n",
    "        \"https://pittsburghopera.org/season/past-seasons/salome?hsLang=en\",\n",
    "        \"https://pittsburghopera.org/season/past-seasons/la-traviata-2016?hsLang=en\",\n",
    "        \"https://pittsburghopera.org/season/past-seasons/the-rakes-progress?hsLang=en\",\n",
    "        \"https://pittsburghopera.org/season/past-seasons/twenty-seven?hsLang=en\",\n",
    "        \"https://pittsburghopera.org/season/past-seasons/the-barber-of-seville-2016?hsLang=en\",\n",
    "        \"https://pittsburghopera.org/season/past-seasons/little-women?hsLang=en\",\n",
    "        \"https://pittsburghopera.org/season/past-seasons/cos%C3%AC-fan-tutte-2015?hsLang=en\",\n",
    "        \"https://pittsburghopera.org/season/past-seasons/nabucco?hsLang=en\",\n",
    "        \"https://pittsburghopera.org/season/free-low-cost-events\",\n",
    "        \"https://pittsburghopera.org/season/free-low-cost-events/songshop\",\n",
    "        \"https://pittsburghopera.org/season/free-low-cost-events/opera-up-close\",\n",
    "        \"https://pittsburghopera.org/season/free-low-cost-events/pre-opera-talks\",\n",
    "        \"https://pittsburghopera.org/season/free-low-cost-events/meet-the-artists\",\n",
    "        \"https://pittsburghopera.org/season/free-low-cost-events/community-concerts\",\n",
    "        \"https://pittsburghopera.org/season/free-low-cost-events/allegheny-county-summer-concert-series\",\n",
    "        \"https://pittsburghopera.org/season/free-low-cost-events/wqed-broadcasts\",\n",
    "        \"https://pittsburghopera.org/season/special-events\",\n",
    "        \"https://pittsburghopera.org/season/special-events/diamond-horseshoe-ball\",\n",
    "        \"https://pittsburghopera.org/season/special-events/pittsburgh-opera-fashion-event\",\n",
    "        \"https://pittsburghopera.org/season/special-events/maecenas\",\n",
    "        \"https://pittsburghopera.org/resident-artists/2024-25resident-artists\",\n",
    "        \"https://pittsburghopera.org/resident-artists/faculty-administration/\",\n",
    "        \"https://pittsburghopera.org/resident-artists/auditions/\",\n",
    "        \"https://pittsburghopera.org/resident-artists/history-alumni/\",\n",
    "        \"https://pittsburghopera.org/our-team/orchestra\",\n",
    "        \"https://pittsburghopera.org/our-team/chorus\",\n",
    "        \"https://pittsburghopera.org/facilities/pittsburgh-opera-headquarters/\",\n",
    "        \"https://pittsburghopera.org/facilities/office-hours\",\n",
    "        \"https://pittsburghopera.org/facilities/production-rentals\",\n",
    "        \"https://pittsburghopera.org/facilities/hold-your-event-at-pittsburgh-opera/\"\n",
    "    ]\n",
    "\n",
    "    scraped_data = scrape_links(scraper_, links)\n",
    "    \n",
    "    save_to_json(scraped_data, \"../raw_documents/Pittsburgh_Opera_New.json\")\n",
    "\n",
    "    scraper_.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned data saved to ../raw_documents/Pittsburgh_Opera_Cleaned.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "def clean_text(text: str):\n",
    "    \"\"\"清理文本中的多余换行符和空白字符\"\"\"\n",
    "    # 替换多个连续的换行符为单个空格\n",
    "    cleaned_text = re.sub(r'\\n+', ' ', text)\n",
    "    # 移除头尾多余的空白字符\n",
    "    return cleaned_text.strip()\n",
    "\n",
    "def clean_json_file(input_file, output_file):\n",
    "    \"\"\"加载JSON文件，清理内容后保存\"\"\"\n",
    "    try:\n",
    "        # 读取现有的 JSON 文件\n",
    "        with open(input_file, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        # 遍历每一项并清理内容\n",
    "        for entry in data:\n",
    "            if 'content' in entry:\n",
    "                entry['content'] = clean_text(entry['content'])\n",
    "\n",
    "        # 将清理后的数据写入新的 JSON 文件\n",
    "        with open(output_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "        print(f\"Cleaned data saved to {output_file}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing JSON file: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_file = \"../raw_documents/Pittsburgh_Opera.json\"  # 现有的JSON文件路径\n",
    "    output_file = \"../raw_documents/Pittsburgh_Opera_Cleaned.json\"  # 清理后的JSON文件路径\n",
    "\n",
    "    # 调用清理函数\n",
    "    clean_json_file(input_file, output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cultural Trust\n",
    "### https://trustarts.org/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping page: https://trustarts.org/\n",
      "Scraping page: https://trustarts.org/calendar?utf8=%E2%9C%93&utf8=%E2%9C%93&genre=All+Genres&organization_id=&start_date=&end_date=2017%2F06%2F14&filter%5Bmin%5D=2024-10-15T13%3A07%3A06-04%3A00&filter%5Bmax%5D=2026-04-15+13%3A07%3A06+-0400&filter%5Bcurrent_page%5D=production\n",
      "Error parsing event: 'NoneType' object has no attribute 'get_text'\n",
      "Error parsing event: 'NoneType' object has no attribute 'get_text'\n",
      "Error parsing event: 'NoneType' object has no attribute 'get_text'\n",
      "Error parsing event: 'NoneType' object has no attribute 'get_text'\n",
      "Error parsing event: 'NoneType' object has no attribute 'get_text'\n",
      "Scraping page: https://trustarts.org/calendar?end_date=722&genre=&order_by=production&page=2\n",
      "Error parsing event: 'NoneType' object has no attribute 'get_text'\n",
      "Error parsing event: 'NoneType' object has no attribute 'get_text'\n",
      "Error parsing event: 'NoneType' object has no attribute 'get_text'\n",
      "Error parsing event: 'NoneType' object has no attribute 'get_text'\n",
      "Error parsing event: 'NoneType' object has no attribute 'get_text'\n",
      "Scraping page: https://trustarts.org/calendar?ad=40&am=broad&askid=2f811b4c-704f-4054-a821-1b9934816698-0-ab_msb&l=sem&o=22837&page=3&q=Byham+Theater+Pittsburgh&qsrc=999\n",
      "Error parsing event: 'NoneType' object has no attribute 'get_text'\n",
      "Error parsing event: 'NoneType' object has no attribute 'get_text'\n",
      "Error parsing event: 'NoneType' object has no attribute 'get_text'\n",
      "Error parsing event: 'NoneType' object has no attribute 'get_text'\n",
      "Error parsing event: 'NoneType' object has no attribute 'get_text'\n",
      "Scraping page: https://trustarts.org/calendar?cluid=3794577&page=4\n",
      "Error parsing event: 'NoneType' object has no attribute 'get_text'\n",
      "Error parsing event: 'NoneType' object has no attribute 'get_text'\n",
      "Error parsing event: 'NoneType' object has no attribute 'get_text'\n",
      "Error parsing event: 'NoneType' object has no attribute 'get_text'\n",
      "Error parsing event: 'NoneType' object has no attribute 'get_text'\n",
      "Scraping page: https://trustarts.org/calendar?end_date=106&genre=&page=5\n",
      "Error parsing event: 'NoneType' object has no attribute 'get_text'\n",
      "Error parsing event: 'NoneType' object has no attribute 'get_text'\n",
      "Error parsing event: 'NoneType' object has no attribute 'get_text'\n",
      "Error parsing event: 'NoneType' object has no attribute 'get_text'\n",
      "Error parsing event: 'NoneType' object has no attribute 'get_text'\n",
      "Scraping page: https://trustarts.org/calendar?cid=Tess_Order&cluid=294&page=6\n",
      "Error parsing event: 'NoneType' object has no attribute 'get_text'\n",
      "Error parsing event: 'NoneType' object has no attribute 'get_text'\n",
      "Error parsing event: 'NoneType' object has no attribute 'get_text'\n",
      "Error parsing event: 'NoneType' object has no attribute 'get_text'\n",
      "Error parsing event: 'NoneType' object has no attribute 'get_text'\n",
      "Scraping page: https://trustarts.org/calendar?ad=102&am=532&an=msn_s&l=sem&o=22837&page=7\n",
      "Error parsing event: 'NoneType' object has no attribute 'get_text'\n",
      "Error parsing event: 'NoneType' object has no attribute 'get_text'\n",
      "Error parsing event: 'NoneType' object has no attribute 'get_text'\n",
      "Error parsing event: 'NoneType' object has no attribute 'get_text'\n",
      "Error parsing event: 'NoneType' object has no attribute 'get_text'\n",
      "Scraping page: https://trustarts.org/calendar?end_date=364&order_by=production&page=8\n",
      "Error parsing event: 'NoneType' object has no attribute 'get_text'\n",
      "Error parsing event: 'NoneType' object has no attribute 'get_text'\n",
      "Error parsing event: 'NoneType' object has no attribute 'get_text'\n",
      "Error parsing event: 'NoneType' object has no attribute 'get_text'\n",
      "Error parsing event: 'NoneType' object has no attribute 'get_text'\n",
      "Scraping page: https://trustarts.org/calendar?ad=102&am=532&an=324&l=sem&page=9&q=594&qsrc=999\n",
      "Error parsing event: 'NoneType' object has no attribute 'get_text'\n",
      "Error parsing event: 'NoneType' object has no attribute 'get_text'\n",
      "Error parsing event: 'NoneType' object has no attribute 'get_text'\n",
      "Error parsing event: 'NoneType' object has no attribute 'get_text'\n",
      "Error parsing event: 'NoneType' object has no attribute 'get_text'\n",
      "Scraping page: https://trustarts.org/calendar?ad=172&am=broad&an=msn_s&l=sem&o=22837&page=10&q=Pittsburgh%2BCultural%2BTrust&qsrc=999\n",
      "Error parsing event: 'NoneType' object has no attribute 'get_text'\n",
      "Error parsing event: 'NoneType' object has no attribute 'get_text'\n",
      "Error parsing event: 'NoneType' object has no attribute 'get_text'\n",
      "Error parsing event: 'NoneType' object has no attribute 'get_text'\n",
      "Error parsing event: 'NoneType' object has no attribute 'get_text'\n",
      "Scraping page: https://trustarts.org/calendar?end_date=&genre=All+Genres&order_by=production&page=11\n",
      "Error parsing event: 'NoneType' object has no attribute 'get_text'\n",
      "Error parsing event: 'NoneType' object has no attribute 'get_text'\n",
      "Error parsing event: 'NoneType' object has no attribute 'get_text'\n",
      "Error parsing event: 'NoneType' object has no attribute 'get_text'\n",
      "Error parsing event: 'NoneType' object has no attribute 'get_text'\n",
      "Scraping page: https://trustarts.org/calendar?cid=tess_order&cluid=830&page=12\n",
      "Error parsing event: 'NoneType' object has no attribute 'get_text'\n",
      "Error parsing event: 'NoneType' object has no attribute 'get_text'\n",
      "Error parsing event: 'NoneType' object has no attribute 'get_text'\n",
      "Error parsing event: 'NoneType' object has no attribute 'get_text'\n",
      "Error parsing event: 'NoneType' object has no attribute 'get_text'\n",
      "Scraping page: https://trustarts.org/calendar?cid=tess_order&cluid=830&page=13\n",
      "Error parsing event: 'NoneType' object has no attribute 'get_text'\n",
      "Error parsing event: 'NoneType' object has no attribute 'get_text'\n",
      "Error parsing event: 'NoneType' object has no attribute 'get_text'\n",
      "Error parsing event: 'NoneType' object has no attribute 'get_text'\n",
      "Error parsing event: 'NoneType' object has no attribute 'get_text'\n",
      "Scraping page: https://trustarts.org/calendar?cid=tess_order&cluid=830&page=14\n",
      "Error parsing event: 'NoneType' object has no attribute 'get_text'\n",
      "Error parsing event: 'NoneType' object has no attribute 'get_text'\n",
      "Error parsing event: 'NoneType' object has no attribute 'get_text'\n",
      "Error parsing event: 'NoneType' object has no attribute 'get_text'\n",
      "Error parsing event: 'NoneType' object has no attribute 'get_text'\n",
      "Scraping page: https://trustarts.org/calendar?cid=12&page=15\n",
      "Error parsing event: 'NoneType' object has no attribute 'get_text'\n",
      "Error parsing event: 'NoneType' object has no attribute 'get_text'\n",
      "Error parsing event: 'NoneType' object has no attribute 'get_text'\n",
      "Error parsing event: 'NoneType' object has no attribute 'get_text'\n",
      "Error parsing event: 'NoneType' object has no attribute 'get_text'\n",
      "Scraping page: https://trustarts.org/calendar?ad=102&page=16&q=Benedum%2BCenter&qsrc=274\n",
      "Error parsing event: 'NoneType' object has no attribute 'get_text'\n",
      "Error parsing event: 'NoneType' object has no attribute 'get_text'\n",
      "Error parsing event: 'NoneType' object has no attribute 'get_text'\n",
      "Error parsing event: 'NoneType' object has no attribute 'get_text'\n",
      "Error parsing event: 'NoneType' object has no attribute 'get_text'\n",
      "Data saved to ../raw_documents/Pittsburgh_Trustarts.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "\n",
    "class Scraper:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.chrome_options = Options()\n",
    "        self.chrome_options.add_argument(\"--headless\")\n",
    "        self.chrome_options.add_argument(\"--disable-gpu\")\n",
    "        self.chrome_options.add_argument(\"--no-sandbox\")\n",
    "        self.chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "        self.driver = webdriver.Chrome(options=self.chrome_options)\n",
    "        self.driver.set_page_load_timeout(10)  # Adjusted timeout for slower pages\n",
    "        self.base_url = \"https://trustarts.org\"\n",
    "\n",
    "    def get_soup(self, url: str):\n",
    "        \"\"\"获取网页内容并解析为 BeautifulSoup 对象\"\"\"\n",
    "        self.driver.get(url)\n",
    "        time.sleep(2)  # 等待页面加载完成\n",
    "        return BeautifulSoup(self.driver.page_source, \"html.parser\")\n",
    "\n",
    "    # def fetch_event_details(self, subpage_url):\n",
    "    #     \"\"\"进入子页面获取详细信息\"\"\"\n",
    "    #     full_url = f\"{self.base_url}{subpage_url}\"\n",
    "    #     soup = self.get_soup(full_url)\n",
    "\n",
    "    #     # 获取 introduction 信息\n",
    "    #     intro_tag = soup.find('article', class_='description')\n",
    "    #     introduction = intro_tag.get_text(strip=True) if intro_tag else None\n",
    "\n",
    "    #     # 获取 address 信息\n",
    "    #     location_section = soup.find('section', class_='location')\n",
    "    #     address_tag = location_section.find('li', class_='address') if location_section else None\n",
    "    #     if address_tag:\n",
    "    #         street_address = address_tag.find('span', property='streetAddress').get_text(strip=True)\n",
    "    #         locality = address_tag.find('span', property='addressLocality').get_text(strip=True)\n",
    "    #         region = address_tag.find('span', property='addressRegion').get_text(strip=True)\n",
    "    #         postal_code = address_tag.find('span', property='postalCode').get_text(strip=True)\n",
    "    #         address = f\"{street_address}, {locality}, {region} {postal_code}\"\n",
    "    #     else:\n",
    "    #         address = None\n",
    "\n",
    "    #     return introduction, address\n",
    "\n",
    "    def scrape_event(self, soup):\n",
    "        \"\"\"爬取单个事件信息\"\"\"\n",
    "        events = []\n",
    "        event_tags = soup.find_all('article', class_='event')\n",
    "\n",
    "        for event_tag in event_tags:\n",
    "            try:\n",
    "                title = event_tag.find('h3', class_='title').get_text(strip=True)\n",
    "                date = event_tag.find('time', class_='range').get_text(strip=True)\n",
    "\n",
    "                # 获取场地和组织者信息\n",
    "                venue = event_tag.find('div', class_='venue').get_text(strip=True)\n",
    "                organization = event_tag.find('div', class_='organization').get_text(strip=True)\n",
    "\n",
    "                # 获取分类\n",
    "                categories = [cat.get_text(strip=True) for cat in event_tag.find_all('li', class_='category')]\n",
    "\n",
    "                # 获取子页面URL\n",
    "                subpage_url = event_tag.find('a')['href']\n",
    "\n",
    "                # # 进入子页面获取更多信息\n",
    "                # introduction, address = self.fetch_event_details(subpage_url)\n",
    "\n",
    "                event_data = {\n",
    "                    \"title\": title,\n",
    "                    \"date\": date,\n",
    "                    \"venue\": venue,\n",
    "                    \"organization\": organization,\n",
    "                    \"category\": categories,\n",
    "                    \"url\": f\"{self.base_url}{subpage_url}\",\n",
    "                    # \"introduction\": introduction,\n",
    "                    # \"address\": address\n",
    "                }\n",
    "\n",
    "                events.append(event_data)\n",
    "\n",
    "            except AttributeError as e:\n",
    "                print(f\"Error parsing event: {e}\")\n",
    "        return events\n",
    "\n",
    "    def scrape_events_from_pages(self, urls):\n",
    "        \"\"\"从多个页面爬取事件信息\"\"\"\n",
    "        all_events = []\n",
    "        for url in urls:\n",
    "            print(f\"Scraping page: {url}\")\n",
    "            soup = self.get_soup(url)\n",
    "            events = self.scrape_event(soup)\n",
    "            all_events.extend(events)\n",
    "        return all_events\n",
    "\n",
    "    def close(self):\n",
    "        self.driver.quit()\n",
    "\n",
    "    def __del__(self):\n",
    "        self.close()\n",
    "\n",
    "\n",
    "def save_to_json(data, filename):\n",
    "    \"\"\"保存数据到JSON文件\"\"\"\n",
    "    try:\n",
    "        with open(filename, 'w', encoding='utf-8') as f:\n",
    "            json.dump(data, f, ensure_ascii=False, indent=4)\n",
    "        print(f\"Data saved to {filename}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error writing to JSON file: {e}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    scraper = Scraper()\n",
    "\n",
    "    # 需要爬取的页面URL\n",
    "    urls = [\n",
    "        \"https://trustarts.org/\",\n",
    "        \"https://trustarts.org/calendar?utf8=%E2%9C%93&utf8=%E2%9C%93&genre=All+Genres&organization_id=&start_date=&end_date=2017%2F06%2F14&filter%5Bmin%5D=2024-10-15T13%3A07%3A06-04%3A00&filter%5Bmax%5D=2026-04-15+13%3A07%3A06+-0400&filter%5Bcurrent_page%5D=production\",\n",
    "        \"https://trustarts.org/calendar?end_date=722&genre=&order_by=production&page=2\",\n",
    "        \"https://trustarts.org/calendar?ad=40&am=broad&askid=2f811b4c-704f-4054-a821-1b9934816698-0-ab_msb&l=sem&o=22837&page=3&q=Byham+Theater+Pittsburgh&qsrc=999\",\n",
    "        \"https://trustarts.org/calendar?cluid=3794577&page=4\",\n",
    "        \"https://trustarts.org/calendar?end_date=106&genre=&page=5\",\n",
    "        \"https://trustarts.org/calendar?cid=Tess_Order&cluid=294&page=6\",\n",
    "        \"https://trustarts.org/calendar?ad=102&am=532&an=msn_s&l=sem&o=22837&page=7\",\n",
    "        \"https://trustarts.org/calendar?end_date=364&order_by=production&page=8\",\n",
    "        \"https://trustarts.org/calendar?ad=102&am=532&an=324&l=sem&page=9&q=594&qsrc=999\",\n",
    "        \"https://trustarts.org/calendar?ad=172&am=broad&an=msn_s&l=sem&o=22837&page=10&q=Pittsburgh%2BCultural%2BTrust&qsrc=999\",\n",
    "        \"https://trustarts.org/calendar?end_date=&genre=All+Genres&order_by=production&page=11\",\n",
    "        \"https://trustarts.org/calendar?cid=tess_order&cluid=830&page=12\",\n",
    "        \"https://trustarts.org/calendar?cid=tess_order&cluid=830&page=13\",\n",
    "        \"https://trustarts.org/calendar?cid=tess_order&cluid=830&page=14\",\n",
    "        \"https://trustarts.org/calendar?cid=12&page=15\",\n",
    "        \"https://trustarts.org/calendar?ad=102&page=16&q=Benedum%2BCenter&qsrc=274\"\n",
    "    ]\n",
    "\n",
    "    # 开始爬取\n",
    "    scraped_events = scraper.scrape_events_from_pages(urls)\n",
    "\n",
    "    # 保存为 JSON 文件\n",
    "    save_to_json(scraped_events, \"../raw_documents/Pittsburgh_Trustarts.json\")\n",
    "\n",
    "    # 关闭爬虫\n",
    "    scraper.close()\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     scraper_ = Scraper()\n",
    "\n",
    "#     links = [\n",
    "        \n",
    "#         \"https://trustarts.org/pct_home/events/series\",\n",
    "#         \"https://trustarts.org/pct_home/events/festivals\",\n",
    "#         \"https://trustarts.org/pct_home/events/groups\",\n",
    "#         \"https://trustarts.org/pct_home/events/university-student-tickets\",\n",
    "#         \"https://trustarts.org/pct_home/events/seating-charts\",\n",
    "#         \"https://trustarts.org/pct_home/events/faq---ticketing\",\n",
    "#         \"https://trustarts.org/pct_home/events/gift-cards\",\n",
    "#         \"https://trustarts.org/pct_home/events/official-ticket-source\",\n",
    "#         \"https://trustarts.org/pct_home/events/venue-tours\",\n",
    "#         \"https://trustarts.org/pct_home/visual-arts#current\",\n",
    "#         \"https://trustarts.org/pct_home/visual-arts#upcoming\",\n",
    "#         \"https://trustarts.org/pct_home/visual-arts#galleries\",\n",
    "#         \"https://trustarts.org/pct_home/engagement\",\n",
    "#         \"https://trustarts.org/pct_home/engagement/lullaby-project\",\n",
    "#         \"https://trustarts.org/pct_home/engagement/broadway-talk-back-series\",\n",
    "#         \"https://trustarts.org/pct_home/engagement/community-classes-with-mr-messado\",\n",
    "#         \"https://trustarts.org/pct_home/engagement/cultural-celebrations\",\n",
    "#         \"https://trustarts.org/pct_home/visit\",\n",
    "#         \"https://trustarts.org/pct_home/about\",\n",
    "           \n",
    "#     ]\n",
    "\n",
    "#     scraped_data = scrape_links(scraper_, links)\n",
    "    \n",
    "#     save_to_json(scraped_data, \"../raw_documents/Pittsburgh_Trustarts.json\")\n",
    "\n",
    "#     scraper_.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching: https://trustarts.org/pct_home/events/series\n",
      "Fetching: https://trustarts.org/pct_home/events/festivals\n",
      "Fetching: https://trustarts.org/pct_home/events/groups\n",
      "Fetching: https://trustarts.org/pct_home/events/university-student-tickets\n",
      "Fetching: https://trustarts.org/pct_home/events/seating-charts\n",
      "Fetching: https://trustarts.org/pct_home/events/faq---ticketing\n",
      "Fetching: https://trustarts.org/pct_home/events/gift-cards\n",
      "Fetching: https://trustarts.org/pct_home/events/official-ticket-source\n",
      "Fetching: https://trustarts.org/pct_home/events/venue-tours\n",
      "Fetching: https://trustarts.org/pct_home/visual-arts#current\n",
      "Fetching: https://trustarts.org/pct_home/visual-arts#upcoming\n",
      "Fetching: https://trustarts.org/pct_home/visual-arts#galleries\n",
      "Fetching: https://trustarts.org/pct_home/engagement\n",
      "Fetching: https://trustarts.org/pct_home/engagement/lullaby-project\n",
      "Fetching: https://trustarts.org/pct_home/engagement/broadway-talk-back-series\n",
      "Fetching: https://trustarts.org/pct_home/engagement/community-classes-with-mr-messado\n",
      "Fetching: https://trustarts.org/pct_home/engagement/cultural-celebrations\n",
      "Fetching: https://trustarts.org/pct_home/visit\n",
      "Fetching: https://trustarts.org/pct_home/about\n",
      "Data saved to ../raw_documents/Pittsburgh_Trustarts_copy.json\n"
     ]
    }
   ],
   "source": [
    "#################### Except events, more information ####################\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import os\n",
    "\n",
    "\n",
    "class Scraper:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.chrome_options = Options()\n",
    "        self.chrome_options.add_argument(\"--headless\")\n",
    "        self.chrome_options.add_argument(\"--disable-gpu\")\n",
    "        self.chrome_options.add_argument(\"--no-sandbox\")\n",
    "        self.chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "        self.driver = webdriver.Chrome(options=self.chrome_options)\n",
    "        self.driver.set_page_load_timeout(10)  # Adjusted timeout for slower pages\n",
    "        self.current_url = None\n",
    "        self.current_domain = None\n",
    "\n",
    "    def set_domain(self, url: str):\n",
    "        \"\"\"Set domain name and URL.\"\"\"\n",
    "        self.current_domain = url.split(\"/\")[2]  # 例如 'www.example.com'\n",
    "        self.current_url = url\n",
    "\n",
    "    def fetch(self, url: str, raw_html: bool = False):\n",
    "        \"\"\"Capture the webpage and choose whether to remove JS or CSS.\"\"\"\n",
    "        soup = self.get_soup(url)\n",
    "        if not raw_html:\n",
    "            soup = self.remove_js_css(soup)\n",
    "        return soup, self.get_links(soup), self.get_title(soup)\n",
    "\n",
    "    def get_html(self, url: str):\n",
    "        \"\"\"Get HTML.\"\"\"\n",
    "        self.set_domain(url)  # 调用 set_domain 方法\n",
    "        self.driver.get(url)\n",
    "        return self.driver.page_source\n",
    "\n",
    "    def get_soup(self, url: str):\n",
    "        \"\"\"Parse and get BeautifulSoup object.\"\"\"\n",
    "        html = self.get_html(url)\n",
    "        return BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "    def get_links(self, soup: BeautifulSoup):\n",
    "        \"\"\"Get external links.\"\"\"\n",
    "        links = [link.get(\"href\") for link in soup.find_all(\"a\")]\n",
    "        return self.filter_links(links)\n",
    "\n",
    "    def get_title(self, soup: BeautifulSoup):\n",
    "        \"\"\"Get the title.\"\"\"\n",
    "        if soup.title is None:\n",
    "            return f\"untitled_{self.get_timestamp()}\"\n",
    "        title = soup.title.string.replace(\" \", \"_\").replace(\"/\", \"__\")\n",
    "        return title.replace(\"\\n\", \"\")\n",
    "\n",
    "    def remove_js_css(self, soup: BeautifulSoup):\n",
    "        \"\"\"Remove JS and CSS.\"\"\"\n",
    "        for script in soup([\"script\", \"style\"]):\n",
    "            script.extract()  # 移除所有的 <script> 和 <style> 标签\n",
    "        return soup\n",
    "\n",
    "    def remove_unnecessary_sections(self, soup: BeautifulSoup):\n",
    "        \"\"\"Remove unnecessary sections such as header, footer, nav, etc.\"\"\"\n",
    "        for tag in [\"header\", \"footer\", \"nav\", \"aside\"]:\n",
    "            for element in soup.find_all(tag):\n",
    "                element.decompose()\n",
    "        return soup\n",
    "\n",
    "    def filter_links(self, links: list):\n",
    "        filtered_links = []\n",
    "        for link in links:\n",
    "            if link is None:\n",
    "                continue\n",
    "            elif link.startswith(\"#\"):\n",
    "                continue\n",
    "            elif link.startswith(\"http\"):\n",
    "                filtered_links.append(link)\n",
    "            elif link.startswith(\"//\"):\n",
    "                filtered_links.append(f\"https:{link}\")\n",
    "            elif link.startswith(\"/\"):\n",
    "                filtered_links.append(f\"https://{self.current_domain}{link}\")\n",
    "            else:\n",
    "                filtered_links.append(f\"{self.current_url}/{link}\")\n",
    "        return filtered_links\n",
    "\n",
    "    def get_timestamp(self):\n",
    "        return datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "    def close(self):\n",
    "        self.driver.quit()\n",
    "\n",
    "    def __del__(self):\n",
    "        self.close()\n",
    "\n",
    "\n",
    "def organize_content_by_heading(soup: BeautifulSoup):\n",
    "    \"\"\"Organize content based on headings, ensuring all relevant text is captured.\"\"\"\n",
    "    data = {}\n",
    "    current_h2, current_h3, current_h4, current_h5, current_h6 = None, None, None, None, None\n",
    "\n",
    "    # Traverse all the relevant tags including headings, paragraphs, divs, articles, spans, etc.\n",
    "    for tag in soup.find_all(['h2', 'h3', 'h4', 'h5', 'h6', 'p', 'div', 'span', 'article']):\n",
    "        if tag.name == 'h2':\n",
    "            current_h2 = tag.get_text(strip=True)\n",
    "            data[current_h2] = {}\n",
    "            current_h3 = current_h4 = current_h5 = current_h6 = None  # Reset lower-level headings\n",
    "        elif tag.name == 'h3' and current_h2:\n",
    "            current_h3 = tag.get_text(strip=True)\n",
    "            data[current_h2][current_h3] = {}\n",
    "            current_h4 = current_h5 = current_h6 = None\n",
    "        elif tag.name == 'h4' and current_h3:\n",
    "            current_h4 = tag.get_text(strip=True)\n",
    "            data[current_h2][current_h3][current_h4] = {}\n",
    "            current_h5 = current_h6 = None\n",
    "        elif tag.name == 'h5' and current_h4:\n",
    "            current_h5 = tag.get_text(strip=True)\n",
    "            data[current_h2][current_h3][current_h4][current_h5] = {}\n",
    "            current_h6 = None\n",
    "        elif tag.name == 'h6' and current_h5:\n",
    "            current_h6 = tag.get_text(strip=True)\n",
    "            data[current_h2][current_h3][current_h4][current_h5][current_h6] = {}\n",
    "        elif tag.name in ['p', 'div', 'span', 'article']:\n",
    "            # Only add content if it's not empty\n",
    "            content = tag.get_text(strip=True)\n",
    "            if content:\n",
    "                if current_h6:\n",
    "                    data[current_h2][current_h3][current_h4][current_h5][current_h6].setdefault('content', []).append(content)\n",
    "                elif current_h5:\n",
    "                    data[current_h2][current_h3][current_h4][current_h5].setdefault('content', []).append(content)\n",
    "                elif current_h4:\n",
    "                    data[current_h2][current_h3][current_h4].setdefault('content', []).append(content)\n",
    "                elif current_h3:\n",
    "                    data[current_h2][current_h3].setdefault('content', []).append(content)\n",
    "                elif current_h2:\n",
    "                    data[current_h2].setdefault('content', []).append(content)\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def save_to_json(data, filename):\n",
    "    \"\"\"Save data to a JSON file, ensuring no overwrite of existing content.\"\"\"\n",
    "    try:\n",
    "        if os.path.exists(filename):\n",
    "            # 读取现有的 JSON 文件并合并\n",
    "            with open(filename, 'r', encoding='utf-8') as f:\n",
    "                existing_data = json.load(f)\n",
    "            # 合并新的数据到现有数据\n",
    "            if isinstance(existing_data, list):\n",
    "                existing_data.extend(data)  # 如果是列表，将数据合并\n",
    "            else:\n",
    "                existing_data.update(data)  # 如果是字典，更新数据\n",
    "        else:\n",
    "            existing_data = data  # 如果文件不存在，则新建\n",
    "\n",
    "        # 保存合并后的数据到文件\n",
    "        with open(filename, 'w', encoding='utf-8') as f:\n",
    "            json.dump(existing_data, f, ensure_ascii=False, indent=4)\n",
    "        print(f\"Data saved to {filename}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error writing to JSON file: {e}\")\n",
    "\n",
    "\n",
    "def scrape_links(scraper_, links):\n",
    "    \"\"\"Scrape multiple links and return organized data.\"\"\"\n",
    "    all_data = []\n",
    "    for link in links:\n",
    "        try:\n",
    "            print(f\"Fetching: {link}\")\n",
    "            soup = scraper_.get_soup(link)\n",
    "\n",
    "            soup = scraper_.remove_js_css(soup)\n",
    "            soup = scraper_.remove_unnecessary_sections(soup)\n",
    "\n",
    "            page_data = organize_content_by_heading(soup)\n",
    "            page_data[\"url\"] = link\n",
    "\n",
    "            all_data.append(page_data)\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching {link}: {e}\")\n",
    "\n",
    "    return all_data\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    scraper_ = Scraper()\n",
    "\n",
    "    links = [\n",
    "        \"https://trustarts.org/pct_home/events/series\",\n",
    "        \"https://trustarts.org/pct_home/events/festivals\",\n",
    "        \"https://trustarts.org/pct_home/events/groups\",\n",
    "        \"https://trustarts.org/pct_home/events/university-student-tickets\",\n",
    "        \"https://trustarts.org/pct_home/events/seating-charts\",\n",
    "        \"https://trustarts.org/pct_home/events/faq---ticketing\",\n",
    "        \"https://trustarts.org/pct_home/events/gift-cards\",\n",
    "        \"https://trustarts.org/pct_home/events/official-ticket-source\",\n",
    "        \"https://trustarts.org/pct_home/events/venue-tours\",\n",
    "        \"https://trustarts.org/pct_home/visual-arts#current\",\n",
    "        \"https://trustarts.org/pct_home/visual-arts#upcoming\",\n",
    "        \"https://trustarts.org/pct_home/visual-arts#galleries\",\n",
    "        \"https://trustarts.org/pct_home/engagement\",\n",
    "        \"https://trustarts.org/pct_home/engagement/lullaby-project\",\n",
    "        \"https://trustarts.org/pct_home/engagement/broadway-talk-back-series\",\n",
    "        \"https://trustarts.org/pct_home/engagement/community-classes-with-mr-messado\",\n",
    "        \"https://trustarts.org/pct_home/engagement/cultural-celebrations\",\n",
    "        \"https://trustarts.org/pct_home/visit\",\n",
    "        \"https://trustarts.org/pct_home/about\"\n",
    "    ]\n",
    "\n",
    "    scraped_data = scrape_links(scraper_, links)\n",
    "\n",
    "    # 合并并保存到原来的文件，而不是覆盖\n",
    "    save_to_json(scraped_data, \"../raw_documents/Pittsburgh_Trustarts_copy.json\")\n",
    "\n",
    "    scraper_.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Museum\n",
    "## Carnegie Museum\n",
    "### https://carnegiemuseums.org/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching: https://carnegiemuseums.org/events/\n",
      "Fetching: https://carnegiemuseums.org/events/page/2/\n",
      "Fetching: https://carnegiemuseums.org/events/page/3/\n",
      "Fetching: https://carnegiemuseums.org/events/page/4/\n",
      "Fetching: https://carnegiemuseums.org/events/page/5/\n",
      "Data saved to ../raw_documents/Pittsburgh_Carnegie_Museums.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "class Scraper:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.chrome_options = Options()\n",
    "        self.chrome_options.add_argument(\"--headless\")\n",
    "        self.chrome_options.add_argument(\"--disable-gpu\")\n",
    "        self.chrome_options.add_argument(\"--no-sandbox\")\n",
    "        self.chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "        self.driver = webdriver.Chrome(options=self.chrome_options)\n",
    "        self.driver.set_page_load_timeout(20)  # Adjusted timeout for slower pages\n",
    "        self.current_url = None\n",
    "        self.current_domain = None\n",
    "\n",
    "    def set_domain(self, url: str):\n",
    "        \"\"\"set domain name and URL\"\"\"\n",
    "        self.current_domain = url.split(\"/\")[2]\n",
    "        self.current_url = url\n",
    "\n",
    "    def fetch(self, url: str):\n",
    "        \"\"\"capture the webpage\"\"\"\n",
    "        soup = self.get_soup(url)\n",
    "        return soup\n",
    "\n",
    "    def get_html(self, url: str):\n",
    "        \"\"\"get HTML\"\"\"\n",
    "        self.set_domain(url)\n",
    "        self.driver.get(url)\n",
    "        return self.driver.page_source\n",
    "\n",
    "    def get_soup(self, url: str):\n",
    "        \"\"\"parse and get BeautifulSoup object\"\"\"\n",
    "        html = self.get_html(url)\n",
    "        return BeautifulSoup(html, \"html.parser\")\n",
    "    \n",
    "    def remove_unnecessary_sections(self, soup: BeautifulSoup):\n",
    "        \"\"\"remove header and footer\"\"\"\n",
    "        for tag in [\"header\", \"footer\", \"nav\", \"aside\"]:\n",
    "            for element in soup.find_all(tag):\n",
    "                element.decompose()\n",
    "        return soup\n",
    "\n",
    "    def close(self):\n",
    "        self.driver.quit()\n",
    "\n",
    "    def __del__(self):\n",
    "        self.close()\n",
    "\n",
    "def scrape_links(scraper_, links):\n",
    "    all_data = []\n",
    "    for link in links:\n",
    "        try:\n",
    "            print(f\"Fetching: {link}\")\n",
    "            soup = scraper_.get_soup(link)\n",
    "\n",
    "            # 查找所有的事件卡片\n",
    "            events = soup.find_all('article', class_='event-card')\n",
    "\n",
    "            for event in events:\n",
    "                page_data = {}\n",
    "\n",
    "                # 提取事件名称\n",
    "                event_name_tag = event.find('h2')\n",
    "                event_name = event_name_tag.text.strip() if event_name_tag else \"No Event Name\"\n",
    "                page_data[\"event_name\"] = event_name\n",
    "\n",
    "                # 提取开始和结束日期\n",
    "                event_start = event.get('data-event-start', 'No Start Date')\n",
    "                event_end = event.get('data-event-end', 'No End Date')\n",
    "                page_data[\"event_start\"] = event_start\n",
    "                page_data[\"event_end\"] = event_end\n",
    "\n",
    "                # 提取场馆\n",
    "                venue_tag = event.find('a', class_='event-card__venue-tag')\n",
    "                venue = venue_tag.text.strip() if venue_tag else \"No Venue\"\n",
    "                page_data[\"venue\"] = venue\n",
    "\n",
    "                # 提取事件类型\n",
    "                event_type_tag = event.find('a', class_='event-card__event-type')\n",
    "                event_type = event_type_tag.text.strip() if event_type_tag else \"No Event Type\"\n",
    "                page_data[\"event_type\"] = event_type\n",
    "\n",
    "                # 提取是否是 Featured Event\n",
    "                featured_flag_tag = event.find('span', class_='event-card__featured-flag')\n",
    "                featured_flag = \"Featured\" if featured_flag_tag else \"Not Featured\"\n",
    "                page_data[\"featured_flag\"] = featured_flag\n",
    "\n",
    "                # 将结果添加到最终的数据列表中\n",
    "                all_data.append(page_data)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching {link}: {e}\")\n",
    "    \n",
    "    return all_data\n",
    "\n",
    "def save_to_json(data, filename):\n",
    "    try:\n",
    "        with open(filename, 'w', encoding='utf-8') as f:\n",
    "            json.dump(data, f, ensure_ascii=False, indent=4)\n",
    "        print(f\"Data saved to {filename}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error writing to JSON file: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    scraper_ = Scraper()\n",
    "\n",
    "    links = [\n",
    "        \"https://carnegiemuseums.org/events/\",\n",
    "        \"https://carnegiemuseums.org/events/page/2/\",\n",
    "        \"https://carnegiemuseums.org/events/page/3/\",\n",
    "        \"https://carnegiemuseums.org/events/page/4/\",\n",
    "        \"https://carnegiemuseums.org/events/page/5/\"\n",
    "    ]\n",
    "\n",
    "    scraped_data = scrape_links(scraper_, links)\n",
    "    \n",
    "    save_to_json(scraped_data, \"../raw_documents/Pittsburgh_Carnegie_Museums.json\")\n",
    "\n",
    "    scraper_.close()\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     scraper_ = Scraper()\n",
    "\n",
    "#     links = [\n",
    "#         # \"https://carnegiemnh.org/explore/explore/exhibitions/\",\n",
    "#         # \"https://carnegiesciencecenter.org/exhibits/\",\n",
    "#         # \"https://www.warhol.org/exhibitions/\",\n",
    "#         # \"https://www.warhol.org/calendar/?_ga=2.140397363.1042301114.1729024596-1109875255.1729024596\",\n",
    "\n",
    "#     ]\n",
    "\n",
    "#     scraped_data = scrape_links(scraper_, links)\n",
    "    \n",
    "#     save_to_json(scraped_data, \"../raw_documents/Pittsburgh_Carnegie_Museums.json\")\n",
    "\n",
    "#     scraper_.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching: https://carnegieart.org/about/our-story/\n",
      "Data appended to ../raw_documents/Pittsburgh_Carnegie_Museums.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import os\n",
    "\n",
    "\n",
    "class Scraper:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.chrome_options = Options()\n",
    "        self.chrome_options.add_argument(\"--headless\")\n",
    "        self.chrome_options.add_argument(\"--disable-gpu\")\n",
    "        self.chrome_options.add_argument(\"--no-sandbox\")\n",
    "        self.chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "        self.driver = webdriver.Chrome(options=self.chrome_options)\n",
    "        self.driver.set_page_load_timeout(20)  # Adjusted timeout for slower pages\n",
    "        self.current_url = None\n",
    "        self.current_domain = None\n",
    "\n",
    "    def set_domain(self, url: str):\n",
    "        \"\"\"set domain name and URL\"\"\"\n",
    "        self.current_domain = url.split(\"/\")[2]\n",
    "        self.current_url = url\n",
    "\n",
    "    def fetch(self, url: str):\n",
    "        \"\"\"capture the webpage\"\"\"\n",
    "        soup = self.get_soup(url)\n",
    "        return soup\n",
    "\n",
    "    def get_html(self, url: str):\n",
    "        \"\"\"get HTML\"\"\"\n",
    "        self.set_domain(url)\n",
    "        self.driver.get(url)\n",
    "        return self.driver.page_source\n",
    "\n",
    "    def get_soup(self, url: str):\n",
    "        \"\"\"parse and get BeautifulSoup object\"\"\"\n",
    "        html = self.get_html(url)\n",
    "        return BeautifulSoup(html, \"html.parser\")\n",
    "    \n",
    "    def remove_unnecessary_sections(self, soup: BeautifulSoup):\n",
    "        \"\"\"remove header and footer\"\"\"\n",
    "        for tag in [\"header\", \"footer\", \"nav\", \"aside\"]:\n",
    "            for element in soup.find_all(tag):\n",
    "                element.decompose()\n",
    "        return soup\n",
    "\n",
    "    def close(self):\n",
    "        self.driver.quit()\n",
    "\n",
    "    def __del__(self):\n",
    "        self.close()\n",
    "\n",
    "\n",
    "def scrape_page(scraper_, link):\n",
    "    try:\n",
    "        print(f\"Fetching: {link}\")\n",
    "        soup = scraper_.get_soup(link)\n",
    "\n",
    "        # 移除 header 和 footer\n",
    "        scraper_.remove_unnecessary_sections(soup)\n",
    "\n",
    "        page_data = []\n",
    "        # 提取所有的 h2 标题及其对应的正文\n",
    "        sections = soup.find_all(['h2', 'p'])\n",
    "        current_title = None\n",
    "\n",
    "        for element in sections:\n",
    "            if element.name == 'h2':\n",
    "                current_title = element.text.strip()\n",
    "            elif element.name == 'p' and current_title:\n",
    "                # 将标题和正文存入 page_data\n",
    "                page_data.append({\n",
    "                    \"title\": current_title,\n",
    "                    \"content\": element.text.strip()\n",
    "                })\n",
    "\n",
    "        return page_data\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching {link}: {e}\")\n",
    "        return []\n",
    "\n",
    "\n",
    "def append_to_json_file(data, filename):\n",
    "    try:\n",
    "        # 读取现有的 JSON 文件\n",
    "        if os.path.exists(filename):\n",
    "            with open(filename, 'r', encoding='utf-8') as f:\n",
    "                existing_data = json.load(f)\n",
    "        else:\n",
    "            existing_data = []\n",
    "\n",
    "        # 添加新数据\n",
    "        existing_data.extend(data)\n",
    "\n",
    "        # 将更新后的数据保存回文件\n",
    "        with open(filename, 'w', encoding='utf-8') as f:\n",
    "            json.dump(existing_data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "        print(f\"Data appended to {filename}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error writing to JSON file: {e}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    scraper_ = Scraper()\n",
    "\n",
    "    link = \"https://carnegieart.org/about/our-story/\"  # 你要爬取的页面链接\n",
    "\n",
    "    # 爬取页面内容\n",
    "    scraped_data = scrape_page(scraper_, link)\n",
    "\n",
    "    # 将数据追加到现有的 JSON 文件中\n",
    "    append_to_json_file(scraped_data, \"../raw_documents/Pittsburgh_Carnegie_Museums.json\")\n",
    "\n",
    "    scraper_.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching: https://carnegieart.org/art/whats-on-view/\n",
      "Data appended to ../raw_documents/Pittsburgh_Carnegie_Museums.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "class Scraper:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.chrome_options = Options()\n",
    "        self.chrome_options.add_argument(\"--headless\")\n",
    "        self.chrome_options.add_argument(\"--disable-gpu\")\n",
    "        self.chrome_options.add_argument(\"--no-sandbox\")\n",
    "        self.chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "        self.driver = webdriver.Chrome(options=self.chrome_options)\n",
    "        self.driver.set_page_load_timeout(20)  # Adjusted timeout for slower pages\n",
    "        self.current_url = None\n",
    "        self.current_domain = None\n",
    "\n",
    "    def set_domain(self, url: str):\n",
    "        \"\"\"set domain name and URL\"\"\"\n",
    "        self.current_domain = url.split(\"/\")[2]\n",
    "        self.current_url = url\n",
    "\n",
    "    def fetch(self, url: str):\n",
    "        \"\"\"capture the webpage\"\"\"\n",
    "        soup = self.get_soup(url)\n",
    "        return soup\n",
    "\n",
    "    def get_html(self, url: str):\n",
    "        \"\"\"get HTML\"\"\"\n",
    "        self.set_domain(url)\n",
    "        self.driver.get(url)\n",
    "        return self.driver.page_source\n",
    "\n",
    "    def get_soup(self, url: str):\n",
    "        \"\"\"parse and get BeautifulSoup object\"\"\"\n",
    "        html = self.get_html(url)\n",
    "        return BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "    def close(self):\n",
    "        self.driver.quit()\n",
    "\n",
    "    def __del__(self):\n",
    "        self.close()\n",
    "\n",
    "def scrape_page(scraper_, link):\n",
    "    try:\n",
    "        print(f\"Fetching: {link}\")\n",
    "        soup = scraper_.get_soup(link)\n",
    "\n",
    "        page_data = []\n",
    "        # 查找所有展览事件卡片\n",
    "        events = soup.find_all('div', class_='cmoa-grid-item')\n",
    "\n",
    "        for event in events:\n",
    "            event_data = {}\n",
    "\n",
    "            # 获取事件名称\n",
    "            event_name_tag = event.find('a', class_='font-bold')\n",
    "            event_name = event_name_tag.text.strip() if event_name_tag else \"No Event Name\"\n",
    "            event_data[\"event_name\"] = event_name\n",
    "\n",
    "            # 获取展览时间\n",
    "            time_tag = event.find('div', class_='break-words')\n",
    "            event_time = time_tag.text.strip() if time_tag else \"No Time Information\"\n",
    "            event_data[\"time\"] = event_time\n",
    "\n",
    "            # 获取展览地点\n",
    "            location_tag = event.find('ul', class_='metadata')\n",
    "            if location_tag:\n",
    "                location = location_tag.find('li').text.strip()\n",
    "            else:\n",
    "                location = \"No Location\"\n",
    "            event_data[\"location\"] = location\n",
    "\n",
    "            # 将该展览信息添加到列表中\n",
    "            page_data.append(event_data)\n",
    "\n",
    "        return page_data\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching {link}: {e}\")\n",
    "        return []\n",
    "\n",
    "def append_to_json_file(data, filename):\n",
    "    try:\n",
    "        # 检查文件是否存在\n",
    "        if os.path.exists(filename):\n",
    "            with open(filename, 'r', encoding='utf-8') as f:\n",
    "                existing_data = json.load(f)\n",
    "        else:\n",
    "            existing_data = []\n",
    "\n",
    "        # 添加新数据\n",
    "        existing_data.extend(data)\n",
    "\n",
    "        # 保存回 JSON 文件\n",
    "        with open(filename, 'w', encoding='utf-8') as f:\n",
    "            json.dump(existing_data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "        print(f\"Data appended to {filename}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error writing to JSON file: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    scraper_ = Scraper()\n",
    "\n",
    "    # 要爬取的网页链接\n",
    "    link = \"https://carnegieart.org/art/whats-on-view/\"\n",
    "\n",
    "    # 爬取页面内容\n",
    "    scraped_data = scrape_page(scraper_, link)\n",
    "\n",
    "    # 将数据追加到现有的 JSON 文件中\n",
    "    append_to_json_file(scraped_data, \"../raw_documents/Pittsburgh_Carnegie_Museums.json\")\n",
    "\n",
    "    scraper_.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching: https://carnegieart.org/art/whats-on-view/page/2/\n",
      "Data appended to ../raw_documents/Pittsburgh_Carnegie_Museums.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "class Scraper:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.chrome_options = Options()\n",
    "        self.chrome_options.add_argument(\"--headless\")\n",
    "        self.chrome_options.add_argument(\"--disable-gpu\")\n",
    "        self.chrome_options.add_argument(\"--no-sandbox\")\n",
    "        self.chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "        self.driver = webdriver.Chrome(options=self.chrome_options)\n",
    "        self.driver.set_page_load_timeout(20)  # Adjusted timeout for slower pages\n",
    "        self.current_url = None\n",
    "        self.current_domain = None\n",
    "\n",
    "    def set_domain(self, url: str):\n",
    "        \"\"\"set domain name and URL\"\"\"\n",
    "        self.current_domain = url.split(\"/\")[2]\n",
    "        self.current_url = url\n",
    "\n",
    "    def fetch(self, url: str):\n",
    "        \"\"\"capture the webpage\"\"\"\n",
    "        soup = self.get_soup(url)\n",
    "        return soup\n",
    "\n",
    "    def get_html(self, url: str):\n",
    "        \"\"\"get HTML\"\"\"\n",
    "        self.set_domain(url)\n",
    "        self.driver.get(url)\n",
    "        return self.driver.page_source\n",
    "\n",
    "    def get_soup(self, url: str):\n",
    "        \"\"\"parse and get BeautifulSoup object\"\"\"\n",
    "        html = self.get_html(url)\n",
    "        return BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "    def close(self):\n",
    "        self.driver.quit()\n",
    "\n",
    "    def __del__(self):\n",
    "        self.close()\n",
    "\n",
    "def scrape_page(scraper_, link):\n",
    "    try:\n",
    "        print(f\"Fetching: {link}\")\n",
    "        soup = scraper_.get_soup(link)\n",
    "\n",
    "        page_data = []\n",
    "        # 查找所有展览事件卡片\n",
    "        events = soup.find_all('div', class_='cmoa-grid-item')\n",
    "\n",
    "        for event in events:\n",
    "            event_data = {}\n",
    "\n",
    "            # 获取事件名称\n",
    "            event_name_tag = event.find('a', class_='font-bold')\n",
    "            event_name = event_name_tag.text.strip() if event_name_tag else \"No Event Name\"\n",
    "            event_data[\"event_name\"] = event_name\n",
    "\n",
    "            # 获取展览时间\n",
    "            time_tag = event.find('div', class_='break-words')\n",
    "            event_time = time_tag.text.strip() if time_tag else \"No Time Information\"\n",
    "            event_data[\"time\"] = event_time\n",
    "\n",
    "            # 获取展览地点\n",
    "            location_tag = event.find('ul', class_='metadata')\n",
    "            if location_tag:\n",
    "                location = location_tag.find('li').text.strip()\n",
    "            else:\n",
    "                location = \"No Location\"\n",
    "            event_data[\"location\"] = location\n",
    "\n",
    "            # 将该展览信息添加到列表中\n",
    "            page_data.append(event_data)\n",
    "\n",
    "        return page_data\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching {link}: {e}\")\n",
    "        return []\n",
    "\n",
    "def append_to_json_file(data, filename):\n",
    "    try:\n",
    "        # 检查文件是否存在\n",
    "        if os.path.exists(filename):\n",
    "            with open(filename, 'r', encoding='utf-8') as f:\n",
    "                existing_data = json.load(f)\n",
    "        else:\n",
    "            existing_data = []\n",
    "\n",
    "        # 添加新数据\n",
    "        existing_data.extend(data)\n",
    "\n",
    "        # 保存回 JSON 文件\n",
    "        with open(filename, 'w', encoding='utf-8') as f:\n",
    "            json.dump(existing_data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "        print(f\"Data appended to {filename}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error writing to JSON file: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    scraper_ = Scraper()\n",
    "\n",
    "    # 要爬取的网页链接\n",
    "    link = \"https://carnegieart.org/art/whats-on-view/page/2/\"\n",
    "\n",
    "    # 爬取页面内容\n",
    "    scraped_data = scrape_page(scraper_, link)\n",
    "\n",
    "    # 将数据追加到现有的 JSON 文件中\n",
    "    append_to_json_file(scraped_data, \"../raw_documents/Pittsburgh_Carnegie_Museums.json\")\n",
    "\n",
    "    scraper_.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching: https://carnegieart.org/art/whats-on-view/page/3/\n",
      "Data appended to ../raw_documents/Pittsburgh_Carnegie_Museums.json\n",
      "Fetching: https://carnegieart.org/art/whats-on-view/page/4/\n",
      "Data appended to ../raw_documents/Pittsburgh_Carnegie_Museums.json\n",
      "Fetching: https://carnegieart.org/art/whats-on-view/page/5/\n",
      "Data appended to ../raw_documents/Pittsburgh_Carnegie_Museums.json\n",
      "Fetching: https://carnegieart.org/art/whats-on-view/page/6/\n",
      "Data appended to ../raw_documents/Pittsburgh_Carnegie_Museums.json\n",
      "Fetching: https://carnegieart.org/art/whats-on-view/page/7/\n",
      "Data appended to ../raw_documents/Pittsburgh_Carnegie_Museums.json\n",
      "Fetching: https://carnegieart.org/art/whats-on-view/page/8/\n",
      "Data appended to ../raw_documents/Pittsburgh_Carnegie_Museums.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "class Scraper:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.chrome_options = Options()\n",
    "        self.chrome_options.add_argument(\"--headless\")\n",
    "        self.chrome_options.add_argument(\"--disable-gpu\")\n",
    "        self.chrome_options.add_argument(\"--no-sandbox\")\n",
    "        self.chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "        self.driver = webdriver.Chrome(options=self.chrome_options)\n",
    "        self.driver.set_page_load_timeout(20)  # Adjusted timeout for slower pages\n",
    "        self.current_url = None\n",
    "        self.current_domain = None\n",
    "\n",
    "    def set_domain(self, url: str):\n",
    "        \"\"\"set domain name and URL\"\"\"\n",
    "        self.current_domain = url.split(\"/\")[2]\n",
    "        self.current_url = url\n",
    "\n",
    "    def fetch(self, url: str):\n",
    "        \"\"\"capture the webpage\"\"\"\n",
    "        soup = self.get_soup(url)\n",
    "        return soup\n",
    "\n",
    "    def get_html(self, url: str):\n",
    "        \"\"\"get HTML\"\"\"\n",
    "        self.set_domain(url)\n",
    "        self.driver.get(url)\n",
    "        return self.driver.page_source\n",
    "\n",
    "    def get_soup(self, url: str):\n",
    "        \"\"\"parse and get BeautifulSoup object\"\"\"\n",
    "        html = self.get_html(url)\n",
    "        return BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "    def close(self):\n",
    "        self.driver.quit()\n",
    "\n",
    "    def __del__(self):\n",
    "        self.close()\n",
    "\n",
    "def scrape_page(scraper_, link):\n",
    "    try:\n",
    "        print(f\"Fetching: {link}\")\n",
    "        soup = scraper_.get_soup(link)\n",
    "\n",
    "        page_data = []\n",
    "        # 查找所有展览事件卡片\n",
    "        events = soup.find_all('div', class_='cmoa-grid-item')\n",
    "\n",
    "        for event in events:\n",
    "            event_data = {}\n",
    "\n",
    "            # 获取事件名称\n",
    "            event_name_tag = event.find('a', class_='font-bold')\n",
    "            event_name = event_name_tag.text.strip() if event_name_tag else \"No Event Name\"\n",
    "            event_data[\"event_name\"] = event_name\n",
    "\n",
    "            # 获取展览时间\n",
    "            time_tag = event.find('div', class_='break-words')\n",
    "            event_time = time_tag.text.strip() if time_tag else \"No Time Information\"\n",
    "            event_data[\"time\"] = event_time\n",
    "\n",
    "            # 获取展览地点\n",
    "            location_tag = event.find('ul', class_='metadata')\n",
    "            if location_tag:\n",
    "                location = location_tag.find('li').text.strip()\n",
    "            else:\n",
    "                location = \"No Location\"\n",
    "            event_data[\"location\"] = location\n",
    "\n",
    "            # 将该展览信息添加到列表中\n",
    "            page_data.append(event_data)\n",
    "\n",
    "        return page_data\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching {link}: {e}\")\n",
    "        return []\n",
    "\n",
    "def append_to_json_file(data, filename):\n",
    "    try:\n",
    "        # 检查文件是否存在\n",
    "        if os.path.exists(filename):\n",
    "            with open(filename, 'r', encoding='utf-8') as f:\n",
    "                existing_data = json.load(f)\n",
    "        else:\n",
    "            existing_data = []\n",
    "\n",
    "        # 添加新数据\n",
    "        existing_data.extend(data)\n",
    "\n",
    "        # 保存回 JSON 文件\n",
    "        with open(filename, 'w', encoding='utf-8') as f:\n",
    "            json.dump(existing_data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "        print(f\"Data appended to {filename}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error writing to JSON file: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    scraper_ = Scraper()\n",
    "\n",
    "    link = \"https://carnegieart.org/art/whats-on-view/page/3/\"\n",
    "    scraped_data = scrape_page(scraper_, link)\n",
    "    append_to_json_file(scraped_data, \"../raw_documents/Pittsburgh_Carnegie_Museums.json\")\n",
    "\n",
    "    link = \"https://carnegieart.org/art/whats-on-view/page/4/\"\n",
    "    scraped_data = scrape_page(scraper_, link)\n",
    "    append_to_json_file(scraped_data, \"../raw_documents/Pittsburgh_Carnegie_Museums.json\")\n",
    "\n",
    "    link = \"https://carnegieart.org/art/whats-on-view/page/5/\"\n",
    "    scraped_data = scrape_page(scraper_, link)\n",
    "    append_to_json_file(scraped_data, \"../raw_documents/Pittsburgh_Carnegie_Museums.json\")\n",
    "\n",
    "    link = \"https://carnegieart.org/art/whats-on-view/page/6/\"\n",
    "    scraped_data = scrape_page(scraper_, link)\n",
    "    append_to_json_file(scraped_data, \"../raw_documents/Pittsburgh_Carnegie_Museums.json\")\n",
    "\n",
    "    link = \"https://carnegieart.org/art/whats-on-view/page/7/\"\n",
    "    scraped_data = scrape_page(scraper_, link)\n",
    "    append_to_json_file(scraped_data, \"../raw_documents/Pittsburgh_Carnegie_Museums.json\")\n",
    "\n",
    "    link = \"https://carnegieart.org/art/whats-on-view/page/8/\"\n",
    "    scraped_data = scrape_page(scraper_, link)\n",
    "    append_to_json_file(scraped_data, \"../raw_documents/Pittsburgh_Carnegie_Museums.json\")\n",
    "\n",
    "    scraper_.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching: https://www.warhol.org/calendar/\n",
      "Fetching: https://www.warhol.org/calendar/?date=2024-11-01&days=30&0=#calendar-header\n",
      "Fetching: https://www.warhol.org/calendar/?date=2024-12-01&days=31&0=#calendar-header\n",
      "Fetching: https://www.warhol.org/calendar/?date=2025-01-01&days=31&0=#calendar-header\n",
      "Fetching: https://www.warhol.org/calendar/?date=2025-02-01&days=28&0=#calendar-header\n",
      "Fetching: https://www.warhol.org/calendar/?date=2025-03-01&days=31&0=#calendar-header\n",
      "Fetching: https://www.warhol.org/calendar/?date=2025-04-01&days=30&0=#calendar-header\n",
      "Fetching: https://www.warhol.org/calendar/?date=2025-05-01&days=31&0=#calendar-header\n",
      "Fetching: https://www.warhol.org/calendar/?date=2025-06-01&days=30&0=#calendar-header\n",
      "Fetching: https://www.warhol.org/calendar/?date=2025-07-01&days=31&0=#calendar-header\n",
      "Fetching: https://www.warhol.org/calendar/?date=2025-08-01&days=31&0=#calendar-header\n",
      "Fetching: https://www.warhol.org/calendar/?date=2025-09-01&days=30&0=#calendar-header\n",
      "Fetching: https://www.warhol.org/calendar/?date=2025-10-01&days=31&0=#calendar-header\n",
      "Fetching: https://www.warhol.org/calendar/?date=2025-11-01&days=30&0=#calendar-header\n",
      "Fetching: https://www.warhol.org/calendar/?date=2025-12-01&days=31&0=#calendar-header\n",
      "Data appended to ../raw_documents/Pittsburgh_Carnegie_Museums.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "\n",
    "class Scraper:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.chrome_options = Options()\n",
    "        self.chrome_options.add_argument(\"--headless\")\n",
    "        self.chrome_options.add_argument(\"--disable-gpu\")\n",
    "        self.chrome_options.add_argument(\"--no-sandbox\")\n",
    "        self.chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "        self.driver = webdriver.Chrome(options=self.chrome_options)\n",
    "        self.driver.set_page_load_timeout(30)  # Increase timeout for slower pages\n",
    "        self.current_url = None\n",
    "        self.current_domain = None\n",
    "\n",
    "    def set_domain(self, url: str):\n",
    "        \"\"\"Set domain name and URL\"\"\"\n",
    "        self.current_domain = url.split(\"/\")[2]\n",
    "        self.current_url = url\n",
    "\n",
    "    def fetch(self, url: str):\n",
    "        \"\"\"Capture the webpage\"\"\"\n",
    "        soup = self.get_soup(url)\n",
    "        return soup\n",
    "\n",
    "    def get_html(self, url: str):\n",
    "        \"\"\"Get HTML content\"\"\"\n",
    "        self.set_domain(url)\n",
    "        self.driver.get(url)\n",
    "        try:\n",
    "            # Explicit wait for the main content to load (adjust the selector as necessary)\n",
    "            WebDriverWait(self.driver, 30).until(EC.presence_of_element_located((By.CSS_SELECTOR, \"div.item\")))\n",
    "        except Exception as e:\n",
    "            print(f\"Error while waiting for the page to load: {e}\")\n",
    "        return self.driver.page_source\n",
    "\n",
    "    def get_soup(self, url: str):\n",
    "        \"\"\"Parse and return BeautifulSoup object\"\"\"\n",
    "        html = self.get_html(url)\n",
    "        return BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "    def close(self):\n",
    "        self.driver.quit()\n",
    "\n",
    "    def __del__(self):\n",
    "        self.close()\n",
    "\n",
    "def scrape_page(scraper_, link):\n",
    "    try:\n",
    "        print(f\"Fetching: {link}\")\n",
    "        soup = scraper_.get_soup(link)\n",
    "\n",
    "        page_data = []\n",
    "        \n",
    "        # 查找所有活动卡片\n",
    "        events = soup.find_all('div', class_='item')\n",
    "\n",
    "        for event in events:\n",
    "            event_data = {}\n",
    "\n",
    "            # 获取事件名称\n",
    "            event_name_tag = event.find('h3')\n",
    "            event_name = event_name_tag.text.strip() if event_name_tag else 'No Event Name'\n",
    "            event_data['event_name'] = event_name\n",
    "\n",
    "            # 获取时间\n",
    "            time_tag = event.find('time')\n",
    "            event_time = time_tag.text.strip() if time_tag else 'No Time Information'\n",
    "            event_data['time'] = event_time\n",
    "\n",
    "            # 获取地点\n",
    "            location_tag = event.find('span', class_='screen-reader-text', string='Location:')\n",
    "            if location_tag:\n",
    "                # 获取 span 标签后面的所有兄弟节点，包括文本和 <br> 标签\n",
    "                location_parts = location_tag.find_parent('p').contents\n",
    "                location = ''.join([str(part).strip() for part in location_parts if isinstance(part, str)]).replace('<br>', ', ')\n",
    "                location = location.replace('Location:', '').strip()\n",
    "            else:\n",
    "                location = 'No Location'\n",
    "            event_data['location'] = location\n",
    "\n",
    "            # 获取事件类型\n",
    "            event_type_list = event.find_all('li')\n",
    "            event_type = ', '.join([et.text.strip() for et in event_type_list if et])\n",
    "            event_data['event_type'] = event_type\n",
    "\n",
    "            # 获取受众类型\n",
    "            audience_tag = event.find('h4', string=lambda x: 'Audience' in x)\n",
    "            if audience_tag:\n",
    "                audience_list = audience_tag.find_next('ul').find_all('li')\n",
    "                audience = ', '.join([a.text.strip() for a in audience_list if a])\n",
    "            else:\n",
    "                audience = 'No Audience Information'\n",
    "            event_data['audience'] = audience\n",
    "\n",
    "            # 将该事件信息添加到列表中\n",
    "            page_data.append(event_data)\n",
    "\n",
    "        return page_data\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching {link}: {e}\")\n",
    "        return []\n",
    "        \n",
    "def scrape_multiple_pages(scraper_, links):\n",
    "    all_data = []\n",
    "    \n",
    "    # 遍历每个链接并爬取数据\n",
    "    for link in links:\n",
    "        data = scrape_page(scraper_, link)\n",
    "        all_data.extend(data)  # 将每个页面的数据合并到总数据中\n",
    "\n",
    "    return all_data\n",
    "\n",
    "def append_to_json_file(data, filename):\n",
    "    try:\n",
    "        # 检查文件是否存在\n",
    "        if os.path.exists(filename):\n",
    "            with open(filename, 'r', encoding='utf-8') as f:\n",
    "                existing_data = json.load(f)\n",
    "        else:\n",
    "            existing_data = []\n",
    "\n",
    "        # 添加新数据\n",
    "        existing_data.extend(data)\n",
    "\n",
    "        # 保存回 JSON 文件\n",
    "        with open(filename, 'w', encoding='utf-8') as f:\n",
    "            json.dump(existing_data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "        print(f\"Data appended to {filename}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error writing to JSON file: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    scraper_ = Scraper()\n",
    "\n",
    "    # 要爬取的网页链接列表\n",
    "    links = [\n",
    "        \"https://www.warhol.org/calendar/\",\n",
    "        \"https://www.warhol.org/calendar/?date=2024-11-01&days=30&0=#calendar-header\",\n",
    "        \"https://www.warhol.org/calendar/?date=2024-12-01&days=31&0=#calendar-header\",\n",
    "        \"https://www.warhol.org/calendar/?date=2025-01-01&days=31&0=#calendar-header\",\n",
    "        \"https://www.warhol.org/calendar/?date=2025-02-01&days=28&0=#calendar-header\",\n",
    "        \"https://www.warhol.org/calendar/?date=2025-03-01&days=31&0=#calendar-header\",\n",
    "        \"https://www.warhol.org/calendar/?date=2025-04-01&days=30&0=#calendar-header\",\n",
    "        \"https://www.warhol.org/calendar/?date=2025-05-01&days=31&0=#calendar-header\",\n",
    "        \"https://www.warhol.org/calendar/?date=2025-06-01&days=30&0=#calendar-header\",\n",
    "        \"https://www.warhol.org/calendar/?date=2025-07-01&days=31&0=#calendar-header\",\n",
    "        \"https://www.warhol.org/calendar/?date=2025-08-01&days=31&0=#calendar-header\",\n",
    "        \"https://www.warhol.org/calendar/?date=2025-09-01&days=30&0=#calendar-header\",\n",
    "        \"https://www.warhol.org/calendar/?date=2025-10-01&days=31&0=#calendar-header\",\n",
    "        \"https://www.warhol.org/calendar/?date=2025-11-01&days=30&0=#calendar-header\",\n",
    "        \"https://www.warhol.org/calendar/?date=2025-12-01&days=31&0=#calendar-header\"\n",
    "    ]\n",
    "\n",
    "    # 爬取所有页面的内容\n",
    "    scraped_data = scrape_multiple_pages(scraper_, links)\n",
    "\n",
    "    # 将数据追加到现有的 JSON 文件中\n",
    "    append_to_json_file(scraped_data, \"../raw_documents/Pittsburgh_Carnegie_Museums.json\")\n",
    "\n",
    "    scraper_.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Heinz History Center\n",
    "### https://www.heinzhistorycenter.org/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching: https://www.heinzhistorycenter.org/events/https://www.heinzhistorycenter.org/\n",
      "Fetching: https://www.heinzhistorycenter.org/research/detre-library-archives/\n",
      "Fetching: https://www.heinzhistorycenter.org/whats-on/exhibits/\n",
      "Data saved to ../raw_documents/Heinz_History_Center.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import os\n",
    "\n",
    "\n",
    "class Scraper:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.chrome_options = Options()\n",
    "        self.chrome_options.add_argument(\"--headless\")\n",
    "        self.chrome_options.add_argument(\"--disable-gpu\")\n",
    "        self.chrome_options.add_argument(\"--no-sandbox\")\n",
    "        self.chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "        self.driver = webdriver.Chrome(options=self.chrome_options)\n",
    "        self.driver.set_page_load_timeout(10)  # Adjusted timeout for slower pages\n",
    "        self.current_url = None\n",
    "        self.current_domain = None\n",
    "\n",
    "    def set_domain(self, url: str):\n",
    "        \"\"\"Set domain name and URL.\"\"\"\n",
    "        self.current_domain = url.split(\"/\")[2]  # 例如 'www.example.com'\n",
    "        self.current_url = url\n",
    "\n",
    "    def fetch(self, url: str, raw_html: bool = False):\n",
    "        \"\"\"Capture the webpage and choose whether to remove JS or CSS.\"\"\"\n",
    "        soup = self.get_soup(url)\n",
    "        if not raw_html:\n",
    "            soup = self.remove_js_css(soup)\n",
    "        return soup, self.get_links(soup), self.get_title(soup)\n",
    "\n",
    "    def get_html(self, url: str):\n",
    "        \"\"\"Get HTML.\"\"\"\n",
    "        self.set_domain(url)  # 调用 set_domain 方法\n",
    "        self.driver.get(url)\n",
    "        return self.driver.page_source\n",
    "\n",
    "    def get_soup(self, url: str):\n",
    "        \"\"\"Parse and get BeautifulSoup object.\"\"\"\n",
    "        html = self.get_html(url)\n",
    "        return BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "    def get_links(self, soup: BeautifulSoup):\n",
    "        \"\"\"Get external links.\"\"\"\n",
    "        links = [link.get(\"href\") for link in soup.find_all(\"a\")]\n",
    "        return self.filter_links(links)\n",
    "\n",
    "    def remove_js_css(self, soup: BeautifulSoup):\n",
    "        \"\"\"Remove JS and CSS.\"\"\"\n",
    "        for script in soup([\"script\", \"style\"]):\n",
    "            script.extract()  # 移除所有的 <script> 和 <style> 标签\n",
    "        return soup\n",
    "\n",
    "    def remove_unnecessary_sections(self, soup: BeautifulSoup):\n",
    "        \"\"\"Remove unnecessary sections such as header, footer, nav, etc.\"\"\"\n",
    "        for tag in [\"header\", \"footer\", \"nav\", \"aside\"]:\n",
    "            for element in soup.find_all(tag):\n",
    "                element.decompose()\n",
    "        return soup\n",
    "\n",
    "    def filter_links(self, links: list):\n",
    "        filtered_links = []\n",
    "        for link in links:\n",
    "            if link is None:\n",
    "                continue\n",
    "            elif link.startswith(\"#\"):\n",
    "                continue\n",
    "            elif link.startswith(\"http\"):\n",
    "                filtered_links.append(link)\n",
    "            elif link.startswith(\"//\"):\n",
    "                filtered_links.append(f\"https:{link}\")\n",
    "            elif link.startswith(\"/\"):\n",
    "                filtered_links.append(f\"https://{self.current_domain}{link}\")\n",
    "            else:\n",
    "                filtered_links.append(f\"{self.current_url}/{link}\")\n",
    "        return filtered_links\n",
    "\n",
    "    def close(self):\n",
    "        self.driver.quit()\n",
    "\n",
    "    def __del__(self):\n",
    "        self.close()\n",
    "\n",
    "\n",
    "def extract_event_data(soup: BeautifulSoup):\n",
    "    \"\"\"Extract event data from the soup.\"\"\"\n",
    "    events = []\n",
    "\n",
    "    # 找到所有包含活动信息的卡片\n",
    "    event_cards = soup.find_all(\"div\", class_=\"card_body\")\n",
    "    for card in event_cards:\n",
    "        event = {}\n",
    "        # 提取活动名称\n",
    "        event_name_tag = card.find(\"h3\", class_=\"card_title\")\n",
    "        event['event_name'] = event_name_tag.get_text(strip=True) if event_name_tag else \"No Event Name\"\n",
    "\n",
    "        # 提取活动时间\n",
    "        time_tag = card.find(\"span\", class_=\"card_time\")\n",
    "        event['time'] = time_tag.get_text(strip=True) if time_tag else \"No Time Information\"\n",
    "\n",
    "        # 提取活动地点\n",
    "        location_tag = card.find(\"span\", class_=\"card_location\")\n",
    "        event['location'] = location_tag.get_text(strip=True) if location_tag else \"No Location\"\n",
    "\n",
    "        # 提取活动描述\n",
    "        description_tag = card.find(\"div\", class_=\"card_description\")\n",
    "        event['description'] = description_tag.get_text(strip=True) if description_tag else \"No Description\"\n",
    "\n",
    "        # 添加到事件列表\n",
    "        events.append(event)\n",
    "\n",
    "    return events\n",
    "\n",
    "\n",
    "def save_to_json(data, filename):\n",
    "    \"\"\"Save data to a JSON file, ensuring no overwrite of existing content.\"\"\"\n",
    "    try:\n",
    "        if os.path.exists(filename):\n",
    "            # 读取现有的 JSON 文件并合并\n",
    "            with open(filename, 'r', encoding='utf-8') as f:\n",
    "                existing_data = json.load(f)\n",
    "            # 合并新的数据到现有数据\n",
    "            if isinstance(existing_data, list):\n",
    "                existing_data.extend(data)  # 如果是列表，将数据合并\n",
    "            else:\n",
    "                existing_data.update(data)  # 如果是字典，更新数据\n",
    "        else:\n",
    "            existing_data = data  # 如果文件不存在，则新建\n",
    "\n",
    "        # 保存合并后的数据到文件\n",
    "        with open(filename, 'w', encoding='utf-8') as f:\n",
    "            json.dump(existing_data, f, ensure_ascii=False, indent=4)\n",
    "        print(f\"Data saved to {filename}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error writing to JSON file: {e}\")\n",
    "\n",
    "\n",
    "def scrape_links(scraper_, links):\n",
    "    \"\"\"Scrape multiple links and return organized data.\"\"\"\n",
    "    all_data = []\n",
    "    for link in links:\n",
    "        try:\n",
    "            print(f\"Fetching: {link}\")\n",
    "            soup = scraper_.get_soup(link)\n",
    "\n",
    "            soup = scraper_.remove_js_css(soup)\n",
    "            soup = scraper_.remove_unnecessary_sections(soup)\n",
    "\n",
    "            page_data = extract_event_data(soup)\n",
    "            all_data.extend(page_data)\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching {link}: {e}\")\n",
    "\n",
    "    return all_data\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    scraper_ = Scraper()\n",
    "\n",
    "    links = [\n",
    "        \"https://www.heinzhistorycenter.org/events/\"\n",
    "        \"https://www.heinzhistorycenter.org/\",\n",
    "        \"https://www.heinzhistorycenter.org/research/detre-library-archives/\",\n",
    "        \"https://www.heinzhistorycenter.org/whats-on/exhibits/\",\n",
    "\n",
    "    ]\n",
    "\n",
    "    scraped_data = scrape_links(scraper_, links)\n",
    "\n",
    "    # 合并并保存到原来的文件，而不是覆盖\n",
    "    save_to_json(scraped_data, \"../raw_documents/Heinz_History_Center.json\")\n",
    "\n",
    "    scraper_.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching: https://www.heinzhistorycenter.org/whats-on/sports-museum/exhibits/\n",
      "Fetching: https://www.heinzhistorycenter.org/whats-on/fort-pitt/\n",
      "Fetching: https://www.heinzhistorycenter.org/whats-on/meadowcroft/exhibits/\n",
      "Fetching: https://www.heinzhistorycenter.org/whats-on/exhibits/past-exhibits/\n",
      "Data saved to ../raw_documents/Heinz_History_Center.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import os\n",
    "\n",
    "class Scraper:\n",
    "    def __init__(self):\n",
    "        self.chrome_options = Options()\n",
    "        self.chrome_options.add_argument(\"--headless\")\n",
    "        self.chrome_options.add_argument(\"--disable-gpu\")\n",
    "        self.chrome_options.add_argument(\"--no-sandbox\")\n",
    "        self.chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "        self.driver = webdriver.Chrome(options=self.chrome_options)\n",
    "        self.driver.set_page_load_timeout(10)\n",
    "\n",
    "    def get_html(self, url: str):\n",
    "        \"\"\"Get HTML.\"\"\"\n",
    "        self.driver.get(url)\n",
    "        return self.driver.page_source\n",
    "\n",
    "    def get_soup(self, url: str):\n",
    "        \"\"\"Parse and return BeautifulSoup object.\"\"\"\n",
    "        html = self.get_html(url)\n",
    "        return BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "    def close(self):\n",
    "        self.driver.quit()\n",
    "\n",
    "    def __del__(self):\n",
    "        self.close()\n",
    "\n",
    "def organize_content_by_heading(soup: BeautifulSoup):\n",
    "    \"\"\"Organize content based on headings, ensuring all relevant text is captured.\"\"\"\n",
    "    data = {}\n",
    "    current_h1, current_h2, current_h3, current_h4, current_h5, current_h6 = None, None, None, None, None, None\n",
    "\n",
    "    for tag in soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'p', 'div', 'span']):\n",
    "        if tag.name == 'h1':\n",
    "            current_h1 = tag.get_text(strip=True)\n",
    "            data[current_h1] = {'content': \"\"}\n",
    "            current_h2 = current_h3 = current_h4 = current_h5 = current_h6 = None  # Reset lower headings\n",
    "        elif tag.name == 'h2' and current_h1:\n",
    "            current_h2 = tag.get_text(strip=True)\n",
    "            data[current_h1][current_h2] = {'content': \"\"}\n",
    "            current_h3 = current_h4 = current_h5 = current_h6 = None\n",
    "        elif tag.name == 'h3' and current_h2:\n",
    "            current_h3 = tag.get_text(strip=True)\n",
    "            data[current_h1][current_h2][current_h3] = {'content': \"\"}\n",
    "            current_h4 = current_h5 = current_h6 = None\n",
    "        elif tag.name == 'h4' and current_h3:\n",
    "            current_h4 = tag.get_text(strip=True)\n",
    "            data[current_h1][current_h2][current_h3][current_h4] = {'content': \"\"}\n",
    "            current_h5 = current_h6 = None\n",
    "        elif tag.name == 'h5' and current_h4:\n",
    "            current_h5 = tag.get_text(strip=True)\n",
    "            data[current_h1][current_h2][current_h3][current_h4][current_h5] = {'content': \"\"}\n",
    "            current_h6 = None\n",
    "        elif tag.name == 'h6' and current_h5:\n",
    "            current_h6 = tag.get_text(strip=True)\n",
    "            data[current_h1][current_h2][current_h3][current_h4][current_h5][current_h6] = {'content': \"\"}\n",
    "        else:\n",
    "            content = tag.get_text(strip=True)\n",
    "            if content:\n",
    "                if current_h6:\n",
    "                    data[current_h1][current_h2][current_h3][current_h4][current_h5][current_h6]['content'] += content + \" \"\n",
    "                elif current_h5:\n",
    "                    data[current_h1][current_h2][current_h3][current_h4][current_h5]['content'] += content + \" \"\n",
    "                elif current_h4:\n",
    "                    data[current_h1][current_h2][current_h3][current_h4]['content'] += content + \" \"\n",
    "                elif current_h3:\n",
    "                    data[current_h1][current_h2][current_h3]['content'] += content + \" \"\n",
    "                elif current_h2:\n",
    "                    data[current_h1][current_h2]['content'] += content + \" \"\n",
    "                elif current_h1:\n",
    "                    data[current_h1]['content'] += content + \" \"\n",
    "\n",
    "    return data\n",
    "\n",
    "def save_to_json(data, filename):\n",
    "    \"\"\"Save data to a JSON file, ensuring no overwrite of existing content.\"\"\"\n",
    "    try:\n",
    "        if os.path.exists(filename):\n",
    "            # 读取现有的 JSON 文件并合并\n",
    "            with open(filename, 'r', encoding='utf-8') as f:\n",
    "                existing_data = json.load(f)\n",
    "            # 合并新的数据到现有数据\n",
    "            if isinstance(existing_data, list):\n",
    "                existing_data.extend(data)  # 如果是列表，将数据合并\n",
    "            else:\n",
    "                existing_data.update(data)  # 如果是字典，更新数据\n",
    "        else:\n",
    "            existing_data = data  # 如果文件不存在，则新建\n",
    "\n",
    "        # 保存合并后的数据到文件\n",
    "        with open(filename, 'w', encoding='utf-8') as f:\n",
    "            json.dump(existing_data, f, ensure_ascii=False, indent=4)\n",
    "        print(f\"Data saved to {filename}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error writing to JSON file: {e}\")\n",
    "\n",
    "def scrape_links(scraper_, links):\n",
    "    \"\"\"Scrape multiple links and return organized data.\"\"\"\n",
    "    all_data = []\n",
    "    for link in links:\n",
    "        try:\n",
    "            print(f\"Fetching: {link}\")\n",
    "            soup = scraper_.get_soup(link)\n",
    "            # 提取内容并按照标题层级组织\n",
    "            page_data = organize_content_by_heading(soup)\n",
    "            page_data['url'] = link  # 保存爬取的链接\n",
    "            all_data.append(page_data)\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching {link}: {e}\")\n",
    "\n",
    "    return all_data\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    scraper_ = Scraper()\n",
    "\n",
    "    links = [\n",
    "        \"https://www.heinzhistorycenter.org/whats-on/sports-museum/exhibits/\",\n",
    "        \"https://www.heinzhistorycenter.org/whats-on/fort-pitt/\",\n",
    "        \"https://www.heinzhistorycenter.org/whats-on/meadowcroft/exhibits/\",\n",
    "        \"https://www.heinzhistorycenter.org/whats-on/exhibits/past-exhibits/\"\n",
    "    ]\n",
    "\n",
    "    # 爬取所有页面的内容\n",
    "    scraped_data = scrape_links(scraper_, links)\n",
    "\n",
    "    # 保存到 JSON 文件，追加到现有内容中\n",
    "    save_to_json(scraped_data, \"../raw_documents/Heinz_History_Center.json\")\n",
    "\n",
    "    scraper_.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Frick\n",
    "### https://www.thefrickpittsburgh.org/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching: https://www.thefrickpittsburgh.org/calendar?search=1&page=1&search_date_from=10%2F01%2F2024&search_date_to=3%2F31%2F2025\n",
      "Fetching: https://www.thefrickpittsburgh.org/calendar?search=1&page=2&search_date_from=10%2F01%2F2024&search_date_to=3%2F31%2F2025\n",
      "Fetching: https://www.thefrickpittsburgh.org/stories\n",
      "Fetching: https://www.thefrickpittsburgh.org/exhibitions\n",
      "Fetching: https://www.thefrickpittsburgh.org/plan-your-visit\n",
      "Fetching: https://www.thefrickpittsburgh.org/mission\n",
      "Fetching: https://www.thefrickpittsburgh.org/collection\n",
      "Fetching: https://collection.thefrickpittsburgh.org/objects?query=object_type%3A%22Carriage%22+OR+object_type%3A%22Cars+and+Carriages%22&limit=40\n",
      "Fetching: https://collection.thefrickpittsburgh.org/objects?query=object_type%3A%22Costume%22&limit=40\n",
      "Fetching: https://collection.thefrickpittsburgh.org/objects?query=object_type%3A%22Decorative+Arts%22&limit=40\n",
      "Fetching: https://collection.thefrickpittsburgh.org/objects?query=object_type%3A%22Painting%22&limit=40\n",
      "Fetching: https://collection.thefrickpittsburgh.org/objects?query=object_type%3A%22Photography%22&limit=40\n",
      "Fetching: https://collection.thefrickpittsburgh.org/objects?query=object_type%3A%22Works+on+Paper%2FDrawing%22+OR+object_type%3A%22Works+on+Paper%2FPrint%22&limit=40\n",
      "Data saved to ../raw_documents/Pittsburgh_Frick.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import os\n",
    "\n",
    "class Scraper:\n",
    "    def __init__(self):\n",
    "        self.chrome_options = Options()\n",
    "        self.chrome_options.add_argument(\"--headless\")\n",
    "        self.chrome_options.add_argument(\"--disable-gpu\")\n",
    "        self.chrome_options.add_argument(\"--no-sandbox\")\n",
    "        self.chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "        self.driver = webdriver.Chrome(options=self.chrome_options)\n",
    "        self.driver.set_page_load_timeout(10)\n",
    "\n",
    "    def get_html(self, url: str):\n",
    "        \"\"\"Get HTML.\"\"\"\n",
    "        self.driver.get(url)\n",
    "        return self.driver.page_source\n",
    "\n",
    "    def get_soup(self, url: str):\n",
    "        \"\"\"Parse and return BeautifulSoup object.\"\"\"\n",
    "        html = self.get_html(url)\n",
    "        return BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "    def close(self):\n",
    "        self.driver.quit()\n",
    "\n",
    "    def __del__(self):\n",
    "        self.close()\n",
    "\n",
    "def organize_content_by_heading(soup: BeautifulSoup):\n",
    "    \"\"\"Organize content based on headings, ensuring all relevant text is captured.\"\"\"\n",
    "    data = {}\n",
    "    current_h1, current_h2, current_h3, current_h4, current_h5, current_h6 = None, None, None, None, None, None\n",
    "\n",
    "    for tag in soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'p', 'div', 'span']):\n",
    "        if tag.name == 'h1':\n",
    "            current_h1 = tag.get_text(strip=True)\n",
    "            data[current_h1] = {'content': \"\"}\n",
    "            current_h2 = current_h3 = current_h4 = current_h5 = current_h6 = None  # Reset lower headings\n",
    "        elif tag.name == 'h2' and current_h1:\n",
    "            current_h2 = tag.get_text(strip=True)\n",
    "            data[current_h1][current_h2] = {'content': \"\"}\n",
    "            current_h3 = current_h4 = current_h5 = current_h6 = None\n",
    "        elif tag.name == 'h3' and current_h2:\n",
    "            current_h3 = tag.get_text(strip=True)\n",
    "            data[current_h1][current_h2][current_h3] = {'content': \"\"}\n",
    "            current_h4 = current_h5 = current_h6 = None\n",
    "        elif tag.name == 'h4' and current_h3:\n",
    "            current_h4 = tag.get_text(strip=True)\n",
    "            data[current_h1][current_h2][current_h3][current_h4] = {'content': \"\"}\n",
    "            current_h5 = current_h6 = None\n",
    "        elif tag.name == 'h5' and current_h4:\n",
    "            current_h5 = tag.get_text(strip=True)\n",
    "            data[current_h1][current_h2][current_h3][current_h4][current_h5] = {'content': \"\"}\n",
    "            current_h6 = None\n",
    "        elif tag.name == 'h6' and current_h5:\n",
    "            current_h6 = tag.get_text(strip=True)\n",
    "            data[current_h1][current_h2][current_h3][current_h4][current_h5][current_h6] = {'content': \"\"}\n",
    "        else:\n",
    "            content = tag.get_text(strip=True)\n",
    "            if content:\n",
    "                if current_h6:\n",
    "                    data[current_h1][current_h2][current_h3][current_h4][current_h5][current_h6]['content'] += content + \" \"\n",
    "                elif current_h5:\n",
    "                    data[current_h1][current_h2][current_h3][current_h4][current_h5]['content'] += content + \" \"\n",
    "                elif current_h4:\n",
    "                    data[current_h1][current_h2][current_h3][current_h4]['content'] += content + \" \"\n",
    "                elif current_h3:\n",
    "                    data[current_h1][current_h2][current_h3]['content'] += content + \" \"\n",
    "                elif current_h2:\n",
    "                    data[current_h1][current_h2]['content'] += content + \" \"\n",
    "                elif current_h1:\n",
    "                    data[current_h1]['content'] += content + \" \"\n",
    "\n",
    "    return data\n",
    "\n",
    "def save_to_json(data, filename):\n",
    "    \"\"\"Save data to a JSON file, ensuring no overwrite of existing content.\"\"\"\n",
    "    try:\n",
    "        if os.path.exists(filename):\n",
    "            # 读取现有的 JSON 文件并合并\n",
    "            with open(filename, 'r', encoding='utf-8') as f:\n",
    "                existing_data = json.load(f)\n",
    "            # 合并新的数据到现有数据\n",
    "            if isinstance(existing_data, list):\n",
    "                existing_data.extend(data)  # 如果是列表，将数据合并\n",
    "            else:\n",
    "                existing_data.update(data)  # 如果是字典，更新数据\n",
    "        else:\n",
    "            existing_data = data  # 如果文件不存在，则新建\n",
    "\n",
    "        # 保存合并后的数据到文件\n",
    "        with open(filename, 'w', encoding='utf-8') as f:\n",
    "            json.dump(existing_data, f, ensure_ascii=False, indent=4)\n",
    "        print(f\"Data saved to {filename}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error writing to JSON file: {e}\")\n",
    "\n",
    "def scrape_links(scraper_, links):\n",
    "    \"\"\"Scrape multiple links and return organized data.\"\"\"\n",
    "    all_data = []\n",
    "    for link in links:\n",
    "        try:\n",
    "            print(f\"Fetching: {link}\")\n",
    "            soup = scraper_.get_soup(link)\n",
    "            # 提取内容并按照标题层级组织\n",
    "            page_data = organize_content_by_heading(soup)\n",
    "            page_data['url'] = link  # 保存爬取的链接\n",
    "            all_data.append(page_data)\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching {link}: {e}\")\n",
    "\n",
    "    return all_data\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    scraper_ = Scraper()\n",
    "\n",
    "    links = [\n",
    "        \"https://www.thefrickpittsburgh.org/calendar?search=1&page=1&search_date_from=10%2F01%2F2024&search_date_to=3%2F31%2F2025\",\n",
    "        \"https://www.thefrickpittsburgh.org/calendar?search=1&page=2&search_date_from=10%2F01%2F2024&search_date_to=3%2F31%2F2025\",\n",
    "        \"https://www.thefrickpittsburgh.org/stories\",\n",
    "        \"https://www.thefrickpittsburgh.org/exhibitions\",\n",
    "        \"https://www.thefrickpittsburgh.org/plan-your-visit\",\n",
    "        \"https://www.thefrickpittsburgh.org/mission\",\n",
    "        \"https://www.thefrickpittsburgh.org/collection\",\n",
    "        \"https://collection.thefrickpittsburgh.org/objects?query=object_type%3A%22Carriage%22+OR+object_type%3A%22Cars+and+Carriages%22&limit=40\",\n",
    "        \"https://collection.thefrickpittsburgh.org/objects?query=object_type%3A%22Costume%22&limit=40\",\n",
    "        \"https://collection.thefrickpittsburgh.org/objects?query=object_type%3A%22Decorative+Arts%22&limit=40\",\n",
    "        \"https://collection.thefrickpittsburgh.org/objects?query=object_type%3A%22Painting%22&limit=40\",\n",
    "        \"https://collection.thefrickpittsburgh.org/objects?query=object_type%3A%22Photography%22&limit=40\",\n",
    "        \"https://collection.thefrickpittsburgh.org/objects?query=object_type%3A%22Works+on+Paper%2FDrawing%22+OR+object_type%3A%22Works+on+Paper%2FPrint%22&limit=40\"\n",
    "    ]\n",
    "\n",
    "    # 爬取所有页面的内容\n",
    "    scraped_data = scrape_links(scraper_, links)\n",
    "\n",
    "    # 保存到 JSON 文件，追加到现有内容中\n",
    "    save_to_json(scraped_data, \"../raw_documents/Pittsburgh_Frick.json\")\n",
    "\n",
    "    scraper_.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Food\n",
    "## Food Festivals\n",
    "### https://www.visitpittsburgh.com/events-festivals/food-festivals/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching: https://www.visitpittsburgh.com/events-festivals/food-festivals/\n",
      "Fetching: https://www.visitpittsburgh.com/blog/\n",
      "Fetching: https://www.visitpittsburgh.com/plan-your-trip/\n",
      "Fetching: https://www.visitpittsburgh.com/events-festivals/\n",
      "Fetching: https://www.visitpittsburgh.com/events-festivals/?page=2\n",
      "Fetching: https://www.visitpittsburgh.com/events-festivals/?page=3\n",
      "Fetching: https://www.visitpittsburgh.com/events-festivals/?page=4\n",
      "Fetching: https://www.visitpittsburgh.com/events-festivals/?page=5\n",
      "Fetching: https://www.visitpittsburgh.com/events-festivals/?page=6\n",
      "Fetching: https://www.visitpittsburgh.com/events-festivals/?page=7\n",
      "Fetching: https://www.visitpittsburgh.com/events-festivals/?page=8\n",
      "Fetching: https://www.visitpittsburgh.com/events-festivals/?page=9\n",
      "Fetching: https://www.visitpittsburgh.com/events-festivals/?page=10\n",
      "Fetching: https://www.visitpittsburgh.com/events-festivals/?page=11\n",
      "Fetching: https://www.visitpittsburgh.com/events-festivals/?page=12\n",
      "Fetching: https://www.visitpittsburgh.com/events-festivals/?page=13\n",
      "Error fetching https://www.visitpittsburgh.com/events-festivals/?page=13: Message: timeout: Timed out receiving message from renderer: 8.279\n",
      "  (Session info: chrome=129.0.6668.100)\n",
      "Stacktrace:\n",
      "#0 0x55d1201d40ca <unknown>\n",
      "#1 0x55d11fed2a80 <unknown>\n",
      "#2 0x55d11febb073 <unknown>\n",
      "#3 0x55d11febad91 <unknown>\n",
      "#4 0x55d11feb8e58 <unknown>\n",
      "#5 0x55d11feb963f <unknown>\n",
      "#6 0x55d11fec8b57 <unknown>\n",
      "#7 0x55d11fedecdd <unknown>\n",
      "#8 0x55d11fee408b <unknown>\n",
      "#9 0x55d11feb9cf7 <unknown>\n",
      "#10 0x55d11fedeb4c <unknown>\n",
      "#11 0x55d11ff64f1e <unknown>\n",
      "#12 0x55d11ff45933 <unknown>\n",
      "#13 0x55d11ff13e3d <unknown>\n",
      "#14 0x55d11ff1492e <unknown>\n",
      "#15 0x55d12019fc8f <unknown>\n",
      "#16 0x55d1201a3d50 <unknown>\n",
      "#17 0x55d12018d837 <unknown>\n",
      "#18 0x55d1201a44c1 <unknown>\n",
      "#19 0x55d120174d1e <unknown>\n",
      "#20 0x55d1201c32b8 <unknown>\n",
      "#21 0x55d1201c34ba <unknown>\n",
      "#22 0x55d1201d2d5d <unknown>\n",
      "#23 0x7fcd4b8c4ac3 <unknown>\n",
      "\n",
      "Fetching: https://www.visitpittsburgh.com/events-festivals/?page=14\n",
      "Error fetching https://www.visitpittsburgh.com/events-festivals/?page=14: Message: timeout: Timed out receiving message from renderer: 10.000\n",
      "  (Session info: chrome=129.0.6668.100)\n",
      "Stacktrace:\n",
      "#0 0x55d1201d40ca <unknown>\n",
      "#1 0x55d11fed2a80 <unknown>\n",
      "#2 0x55d11febb073 <unknown>\n",
      "#3 0x55d11febad91 <unknown>\n",
      "#4 0x55d11feb8e58 <unknown>\n",
      "#5 0x55d11feb963f <unknown>\n",
      "#6 0x55d11fec8b57 <unknown>\n",
      "#7 0x55d11fedecdd <unknown>\n",
      "#8 0x55d11fee408b <unknown>\n",
      "#9 0x55d11feb9cf7 <unknown>\n",
      "#10 0x55d11fedeb4c <unknown>\n",
      "#11 0x55d11ff64a82 <unknown>\n",
      "#12 0x55d11ff45933 <unknown>\n",
      "#13 0x55d11ff13e3d <unknown>\n",
      "#14 0x55d11ff1492e <unknown>\n",
      "#15 0x55d12019fc8f <unknown>\n",
      "#16 0x55d1201a3d50 <unknown>\n",
      "#17 0x55d12018d837 <unknown>\n",
      "#18 0x55d1201a44c1 <unknown>\n",
      "#19 0x55d120174d1e <unknown>\n",
      "#20 0x55d1201c32b8 <unknown>\n",
      "#21 0x55d1201c34ba <unknown>\n",
      "#22 0x55d1201d2d5d <unknown>\n",
      "#23 0x7fcd4b8c4ac3 <unknown>\n",
      "\n",
      "Fetching: https://www.visitpittsburgh.com/events-festivals/?page=15\n",
      "Error fetching https://www.visitpittsburgh.com/events-festivals/?page=15: Message: timeout: Timed out receiving message from renderer: 10.000\n",
      "  (Session info: chrome=129.0.6668.100)\n",
      "Stacktrace:\n",
      "#0 0x55d1201d40ca <unknown>\n",
      "#1 0x55d11fed2a80 <unknown>\n",
      "#2 0x55d11febb073 <unknown>\n",
      "#3 0x55d11febad91 <unknown>\n",
      "#4 0x55d11feb8e58 <unknown>\n",
      "#5 0x55d11feb963f <unknown>\n",
      "#6 0x55d11fec8b57 <unknown>\n",
      "#7 0x55d11fedecdd <unknown>\n",
      "#8 0x55d11fee408b <unknown>\n",
      "#9 0x55d11feb9cf7 <unknown>\n",
      "#10 0x55d11fedeb4c <unknown>\n",
      "#11 0x55d11ff64a82 <unknown>\n",
      "#12 0x55d11ff45933 <unknown>\n",
      "#13 0x55d11ff13e3d <unknown>\n",
      "#14 0x55d11ff1492e <unknown>\n",
      "#15 0x55d12019fc8f <unknown>\n",
      "#16 0x55d1201a3d50 <unknown>\n",
      "#17 0x55d12018d837 <unknown>\n",
      "#18 0x55d1201a44c1 <unknown>\n",
      "#19 0x55d120174d1e <unknown>\n",
      "#20 0x55d1201c32b8 <unknown>\n",
      "#21 0x55d1201c34ba <unknown>\n",
      "#22 0x55d1201d2d5d <unknown>\n",
      "#23 0x7fcd4b8c4ac3 <unknown>\n",
      "\n",
      "Fetching: https://www.visitpittsburgh.com/events-festivals/?page=16\n",
      "Error fetching https://www.visitpittsburgh.com/events-festivals/?page=16: Message: timeout: Timed out receiving message from renderer: 10.000\n",
      "  (Session info: chrome=129.0.6668.100)\n",
      "Stacktrace:\n",
      "#0 0x55d1201d40ca <unknown>\n",
      "#1 0x55d11fed2a80 <unknown>\n",
      "#2 0x55d11febb073 <unknown>\n",
      "#3 0x55d11febad91 <unknown>\n",
      "#4 0x55d11feb8e58 <unknown>\n",
      "#5 0x55d11feb963f <unknown>\n",
      "#6 0x55d11fec8b57 <unknown>\n",
      "#7 0x55d11fedecdd <unknown>\n",
      "#8 0x55d11fee408b <unknown>\n",
      "#9 0x55d11feb9cf7 <unknown>\n",
      "#10 0x55d11fedeb4c <unknown>\n",
      "#11 0x55d11ff64a82 <unknown>\n",
      "#12 0x55d11ff45933 <unknown>\n",
      "#13 0x55d11ff13e3d <unknown>\n",
      "#14 0x55d11ff1492e <unknown>\n",
      "#15 0x55d12019fc8f <unknown>\n",
      "#16 0x55d1201a3d50 <unknown>\n",
      "#17 0x55d12018d837 <unknown>\n",
      "#18 0x55d1201a44c1 <unknown>\n",
      "#19 0x55d120174d1e <unknown>\n",
      "#20 0x55d1201c32b8 <unknown>\n",
      "#21 0x55d1201c34ba <unknown>\n",
      "#22 0x55d1201d2d5d <unknown>\n",
      "#23 0x7fcd4b8c4ac3 <unknown>\n",
      "\n",
      "Fetching: https://www.visitpittsburgh.com/things-to-do/\n",
      "Error fetching https://www.visitpittsburgh.com/things-to-do/: Message: timeout: Timed out receiving message from renderer: 10.000\n",
      "  (Session info: chrome=129.0.6668.100)\n",
      "Stacktrace:\n",
      "#0 0x55d1201d40ca <unknown>\n",
      "#1 0x55d11fed2a80 <unknown>\n",
      "#2 0x55d11febb073 <unknown>\n",
      "#3 0x55d11febad91 <unknown>\n",
      "#4 0x55d11feb8e58 <unknown>\n",
      "#5 0x55d11feb963f <unknown>\n",
      "#6 0x55d11fec8b57 <unknown>\n",
      "#7 0x55d11fedecdd <unknown>\n",
      "#8 0x55d11fee408b <unknown>\n",
      "#9 0x55d11feb9cf7 <unknown>\n",
      "#10 0x55d11fedeb4c <unknown>\n",
      "#11 0x55d11ff64a82 <unknown>\n",
      "#12 0x55d11ff45933 <unknown>\n",
      "#13 0x55d11ff13e3d <unknown>\n",
      "#14 0x55d11ff1492e <unknown>\n",
      "#15 0x55d12019fc8f <unknown>\n",
      "#16 0x55d1201a3d50 <unknown>\n",
      "#17 0x55d12018d837 <unknown>\n",
      "#18 0x55d1201a44c1 <unknown>\n",
      "#19 0x55d120174d1e <unknown>\n",
      "#20 0x55d1201c32b8 <unknown>\n",
      "#21 0x55d1201c34ba <unknown>\n",
      "#22 0x55d1201d2d5d <unknown>\n",
      "#23 0x7fcd4b8c4ac3 <unknown>\n",
      "\n",
      "Fetching: https://www.visitpittsburgh.com/restaurants-culinary/\n",
      "Error fetching https://www.visitpittsburgh.com/restaurants-culinary/: Message: timeout: Timed out receiving message from renderer: 10.000\n",
      "  (Session info: chrome=129.0.6668.100)\n",
      "Stacktrace:\n",
      "#0 0x55d1201d40ca <unknown>\n",
      "#1 0x55d11fed2a80 <unknown>\n",
      "#2 0x55d11febb073 <unknown>\n",
      "#3 0x55d11febad91 <unknown>\n",
      "#4 0x55d11feb8e58 <unknown>\n",
      "#5 0x55d11feb963f <unknown>\n",
      "#6 0x55d11fec8b57 <unknown>\n",
      "#7 0x55d11fedecdd <unknown>\n",
      "#8 0x55d11fee408b <unknown>\n",
      "#9 0x55d11feb9cf7 <unknown>\n",
      "#10 0x55d11fedeb4c <unknown>\n",
      "#11 0x55d11ff64a82 <unknown>\n",
      "#12 0x55d11ff45933 <unknown>\n",
      "#13 0x55d11ff13e3d <unknown>\n",
      "#14 0x55d11ff1492e <unknown>\n",
      "#15 0x55d12019fc8f <unknown>\n",
      "#16 0x55d1201a3d50 <unknown>\n",
      "#17 0x55d12018d837 <unknown>\n",
      "#18 0x55d1201a44c1 <unknown>\n",
      "#19 0x55d120174d1e <unknown>\n",
      "#20 0x55d1201c32b8 <unknown>\n",
      "#21 0x55d1201c34ba <unknown>\n",
      "#22 0x55d1201d2d5d <unknown>\n",
      "#23 0x7fcd4b8c4ac3 <unknown>\n",
      "\n",
      "Fetching: https://www.visitpittsburgh.com/restaurants-culinary/craft-breweries/\n",
      "Error fetching https://www.visitpittsburgh.com/restaurants-culinary/craft-breweries/: Message: timeout: Timed out receiving message from renderer: 10.000\n",
      "  (Session info: chrome=129.0.6668.100)\n",
      "Stacktrace:\n",
      "#0 0x55d1201d40ca <unknown>\n",
      "#1 0x55d11fed2a80 <unknown>\n",
      "#2 0x55d11febb073 <unknown>\n",
      "#3 0x55d11febad91 <unknown>\n",
      "#4 0x55d11feb8e58 <unknown>\n",
      "#5 0x55d11feb963f <unknown>\n",
      "#6 0x55d11fec8b57 <unknown>\n",
      "#7 0x55d11fedecdd <unknown>\n",
      "#8 0x55d11fee408b <unknown>\n",
      "#9 0x55d11feb9cf7 <unknown>\n",
      "#10 0x55d11fedeb4c <unknown>\n",
      "#11 0x55d11ff64a82 <unknown>\n",
      "#12 0x55d11ff45933 <unknown>\n",
      "#13 0x55d11ff13e3d <unknown>\n",
      "#14 0x55d11ff1492e <unknown>\n",
      "#15 0x55d12019fc8f <unknown>\n",
      "#16 0x55d1201a3d50 <unknown>\n",
      "#17 0x55d12018d837 <unknown>\n",
      "#18 0x55d1201a44c1 <unknown>\n",
      "#19 0x55d120174d1e <unknown>\n",
      "#20 0x55d1201c32b8 <unknown>\n",
      "#21 0x55d1201c34ba <unknown>\n",
      "#22 0x55d1201d2d5d <unknown>\n",
      "#23 0x7fcd4b8c4ac3 <unknown>\n",
      "\n",
      "Fetching: https://www.visitpittsburgh.com/things-to-do/family-fun/\n",
      "Error fetching https://www.visitpittsburgh.com/things-to-do/family-fun/: Message: timeout: Timed out receiving message from renderer: 10.000\n",
      "  (Session info: chrome=129.0.6668.100)\n",
      "Stacktrace:\n",
      "#0 0x55d1201d40ca <unknown>\n",
      "#1 0x55d11fed2a80 <unknown>\n",
      "#2 0x55d11febb073 <unknown>\n",
      "#3 0x55d11febad91 <unknown>\n",
      "#4 0x55d11feb8e58 <unknown>\n",
      "#5 0x55d11feb963f <unknown>\n",
      "#6 0x55d11fec8b57 <unknown>\n",
      "#7 0x55d11fedecdd <unknown>\n",
      "#8 0x55d11fee408b <unknown>\n",
      "#9 0x55d11feb9cf7 <unknown>\n",
      "#10 0x55d11fedeb4c <unknown>\n",
      "#11 0x55d11ff64a82 <unknown>\n",
      "#12 0x55d11ff45933 <unknown>\n",
      "#13 0x55d11ff13e3d <unknown>\n",
      "#14 0x55d11ff1492e <unknown>\n",
      "#15 0x55d12019fc8f <unknown>\n",
      "#16 0x55d1201a3d50 <unknown>\n",
      "#17 0x55d12018d837 <unknown>\n",
      "#18 0x55d1201a44c1 <unknown>\n",
      "#19 0x55d120174d1e <unknown>\n",
      "#20 0x55d1201c32b8 <unknown>\n",
      "#21 0x55d1201c34ba <unknown>\n",
      "#22 0x55d1201d2d5d <unknown>\n",
      "#23 0x7fcd4b8c4ac3 <unknown>\n",
      "\n",
      "Fetching: https://www.visitpittsburgh.com/things-to-do/pittsburgh-sports-teams/\n",
      "Error fetching https://www.visitpittsburgh.com/things-to-do/pittsburgh-sports-teams/: Message: timeout: Timed out receiving message from renderer: 10.000\n",
      "  (Session info: chrome=129.0.6668.100)\n",
      "Stacktrace:\n",
      "#0 0x55d1201d40ca <unknown>\n",
      "#1 0x55d11fed2a80 <unknown>\n",
      "#2 0x55d11febb073 <unknown>\n",
      "#3 0x55d11febad91 <unknown>\n",
      "#4 0x55d11feb8e58 <unknown>\n",
      "#5 0x55d11feb963f <unknown>\n",
      "#6 0x55d11fec8b57 <unknown>\n",
      "#7 0x55d11fedecdd <unknown>\n",
      "#8 0x55d11fee408b <unknown>\n",
      "#9 0x55d11feb9cf7 <unknown>\n",
      "#10 0x55d11fedeb4c <unknown>\n",
      "#11 0x55d11ff64a82 <unknown>\n",
      "#12 0x55d11ff45933 <unknown>\n",
      "#13 0x55d11ff13e3d <unknown>\n",
      "#14 0x55d11ff1492e <unknown>\n",
      "#15 0x55d12019fc8f <unknown>\n",
      "#16 0x55d1201a3d50 <unknown>\n",
      "#17 0x55d12018d837 <unknown>\n",
      "#18 0x55d1201a44c1 <unknown>\n",
      "#19 0x55d120174d1e <unknown>\n",
      "#20 0x55d1201c32b8 <unknown>\n",
      "#21 0x55d1201c34ba <unknown>\n",
      "#22 0x55d1201d2d5d <unknown>\n",
      "#23 0x7fcd4b8c4ac3 <unknown>\n",
      "\n",
      "Fetching: https://www.visitpittsburgh.com/things-to-do/arts-culture/\n",
      "Error fetching https://www.visitpittsburgh.com/things-to-do/arts-culture/: Message: timeout: Timed out receiving message from renderer: 10.000\n",
      "  (Session info: chrome=129.0.6668.100)\n",
      "Stacktrace:\n",
      "#0 0x55d1201d40ca <unknown>\n",
      "#1 0x55d11fed2a80 <unknown>\n",
      "#2 0x55d11febb073 <unknown>\n",
      "#3 0x55d11febad91 <unknown>\n",
      "#4 0x55d11feb8e58 <unknown>\n",
      "#5 0x55d11feb963f <unknown>\n",
      "#6 0x55d11fec8b57 <unknown>\n",
      "#7 0x55d11fedecdd <unknown>\n",
      "#8 0x55d11fee408b <unknown>\n",
      "#9 0x55d11feb9cf7 <unknown>\n",
      "#10 0x55d11fedeb4c <unknown>\n",
      "#11 0x55d11ff64a82 <unknown>\n",
      "#12 0x55d11ff45933 <unknown>\n",
      "#13 0x55d11ff13e3d <unknown>\n",
      "#14 0x55d11ff1492e <unknown>\n",
      "#15 0x55d12019fc8f <unknown>\n",
      "#16 0x55d1201a3d50 <unknown>\n",
      "#17 0x55d12018d837 <unknown>\n",
      "#18 0x55d1201a44c1 <unknown>\n",
      "#19 0x55d120174d1e <unknown>\n",
      "#20 0x55d1201c32b8 <unknown>\n",
      "#21 0x55d1201c34ba <unknown>\n",
      "#22 0x55d1201d2d5d <unknown>\n",
      "#23 0x7fcd4b8c4ac3 <unknown>\n",
      "\n",
      "Fetching: https://www.visitpittsburgh.com/meetings-and-events/\n",
      "Error fetching https://www.visitpittsburgh.com/meetings-and-events/: Message: timeout: Timed out receiving message from renderer: 10.000\n",
      "  (Session info: chrome=129.0.6668.100)\n",
      "Stacktrace:\n",
      "#0 0x55d1201d40ca <unknown>\n",
      "#1 0x55d11fed2a80 <unknown>\n",
      "#2 0x55d11febb073 <unknown>\n",
      "#3 0x55d11febad91 <unknown>\n",
      "#4 0x55d11feb8e58 <unknown>\n",
      "#5 0x55d11feb963f <unknown>\n",
      "#6 0x55d11fec8b57 <unknown>\n",
      "#7 0x55d11fedecdd <unknown>\n",
      "#8 0x55d11fee408b <unknown>\n",
      "#9 0x55d11feb9cf7 <unknown>\n",
      "#10 0x55d11fedeb4c <unknown>\n",
      "#11 0x55d11ff64a82 <unknown>\n",
      "#12 0x55d11ff45933 <unknown>\n",
      "#13 0x55d11ff13e3d <unknown>\n",
      "#14 0x55d11ff1492e <unknown>\n",
      "#15 0x55d12019fc8f <unknown>\n",
      "#16 0x55d1201a3d50 <unknown>\n",
      "#17 0x55d12018d837 <unknown>\n",
      "#18 0x55d1201a44c1 <unknown>\n",
      "#19 0x55d120174d1e <unknown>\n",
      "#20 0x55d1201c32b8 <unknown>\n",
      "#21 0x55d1201c34ba <unknown>\n",
      "#22 0x55d1201d2d5d <unknown>\n",
      "#23 0x7fcd4b8c4ac3 <unknown>\n",
      "\n",
      "Fetching: https://www.visitpittsburgh.com/events-festivals/annual-events/\n",
      "Error fetching https://www.visitpittsburgh.com/events-festivals/annual-events/: Message: timeout: Timed out receiving message from renderer: 10.000\n",
      "  (Session info: chrome=129.0.6668.100)\n",
      "Stacktrace:\n",
      "#0 0x55d1201d40ca <unknown>\n",
      "#1 0x55d11fed2a80 <unknown>\n",
      "#2 0x55d11febb073 <unknown>\n",
      "#3 0x55d11febad91 <unknown>\n",
      "#4 0x55d11feb8e58 <unknown>\n",
      "#5 0x55d11feb963f <unknown>\n",
      "#6 0x55d11fec8b57 <unknown>\n",
      "#7 0x55d11fedecdd <unknown>\n",
      "#8 0x55d11fee408b <unknown>\n",
      "#9 0x55d11feb9cf7 <unknown>\n",
      "#10 0x55d11fedeb4c <unknown>\n",
      "#11 0x55d11ff64a82 <unknown>\n",
      "#12 0x55d11ff45933 <unknown>\n",
      "#13 0x55d11ff13e3d <unknown>\n",
      "#14 0x55d11ff1492e <unknown>\n",
      "#15 0x55d12019fc8f <unknown>\n",
      "#16 0x55d1201a3d50 <unknown>\n",
      "#17 0x55d12018d837 <unknown>\n",
      "#18 0x55d1201a44c1 <unknown>\n",
      "#19 0x55d120174d1e <unknown>\n",
      "#20 0x55d1201c32b8 <unknown>\n",
      "#21 0x55d1201c34ba <unknown>\n",
      "#22 0x55d1201d2d5d <unknown>\n",
      "#23 0x7fcd4b8c4ac3 <unknown>\n",
      "\n",
      "Fetching: https://www.visitpittsburgh.com/events-festivals/holidays/\n",
      "Error fetching https://www.visitpittsburgh.com/events-festivals/holidays/: Message: timeout: Timed out receiving message from renderer: 10.000\n",
      "  (Session info: chrome=129.0.6668.100)\n",
      "Stacktrace:\n",
      "#0 0x55d1201d40ca <unknown>\n",
      "#1 0x55d11fed2a80 <unknown>\n",
      "#2 0x55d11febb073 <unknown>\n",
      "#3 0x55d11febad91 <unknown>\n",
      "#4 0x55d11feb8e58 <unknown>\n",
      "#5 0x55d11feb963f <unknown>\n",
      "#6 0x55d11fec8b57 <unknown>\n",
      "#7 0x55d11fedecdd <unknown>\n",
      "#8 0x55d11fee408b <unknown>\n",
      "#9 0x55d11feb9cf7 <unknown>\n",
      "#10 0x55d11fedeb4c <unknown>\n",
      "#11 0x55d11ff64a82 <unknown>\n",
      "#12 0x55d11ff45933 <unknown>\n",
      "#13 0x55d11ff13e3d <unknown>\n",
      "#14 0x55d11ff1492e <unknown>\n",
      "#15 0x55d12019fc8f <unknown>\n",
      "#16 0x55d1201a3d50 <unknown>\n",
      "#17 0x55d12018d837 <unknown>\n",
      "#18 0x55d1201a44c1 <unknown>\n",
      "#19 0x55d120174d1e <unknown>\n",
      "#20 0x55d1201c32b8 <unknown>\n",
      "#21 0x55d1201c34ba <unknown>\n",
      "#22 0x55d1201d2d5d <unknown>\n",
      "#23 0x7fcd4b8c4ac3 <unknown>\n",
      "\n",
      "Fetching: https://www.visitpittsburgh.com/events-festivals/halloween-events/\n",
      "Error fetching https://www.visitpittsburgh.com/events-festivals/halloween-events/: Message: timeout: Timed out receiving message from renderer: 10.000\n",
      "  (Session info: chrome=129.0.6668.100)\n",
      "Stacktrace:\n",
      "#0 0x55d1201d40ca <unknown>\n",
      "#1 0x55d11fed2a80 <unknown>\n",
      "#2 0x55d11febb073 <unknown>\n",
      "#3 0x55d11febad91 <unknown>\n",
      "#4 0x55d11feb8e58 <unknown>\n",
      "#5 0x55d11feb963f <unknown>\n",
      "#6 0x55d11fec8b57 <unknown>\n",
      "#7 0x55d11fedecdd <unknown>\n",
      "#8 0x55d11fee408b <unknown>\n",
      "#9 0x55d11feb9cf7 <unknown>\n",
      "#10 0x55d11fedeb4c <unknown>\n",
      "#11 0x55d11ff64a82 <unknown>\n",
      "#12 0x55d11ff45933 <unknown>\n",
      "#13 0x55d11ff13e3d <unknown>\n",
      "#14 0x55d11ff1492e <unknown>\n",
      "#15 0x55d12019fc8f <unknown>\n",
      "#16 0x55d1201a3d50 <unknown>\n",
      "#17 0x55d12018d837 <unknown>\n",
      "#18 0x55d1201a44c1 <unknown>\n",
      "#19 0x55d120174d1e <unknown>\n",
      "#20 0x55d1201c32b8 <unknown>\n",
      "#21 0x55d1201c34ba <unknown>\n",
      "#22 0x55d1201d2d5d <unknown>\n",
      "#23 0x7fcd4b8c4ac3 <unknown>\n",
      "\n",
      "Fetching: https://www.visitpittsburgh.com/events-festivals/film-festivals/\n",
      "Error fetching https://www.visitpittsburgh.com/events-festivals/film-festivals/: Message: timeout: Timed out receiving message from renderer: 10.000\n",
      "  (Session info: chrome=129.0.6668.100)\n",
      "Stacktrace:\n",
      "#0 0x55d1201d40ca <unknown>\n",
      "#1 0x55d11fed2a80 <unknown>\n",
      "#2 0x55d11febb073 <unknown>\n",
      "#3 0x55d11febad91 <unknown>\n",
      "#4 0x55d11feb8e58 <unknown>\n",
      "#5 0x55d11feb963f <unknown>\n",
      "#6 0x55d11fec8b57 <unknown>\n",
      "#7 0x55d11fedecdd <unknown>\n",
      "#8 0x55d11fee408b <unknown>\n",
      "#9 0x55d11feb9cf7 <unknown>\n",
      "#10 0x55d11fedeb4c <unknown>\n",
      "#11 0x55d11ff64a82 <unknown>\n",
      "#12 0x55d11ff45933 <unknown>\n",
      "#13 0x55d11ff13e3d <unknown>\n",
      "#14 0x55d11ff1492e <unknown>\n",
      "#15 0x55d12019fc8f <unknown>\n",
      "#16 0x55d1201a3d50 <unknown>\n",
      "#17 0x55d12018d837 <unknown>\n",
      "#18 0x55d1201a44c1 <unknown>\n",
      "#19 0x55d120174d1e <unknown>\n",
      "#20 0x55d1201c32b8 <unknown>\n",
      "#21 0x55d1201c34ba <unknown>\n",
      "#22 0x55d1201d2d5d <unknown>\n",
      "#23 0x7fcd4b8c4ac3 <unknown>\n",
      "\n",
      "Fetching: https://www.visitpittsburgh.com/events-festivals/food-festivals/soul-food-festival/\n",
      "Error fetching https://www.visitpittsburgh.com/events-festivals/food-festivals/soul-food-festival/: Message: timeout: Timed out receiving message from renderer: 10.000\n",
      "  (Session info: chrome=129.0.6668.100)\n",
      "Stacktrace:\n",
      "#0 0x55d1201d40ca <unknown>\n",
      "#1 0x55d11fed2a80 <unknown>\n",
      "#2 0x55d11febb073 <unknown>\n",
      "#3 0x55d11febad91 <unknown>\n",
      "#4 0x55d11feb8e58 <unknown>\n",
      "#5 0x55d11feb963f <unknown>\n",
      "#6 0x55d11fec8b57 <unknown>\n",
      "#7 0x55d11fedecdd <unknown>\n",
      "#8 0x55d11fee408b <unknown>\n",
      "#9 0x55d11feb9cf7 <unknown>\n",
      "#10 0x55d11fedeb4c <unknown>\n",
      "#11 0x55d11ff64a82 <unknown>\n",
      "#12 0x55d11ff45933 <unknown>\n",
      "#13 0x55d11ff13e3d <unknown>\n",
      "#14 0x55d11ff1492e <unknown>\n",
      "#15 0x55d12019fc8f <unknown>\n",
      "#16 0x55d1201a3d50 <unknown>\n",
      "#17 0x55d12018d837 <unknown>\n",
      "#18 0x55d1201a44c1 <unknown>\n",
      "#19 0x55d120174d1e <unknown>\n",
      "#20 0x55d1201c32b8 <unknown>\n",
      "#21 0x55d1201c34ba <unknown>\n",
      "#22 0x55d1201d2d5d <unknown>\n",
      "#23 0x7fcd4b8c4ac3 <unknown>\n",
      "\n",
      "Fetching: https://www.visitpittsburgh.com/events-festivals/food-festivals/the-original-pittsburgh-taco-festival/\n",
      "Error fetching https://www.visitpittsburgh.com/events-festivals/food-festivals/the-original-pittsburgh-taco-festival/: Message: timeout: Timed out receiving message from renderer: 10.000\n",
      "  (Session info: chrome=129.0.6668.100)\n",
      "Stacktrace:\n",
      "#0 0x55d1201d40ca <unknown>\n",
      "#1 0x55d11fed2a80 <unknown>\n",
      "#2 0x55d11febb073 <unknown>\n",
      "#3 0x55d11febad91 <unknown>\n",
      "#4 0x55d11feb8e58 <unknown>\n",
      "#5 0x55d11feb963f <unknown>\n",
      "#6 0x55d11fec8b57 <unknown>\n",
      "#7 0x55d11fedecdd <unknown>\n",
      "#8 0x55d11fee408b <unknown>\n",
      "#9 0x55d11feb9cf7 <unknown>\n",
      "#10 0x55d11fedeb4c <unknown>\n",
      "#11 0x55d11ff64a82 <unknown>\n",
      "#12 0x55d11ff45933 <unknown>\n",
      "#13 0x55d11ff13e3d <unknown>\n",
      "#14 0x55d11ff1492e <unknown>\n",
      "#15 0x55d12019fc8f <unknown>\n",
      "#16 0x55d1201a3d50 <unknown>\n",
      "#17 0x55d12018d837 <unknown>\n",
      "#18 0x55d1201a44c1 <unknown>\n",
      "#19 0x55d120174d1e <unknown>\n",
      "#20 0x55d1201c32b8 <unknown>\n",
      "#21 0x55d1201c34ba <unknown>\n",
      "#22 0x55d1201d2d5d <unknown>\n",
      "#23 0x7fcd4b8c4ac3 <unknown>\n",
      "\n",
      "Fetching: https://www.visitpittsburgh.com/blog/top-beer-festivals-to-attend-in-pittsburgh-this-fall/\n",
      "Error fetching https://www.visitpittsburgh.com/blog/top-beer-festivals-to-attend-in-pittsburgh-this-fall/: Message: timeout: Timed out receiving message from renderer: 10.000\n",
      "  (Session info: chrome=129.0.6668.100)\n",
      "Stacktrace:\n",
      "#0 0x55d1201d40ca <unknown>\n",
      "#1 0x55d11fed2a80 <unknown>\n",
      "#2 0x55d11febb073 <unknown>\n",
      "#3 0x55d11febad91 <unknown>\n",
      "#4 0x55d11feb8e58 <unknown>\n",
      "#5 0x55d11feb963f <unknown>\n",
      "#6 0x55d11fec8b57 <unknown>\n",
      "#7 0x55d11fedecdd <unknown>\n",
      "#8 0x55d11fee408b <unknown>\n",
      "#9 0x55d11feb9cf7 <unknown>\n",
      "#10 0x55d11fedeb4c <unknown>\n",
      "#11 0x55d11ff64a82 <unknown>\n",
      "#12 0x55d11ff45933 <unknown>\n",
      "#13 0x55d11ff13e3d <unknown>\n",
      "#14 0x55d11ff1492e <unknown>\n",
      "#15 0x55d12019fc8f <unknown>\n",
      "#16 0x55d1201a3d50 <unknown>\n",
      "#17 0x55d12018d837 <unknown>\n",
      "#18 0x55d1201a44c1 <unknown>\n",
      "#19 0x55d120174d1e <unknown>\n",
      "#20 0x55d1201c32b8 <unknown>\n",
      "#21 0x55d1201c34ba <unknown>\n",
      "#22 0x55d1201d2d5d <unknown>\n",
      "#23 0x7fcd4b8c4ac3 <unknown>\n",
      "\n",
      "Data saved to ../raw_documents/Pittsburgh_Food_Festivals.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import os\n",
    "\n",
    "class Scraper:\n",
    "    def __init__(self):\n",
    "        self.chrome_options = Options()\n",
    "        self.chrome_options.add_argument(\"--headless\")\n",
    "        self.chrome_options.add_argument(\"--disable-gpu\")\n",
    "        self.chrome_options.add_argument(\"--no-sandbox\")\n",
    "        self.chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "        self.driver = webdriver.Chrome(options=self.chrome_options)\n",
    "        self.driver.set_page_load_timeout(10)\n",
    "\n",
    "    def get_html(self, url: str):\n",
    "        \"\"\"Get HTML.\"\"\"\n",
    "        self.driver.get(url)\n",
    "        return self.driver.page_source\n",
    "\n",
    "    def get_soup(self, url: str):\n",
    "        \"\"\"Parse and return BeautifulSoup object.\"\"\"\n",
    "        html = self.get_html(url)\n",
    "        return BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "    def close(self):\n",
    "        self.driver.quit()\n",
    "\n",
    "    def __del__(self):\n",
    "        self.close()\n",
    "\n",
    "def organize_content_by_heading(soup: BeautifulSoup):\n",
    "    \"\"\"Organize content based on headings, ensuring all relevant text is captured.\"\"\"\n",
    "    data = {}\n",
    "    current_h1, current_h2, current_h3, current_h4, current_h5, current_h6 = None, None, None, None, None, None\n",
    "\n",
    "    for tag in soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'p', 'div', 'span']):\n",
    "        if tag.name == 'h1':\n",
    "            current_h1 = tag.get_text(strip=True)\n",
    "            data[current_h1] = {'content': \"\"}\n",
    "            current_h2 = current_h3 = current_h4 = current_h5 = current_h6 = None  # Reset lower headings\n",
    "        elif tag.name == 'h2' and current_h1:\n",
    "            current_h2 = tag.get_text(strip=True)\n",
    "            data[current_h1][current_h2] = {'content': \"\"}\n",
    "            current_h3 = current_h4 = current_h5 = current_h6 = None\n",
    "        elif tag.name == 'h3' and current_h2:\n",
    "            current_h3 = tag.get_text(strip=True)\n",
    "            data[current_h1][current_h2][current_h3] = {'content': \"\"}\n",
    "            current_h4 = current_h5 = current_h6 = None\n",
    "        elif tag.name == 'h4' and current_h3:\n",
    "            current_h4 = tag.get_text(strip=True)\n",
    "            data[current_h1][current_h2][current_h3][current_h4] = {'content': \"\"}\n",
    "            current_h5 = current_h6 = None\n",
    "        elif tag.name == 'h5' and current_h4:\n",
    "            current_h5 = tag.get_text(strip=True)\n",
    "            data[current_h1][current_h2][current_h3][current_h4][current_h5] = {'content': \"\"}\n",
    "            current_h6 = None\n",
    "        elif tag.name == 'h6' and current_h5:\n",
    "            current_h6 = tag.get_text(strip=True)\n",
    "            data[current_h1][current_h2][current_h3][current_h4][current_h5][current_h6] = {'content': \"\"}\n",
    "        else:\n",
    "            content = tag.get_text(strip=True)\n",
    "            if content:\n",
    "                if current_h6:\n",
    "                    data[current_h1][current_h2][current_h3][current_h4][current_h5][current_h6]['content'] += content + \" \"\n",
    "                elif current_h5:\n",
    "                    data[current_h1][current_h2][current_h3][current_h4][current_h5]['content'] += content + \" \"\n",
    "                elif current_h4:\n",
    "                    data[current_h1][current_h2][current_h3][current_h4]['content'] += content + \" \"\n",
    "                elif current_h3:\n",
    "                    data[current_h1][current_h2][current_h3]['content'] += content + \" \"\n",
    "                elif current_h2:\n",
    "                    data[current_h1][current_h2]['content'] += content + \" \"\n",
    "                elif current_h1:\n",
    "                    data[current_h1]['content'] += content + \" \"\n",
    "\n",
    "    return data\n",
    "\n",
    "def save_to_json(data, filename):\n",
    "    \"\"\"Save data to a JSON file, ensuring no overwrite of existing content.\"\"\"\n",
    "    try:\n",
    "        if os.path.exists(filename):\n",
    "            # 读取现有的 JSON 文件并合并\n",
    "            with open(filename, 'r', encoding='utf-8') as f:\n",
    "                existing_data = json.load(f)\n",
    "            # 合并新的数据到现有数据\n",
    "            if isinstance(existing_data, list):\n",
    "                existing_data.extend(data)  # 如果是列表，将数据合并\n",
    "            else:\n",
    "                existing_data.update(data)  # 如果是字典，更新数据\n",
    "        else:\n",
    "            existing_data = data  # 如果文件不存在，则新建\n",
    "\n",
    "        # 保存合并后的数据到文件\n",
    "        with open(filename, 'w', encoding='utf-8') as f:\n",
    "            json.dump(existing_data, f, ensure_ascii=False, indent=4)\n",
    "        print(f\"Data saved to {filename}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error writing to JSON file: {e}\")\n",
    "\n",
    "def scrape_links(scraper_, links):\n",
    "    \"\"\"Scrape multiple links and return organized data.\"\"\"\n",
    "    all_data = []\n",
    "    for link in links:\n",
    "        try:\n",
    "            print(f\"Fetching: {link}\")\n",
    "            soup = scraper_.get_soup(link)\n",
    "            # 提取内容并按照标题层级组织\n",
    "            page_data = organize_content_by_heading(soup)\n",
    "            page_data['url'] = link  # 保存爬取的链接\n",
    "            all_data.append(page_data)\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching {link}: {e}\")\n",
    "\n",
    "    return all_data\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    scraper_ = Scraper()\n",
    "\n",
    "    links = [\n",
    "        \"https://www.visitpittsburgh.com/events-festivals/food-festivals/\",\n",
    "        \"https://www.visitpittsburgh.com/blog/\",\n",
    "        \"https://www.visitpittsburgh.com/plan-your-trip/\",\n",
    "        \"https://www.visitpittsburgh.com/events-festivals/\",\n",
    "        \"https://www.visitpittsburgh.com/events-festivals/?page=2\",\n",
    "        \"https://www.visitpittsburgh.com/events-festivals/?page=3\",\n",
    "        \"https://www.visitpittsburgh.com/events-festivals/?page=4\",\n",
    "        \"https://www.visitpittsburgh.com/events-festivals/?page=5\",\n",
    "        \"https://www.visitpittsburgh.com/events-festivals/?page=6\",\n",
    "        \"https://www.visitpittsburgh.com/events-festivals/?page=7\",\n",
    "        \"https://www.visitpittsburgh.com/events-festivals/?page=8\",\n",
    "        \"https://www.visitpittsburgh.com/events-festivals/?page=9\",\n",
    "        \"https://www.visitpittsburgh.com/events-festivals/?page=10\",\n",
    "        \"https://www.visitpittsburgh.com/events-festivals/?page=11\",\n",
    "        \"https://www.visitpittsburgh.com/events-festivals/?page=12\",\n",
    "        \"https://www.visitpittsburgh.com/events-festivals/?page=13\",\n",
    "        \"https://www.visitpittsburgh.com/events-festivals/?page=14\",\n",
    "        \"https://www.visitpittsburgh.com/events-festivals/?page=15\",\n",
    "        \"https://www.visitpittsburgh.com/events-festivals/?page=16\",\n",
    "        \"https://www.visitpittsburgh.com/things-to-do/\",\n",
    "        \"https://www.visitpittsburgh.com/restaurants-culinary/\",\n",
    "        \"https://www.visitpittsburgh.com/restaurants-culinary/craft-breweries/\",\n",
    "        \"https://www.visitpittsburgh.com/things-to-do/family-fun/\",\n",
    "        \"https://www.visitpittsburgh.com/things-to-do/pittsburgh-sports-teams/\",\n",
    "        \"https://www.visitpittsburgh.com/things-to-do/arts-culture/\",\n",
    "        \"https://www.visitpittsburgh.com/meetings-and-events/\",\n",
    "        \"https://www.visitpittsburgh.com/events-festivals/annual-events/\",\n",
    "        \"https://www.visitpittsburgh.com/events-festivals/holidays/\",\n",
    "        \"https://www.visitpittsburgh.com/events-festivals/halloween-events/\",\n",
    "        \"https://www.visitpittsburgh.com/events-festivals/film-festivals/\",\n",
    "        \"https://www.visitpittsburgh.com/events-festivals/food-festivals/soul-food-festival/\",\n",
    "        \"https://www.visitpittsburgh.com/events-festivals/food-festivals/the-original-pittsburgh-taco-festival/\",\n",
    "        \"https://www.visitpittsburgh.com/blog/top-beer-festivals-to-attend-in-pittsburgh-this-fall/\"\n",
    "       \n",
    "    ]\n",
    "\n",
    "    # 爬取所有页面的内容\n",
    "    scraped_data = scrape_links(scraper_, links)\n",
    "\n",
    "    # 保存到 JSON 文件，追加到现有内容中\n",
    "    save_to_json(scraped_data, \"../raw_documents/Pittsburgh_Food_Festivals.json\")\n",
    "\n",
    "    scraper_.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching: https://www.visitpittsburgh.com/events-festivals/?page=14\n",
      "Fetching: https://www.visitpittsburgh.com/events-festivals/?page=15\n",
      "Fetching: https://www.visitpittsburgh.com/events-festivals/?page=16\n",
      "Fetching: https://www.visitpittsburgh.com/things-to-do/\n",
      "Fetching: https://www.visitpittsburgh.com/restaurants-culinary/\n",
      "Fetching: https://www.visitpittsburgh.com/restaurants-culinary/craft-breweries/\n",
      "Fetching: https://www.visitpittsburgh.com/things-to-do/family-fun/\n",
      "Fetching: https://www.visitpittsburgh.com/things-to-do/pittsburgh-sports-teams/\n",
      "Fetching: https://www.visitpittsburgh.com/things-to-do/arts-culture/\n",
      "Fetching: https://www.visitpittsburgh.com/meetings-and-events/\n",
      "Fetching: https://www.visitpittsburgh.com/events-festivals/annual-events/\n",
      "Fetching: https://www.visitpittsburgh.com/events-festivals/holidays/\n",
      "Fetching: https://www.visitpittsburgh.com/events-festivals/halloween-events/\n",
      "Fetching: https://www.visitpittsburgh.com/events-festivals/film-festivals/\n",
      "Fetching: https://www.visitpittsburgh.com/events-festivals/food-festivals/soul-food-festival/\n",
      "Fetching: https://www.visitpittsburgh.com/events-festivals/food-festivals/the-original-pittsburgh-taco-festival/\n",
      "Fetching: https://www.visitpittsburgh.com/blog/top-beer-festivals-to-attend-in-pittsburgh-this-fall/\n",
      "Data saved to ../raw_documents/Pittsburgh_Food_Festivals.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import os\n",
    "\n",
    "class Scraper:\n",
    "    def __init__(self):\n",
    "        self.chrome_options = Options()\n",
    "        self.chrome_options.add_argument(\"--headless\")\n",
    "        self.chrome_options.add_argument(\"--disable-gpu\")\n",
    "        self.chrome_options.add_argument(\"--no-sandbox\")\n",
    "        self.chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "        self.driver = webdriver.Chrome(options=self.chrome_options)\n",
    "        self.driver.set_page_load_timeout(10)\n",
    "\n",
    "    def get_html(self, url: str):\n",
    "        \"\"\"Get HTML with explicit wait for dynamic content.\"\"\"\n",
    "        self.driver.get(url)\n",
    "        \n",
    "        try:\n",
    "            # Explicit wait for a specific element that ensures page has loaded dynamically\n",
    "            WebDriverWait(self.driver, 10).until(\n",
    "                EC.presence_of_element_located((By.TAG_NAME, \"body\"))  # Example: wait for body tag to appear\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"Error while waiting for page to load: {e}\")\n",
    "        \n",
    "        return self.driver.page_source\n",
    "\n",
    "    def get_soup(self, url: str):\n",
    "        \"\"\"Parse and return BeautifulSoup object.\"\"\"\n",
    "        html = self.get_html(url)\n",
    "        return BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "    def close(self):\n",
    "        self.driver.quit()\n",
    "\n",
    "    def __del__(self):\n",
    "        self.close()\n",
    "\n",
    "def organize_content_by_heading(soup: BeautifulSoup):\n",
    "    \"\"\"Organize content based on headings, ensuring all relevant text is captured.\"\"\"\n",
    "    data = {}\n",
    "    current_h1, current_h2, current_h3, current_h4, current_h5, current_h6 = None, None, None, None, None, None\n",
    "\n",
    "    for tag in soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'p', 'div', 'span']):\n",
    "        if tag.name == 'h1':\n",
    "            current_h1 = tag.get_text(strip=True)\n",
    "            data[current_h1] = {'content': \"\"}\n",
    "            current_h2 = current_h3 = current_h4 = current_h5 = current_h6 = None  # Reset lower headings\n",
    "        elif tag.name == 'h2' and current_h1:\n",
    "            current_h2 = tag.get_text(strip=True)\n",
    "            data[current_h1][current_h2] = {'content': \"\"}\n",
    "            current_h3 = current_h4 = current_h5 = current_h6 = None\n",
    "        elif tag.name == 'h3' and current_h2:\n",
    "            current_h3 = tag.get_text(strip=True)\n",
    "            data[current_h1][current_h2][current_h3] = {'content': \"\"}\n",
    "            current_h4 = current_h5 = current_h6 = None\n",
    "        elif tag.name == 'h4' and current_h3:\n",
    "            current_h4 = tag.get_text(strip=True)\n",
    "            data[current_h1][current_h2][current_h3][current_h4] = {'content': \"\"}\n",
    "            current_h5 = current_h6 = None\n",
    "        elif tag.name == 'h5' and current_h4:\n",
    "            current_h5 = tag.get_text(strip=True)\n",
    "            data[current_h1][current_h2][current_h3][current_h4][current_h5] = {'content': \"\"}\n",
    "            current_h6 = None\n",
    "        elif tag.name == 'h6' and current_h5:\n",
    "            current_h6 = tag.get_text(strip=True)\n",
    "            data[current_h1][current_h2][current_h3][current_h4][current_h5][current_h6] = {'content': \"\"}\n",
    "        else:\n",
    "            content = tag.get_text(strip=True)\n",
    "            if content:\n",
    "                if current_h6:\n",
    "                    data[current_h1][current_h2][current_h3][current_h4][current_h5][current_h6]['content'] += content + \" \"\n",
    "                elif current_h5:\n",
    "                    data[current_h1][current_h2][current_h3][current_h4][current_h5]['content'] += content + \" \"\n",
    "                elif current_h4:\n",
    "                    data[current_h1][current_h2][current_h3][current_h4]['content'] += content + \" \"\n",
    "                elif current_h3:\n",
    "                    data[current_h1][current_h2][current_h3]['content'] += content + \" \"\n",
    "                elif current_h2:\n",
    "                    data[current_h1][current_h2]['content'] += content + \" \"\n",
    "                elif current_h1:\n",
    "                    data[current_h1]['content'] += content + \" \"\n",
    "\n",
    "    return data\n",
    "\n",
    "def save_to_json(data, filename):\n",
    "    \"\"\"Save data to a JSON file, ensuring no overwrite of existing content.\"\"\"\n",
    "    try:\n",
    "        if os.path.exists(filename):\n",
    "            # 读取现有的 JSON 文件并合并\n",
    "            with open(filename, 'r', encoding='utf-8') as f:\n",
    "                existing_data = json.load(f)\n",
    "            # 合并新的数据到现有数据\n",
    "            if isinstance(existing_data, list):\n",
    "                existing_data.extend(data)  # 如果是列表，将数据合并\n",
    "            else:\n",
    "                existing_data.update(data)  # 如果是字典，更新数据\n",
    "        else:\n",
    "            existing_data = data  # 如果文件不存在，则新建\n",
    "\n",
    "        # 保存合并后的数据到文件\n",
    "        with open(filename, 'w', encoding='utf-8') as f:\n",
    "            json.dump(existing_data, f, ensure_ascii=False, indent=4)\n",
    "        print(f\"Data saved to {filename}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error writing to JSON file: {e}\")\n",
    "\n",
    "def scrape_links(scraper_, links):\n",
    "    \"\"\"Scrape multiple links and return organized data.\"\"\"\n",
    "    all_data = []\n",
    "    for link in links:\n",
    "        try:\n",
    "            print(f\"Fetching: {link}\")\n",
    "            soup = scraper_.get_soup(link)\n",
    "            # 提取内容并按照标题层级组织\n",
    "            page_data = organize_content_by_heading(soup)\n",
    "            page_data['url'] = link  # 保存爬取的链接\n",
    "            all_data.append(page_data)\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching {link}: {e}\")\n",
    "\n",
    "    return all_data\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    scraper_ = Scraper()\n",
    "\n",
    "    links = [\n",
    "        \"https://www.visitpittsburgh.com/events-festivals/?page=14\",\n",
    "        \"https://www.visitpittsburgh.com/events-festivals/?page=15\",\n",
    "        \"https://www.visitpittsburgh.com/events-festivals/?page=16\",\n",
    "        \"https://www.visitpittsburgh.com/things-to-do/\",\n",
    "        \"https://www.visitpittsburgh.com/restaurants-culinary/\",\n",
    "        \"https://www.visitpittsburgh.com/restaurants-culinary/craft-breweries/\",\n",
    "        \"https://www.visitpittsburgh.com/things-to-do/family-fun/\",\n",
    "        \"https://www.visitpittsburgh.com/things-to-do/pittsburgh-sports-teams/\",\n",
    "        \"https://www.visitpittsburgh.com/things-to-do/arts-culture/\",\n",
    "        \"https://www.visitpittsburgh.com/meetings-and-events/\",\n",
    "        \"https://www.visitpittsburgh.com/events-festivals/annual-events/\",\n",
    "        \"https://www.visitpittsburgh.com/events-festivals/holidays/\",\n",
    "        \"https://www.visitpittsburgh.com/events-festivals/halloween-events/\",\n",
    "        \"https://www.visitpittsburgh.com/events-festivals/film-festivals/\",\n",
    "        \"https://www.visitpittsburgh.com/events-festivals/food-festivals/soul-food-festival/\",\n",
    "        \"https://www.visitpittsburgh.com/events-festivals/food-festivals/the-original-pittsburgh-taco-festival/\",\n",
    "        \"https://www.visitpittsburgh.com/blog/top-beer-festivals-to-attend-in-pittsburgh-this-fall/\"\n",
    "    ]\n",
    "\n",
    "    # 爬取所有页面的内容\n",
    "    scraped_data = scrape_links(scraper_, links)\n",
    "\n",
    "    # 保存到 JSON 文件，追加到现有内容中\n",
    "    save_to_json(scraped_data, \"../raw_documents/Pittsburgh_Food_Festivals.json\")\n",
    "\n",
    "    scraper_.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Picklesburgh\n",
    "### https://www.picklesburgh.com/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching: https://www.picklesburgh.com/\n",
      "Fetching: https://www.picklesburgh.com/vendors/\n",
      "Fetching: https://www.picklesburgh.com/entertainment/\n",
      "Fetching: https://www.picklesburgh.com/games/\n",
      "Fetching: https://www.picklesburgh.com/festival-schedule/lil-gherkins-activity-area/\n",
      "Fetching: https://www.picklesburgh.com/taste-of-picklesburgh/\n",
      "Fetching: https://www.picklesburgh.com/news/\n",
      "Fetching: https://www.picklesburgh.com/accessibility/\n",
      "Fetching: https://www.picklesburgh.com/visit/getting-here/\n",
      "Data saved to ../raw_documents/Picklesburgh.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import os\n",
    "\n",
    "class Scraper:\n",
    "    def __init__(self):\n",
    "        self.chrome_options = Options()\n",
    "        self.chrome_options.add_argument(\"--headless\")\n",
    "        self.chrome_options.add_argument(\"--disable-gpu\")\n",
    "        self.chrome_options.add_argument(\"--no-sandbox\")\n",
    "        self.chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "        self.driver = webdriver.Chrome(options=self.chrome_options)\n",
    "        self.driver.set_page_load_timeout(10)\n",
    "\n",
    "    def get_html(self, url: str):\n",
    "        \"\"\"Get HTML.\"\"\"\n",
    "        self.driver.get(url)\n",
    "        return self.driver.page_source\n",
    "\n",
    "    def get_soup(self, url: str):\n",
    "        \"\"\"Parse and return BeautifulSoup object.\"\"\"\n",
    "        html = self.get_html(url)\n",
    "        return BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "    def close(self):\n",
    "        self.driver.quit()\n",
    "\n",
    "    def __del__(self):\n",
    "        self.close()\n",
    "\n",
    "def organize_content_by_heading(soup: BeautifulSoup):\n",
    "    \"\"\"Organize content based on headings, ensuring all relevant text is captured.\"\"\"\n",
    "    data = {}\n",
    "    current_h1, current_h2, current_h3, current_h4, current_h5, current_h6 = None, None, None, None, None, None\n",
    "\n",
    "    for tag in soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'p', 'div', 'span']):\n",
    "        if tag.name == 'h1':\n",
    "            current_h1 = tag.get_text(strip=True)\n",
    "            data[current_h1] = {'content': \"\"}\n",
    "            current_h2 = current_h3 = current_h4 = current_h5 = current_h6 = None  # Reset lower headings\n",
    "        elif tag.name == 'h2' and current_h1:\n",
    "            current_h2 = tag.get_text(strip=True)\n",
    "            data[current_h1][current_h2] = {'content': \"\"}\n",
    "            current_h3 = current_h4 = current_h5 = current_h6 = None\n",
    "        elif tag.name == 'h3' and current_h2:\n",
    "            current_h3 = tag.get_text(strip=True)\n",
    "            data[current_h1][current_h2][current_h3] = {'content': \"\"}\n",
    "            current_h4 = current_h5 = current_h6 = None\n",
    "        elif tag.name == 'h4' and current_h3:\n",
    "            current_h4 = tag.get_text(strip=True)\n",
    "            data[current_h1][current_h2][current_h3][current_h4] = {'content': \"\"}\n",
    "            current_h5 = current_h6 = None\n",
    "        elif tag.name == 'h5' and current_h4:\n",
    "            current_h5 = tag.get_text(strip=True)\n",
    "            data[current_h1][current_h2][current_h3][current_h4][current_h5] = {'content': \"\"}\n",
    "            current_h6 = None\n",
    "        elif tag.name == 'h6' and current_h5:\n",
    "            current_h6 = tag.get_text(strip=True)\n",
    "            data[current_h1][current_h2][current_h3][current_h4][current_h5][current_h6] = {'content': \"\"}\n",
    "        else:\n",
    "            content = tag.get_text(strip=True)\n",
    "            if content:\n",
    "                if current_h6:\n",
    "                    data[current_h1][current_h2][current_h3][current_h4][current_h5][current_h6]['content'] += content + \" \"\n",
    "                elif current_h5:\n",
    "                    data[current_h1][current_h2][current_h3][current_h4][current_h5]['content'] += content + \" \"\n",
    "                elif current_h4:\n",
    "                    data[current_h1][current_h2][current_h3][current_h4]['content'] += content + \" \"\n",
    "                elif current_h3:\n",
    "                    data[current_h1][current_h2][current_h3]['content'] += content + \" \"\n",
    "                elif current_h2:\n",
    "                    data[current_h1][current_h2]['content'] += content + \" \"\n",
    "                elif current_h1:\n",
    "                    data[current_h1]['content'] += content + \" \"\n",
    "\n",
    "    return data\n",
    "\n",
    "def save_to_json(data, filename):\n",
    "    \"\"\"Save data to a JSON file, ensuring no overwrite of existing content.\"\"\"\n",
    "    try:\n",
    "        if os.path.exists(filename):\n",
    "            # 读取现有的 JSON 文件并合并\n",
    "            with open(filename, 'r', encoding='utf-8') as f:\n",
    "                existing_data = json.load(f)\n",
    "            # 合并新的数据到现有数据\n",
    "            if isinstance(existing_data, list):\n",
    "                existing_data.extend(data)  # 如果是列表，将数据合并\n",
    "            else:\n",
    "                existing_data.update(data)  # 如果是字典，更新数据\n",
    "        else:\n",
    "            existing_data = data  # 如果文件不存在，则新建\n",
    "\n",
    "        # 保存合并后的数据到文件\n",
    "        with open(filename, 'w', encoding='utf-8') as f:\n",
    "            json.dump(existing_data, f, ensure_ascii=False, indent=4)\n",
    "        print(f\"Data saved to {filename}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error writing to JSON file: {e}\")\n",
    "\n",
    "def scrape_links(scraper_, links):\n",
    "    \"\"\"Scrape multiple links and return organized data.\"\"\"\n",
    "    all_data = []\n",
    "    for link in links:\n",
    "        try:\n",
    "            print(f\"Fetching: {link}\")\n",
    "            soup = scraper_.get_soup(link)\n",
    "            # 提取内容并按照标题层级组织\n",
    "            page_data = organize_content_by_heading(soup)\n",
    "            page_data['url'] = link  # 保存爬取的链接\n",
    "            all_data.append(page_data)\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching {link}: {e}\")\n",
    "\n",
    "    return all_data\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    scraper_ = Scraper()\n",
    "\n",
    "    links = [\n",
    "        \"https://www.picklesburgh.com/\",\n",
    "        \"https://www.picklesburgh.com/vendors/\",\n",
    "        \"https://www.picklesburgh.com/entertainment/\",\n",
    "        \"https://www.picklesburgh.com/games/\",\n",
    "        \"https://www.picklesburgh.com/festival-schedule/lil-gherkins-activity-area/\",\n",
    "        \"https://www.picklesburgh.com/taste-of-picklesburgh/\",\n",
    "        \"https://www.picklesburgh.com/news/\",\n",
    "        \"https://www.picklesburgh.com/accessibility/\",\n",
    "        \"https://www.picklesburgh.com/visit/getting-here/\"\n",
    "       \n",
    "    ]\n",
    "\n",
    "    # 爬取所有页面的内容\n",
    "    scraped_data = scrape_links(scraper_, links)\n",
    "\n",
    "    # 保存到 JSON 文件，追加到现有内容中\n",
    "    save_to_json(scraped_data, \"../raw_documents/Picklesburgh.json\")\n",
    "\n",
    "    scraper_.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pittsburgh Taco Fest\n",
    "### https://www.pghtacofest.com/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching: https://www.pghtacofest.com/\n",
      "Fetching: https://www.pghtacofest.com/about\n",
      "Fetching: https://www.pghtacofest.com/vendors\n",
      "Fetching: https://www.pghtacofest.com/faqs\n",
      "Data saved to ../raw_documents/Pittsburgh_Taco_Festival.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import os\n",
    "\n",
    "class Scraper:\n",
    "    def __init__(self):\n",
    "        self.chrome_options = Options()\n",
    "        self.chrome_options.add_argument(\"--headless\")\n",
    "        self.chrome_options.add_argument(\"--disable-gpu\")\n",
    "        self.chrome_options.add_argument(\"--no-sandbox\")\n",
    "        self.chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "        self.driver = webdriver.Chrome(options=self.chrome_options)\n",
    "        self.driver.set_page_load_timeout(10)\n",
    "\n",
    "    def get_html(self, url: str):\n",
    "        \"\"\"Get HTML.\"\"\"\n",
    "        self.driver.get(url)\n",
    "        return self.driver.page_source\n",
    "\n",
    "    def get_soup(self, url: str):\n",
    "        \"\"\"Parse and return BeautifulSoup object.\"\"\"\n",
    "        html = self.get_html(url)\n",
    "        return BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "    def close(self):\n",
    "        self.driver.quit()\n",
    "\n",
    "    def __del__(self):\n",
    "        self.close()\n",
    "\n",
    "def organize_content_by_heading(soup: BeautifulSoup):\n",
    "    \"\"\"Organize content based on headings, ensuring all relevant text is captured.\"\"\"\n",
    "    data = {}\n",
    "    current_h1, current_h2, current_h3, current_h4, current_h5, current_h6 = None, None, None, None, None, None\n",
    "\n",
    "    for tag in soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'p', 'div', 'span']):\n",
    "        if tag.name == 'h1':\n",
    "            current_h1 = tag.get_text(strip=True)\n",
    "            data[current_h1] = {'content': \"\"}\n",
    "            current_h2 = current_h3 = current_h4 = current_h5 = current_h6 = None  # Reset lower headings\n",
    "        elif tag.name == 'h2' and current_h1:\n",
    "            current_h2 = tag.get_text(strip=True)\n",
    "            data[current_h1][current_h2] = {'content': \"\"}\n",
    "            current_h3 = current_h4 = current_h5 = current_h6 = None\n",
    "        elif tag.name == 'h3' and current_h2:\n",
    "            current_h3 = tag.get_text(strip=True)\n",
    "            data[current_h1][current_h2][current_h3] = {'content': \"\"}\n",
    "            current_h4 = current_h5 = current_h6 = None\n",
    "        elif tag.name == 'h4' and current_h3:\n",
    "            current_h4 = tag.get_text(strip=True)\n",
    "            data[current_h1][current_h2][current_h3][current_h4] = {'content': \"\"}\n",
    "            current_h5 = current_h6 = None\n",
    "        elif tag.name == 'h5' and current_h4:\n",
    "            current_h5 = tag.get_text(strip=True)\n",
    "            data[current_h1][current_h2][current_h3][current_h4][current_h5] = {'content': \"\"}\n",
    "            current_h6 = None\n",
    "        elif tag.name == 'h6' and current_h5:\n",
    "            current_h6 = tag.get_text(strip=True)\n",
    "            data[current_h1][current_h2][current_h3][current_h4][current_h5][current_h6] = {'content': \"\"}\n",
    "        else:\n",
    "            content = tag.get_text(strip=True)\n",
    "            if content:\n",
    "                if current_h6:\n",
    "                    data[current_h1][current_h2][current_h3][current_h4][current_h5][current_h6]['content'] += content + \" \"\n",
    "                elif current_h5:\n",
    "                    data[current_h1][current_h2][current_h3][current_h4][current_h5]['content'] += content + \" \"\n",
    "                elif current_h4:\n",
    "                    data[current_h1][current_h2][current_h3][current_h4]['content'] += content + \" \"\n",
    "                elif current_h3:\n",
    "                    data[current_h1][current_h2][current_h3]['content'] += content + \" \"\n",
    "                elif current_h2:\n",
    "                    data[current_h1][current_h2]['content'] += content + \" \"\n",
    "                elif current_h1:\n",
    "                    data[current_h1]['content'] += content + \" \"\n",
    "\n",
    "    return data\n",
    "\n",
    "def save_to_json(data, filename):\n",
    "    \"\"\"Save data to a JSON file, ensuring no overwrite of existing content.\"\"\"\n",
    "    try:\n",
    "        if os.path.exists(filename):\n",
    "            # 读取现有的 JSON 文件并合并\n",
    "            with open(filename, 'r', encoding='utf-8') as f:\n",
    "                existing_data = json.load(f)\n",
    "            # 合并新的数据到现有数据\n",
    "            if isinstance(existing_data, list):\n",
    "                existing_data.extend(data)  # 如果是列表，将数据合并\n",
    "            else:\n",
    "                existing_data.update(data)  # 如果是字典，更新数据\n",
    "        else:\n",
    "            existing_data = data  # 如果文件不存在，则新建\n",
    "\n",
    "        # 保存合并后的数据到文件\n",
    "        with open(filename, 'w', encoding='utf-8') as f:\n",
    "            json.dump(existing_data, f, ensure_ascii=False, indent=4)\n",
    "        print(f\"Data saved to {filename}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error writing to JSON file: {e}\")\n",
    "\n",
    "def scrape_links(scraper_, links):\n",
    "    \"\"\"Scrape multiple links and return organized data.\"\"\"\n",
    "    all_data = []\n",
    "    for link in links:\n",
    "        try:\n",
    "            print(f\"Fetching: {link}\")\n",
    "            soup = scraper_.get_soup(link)\n",
    "            # 提取内容并按照标题层级组织\n",
    "            page_data = organize_content_by_heading(soup)\n",
    "            page_data['url'] = link  # 保存爬取的链接\n",
    "            all_data.append(page_data)\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching {link}: {e}\")\n",
    "\n",
    "    return all_data\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    scraper_ = Scraper()\n",
    "\n",
    "    links = [\n",
    "        \"https://www.pghtacofest.com/\",\n",
    "        \"https://www.pghtacofest.com/about\",\n",
    "        \"https://www.pghtacofest.com/vendors\",\n",
    "        \"https://www.pghtacofest.com/faqs\"\n",
    "       \n",
    "    ]\n",
    "\n",
    "    # 爬取所有页面的内容\n",
    "    scraped_data = scrape_links(scraper_, links)\n",
    "\n",
    "    # 保存到 JSON 文件，追加到现有内容中\n",
    "    save_to_json(scraped_data, \"../raw_documents/Pittsburgh_Taco_Festival.json\")\n",
    "\n",
    "    scraper_.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pittsburgh Restaurant Week\n",
    "### https://pittsburghrestaurantweek.com/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching: https://pittsburghrestaurantweek.com/\n",
      "Fetching: https://pittsburghrestaurantweek.com/about/\n",
      "Fetching: https://pittsburghrestaurantweek.com/about/history/\n",
      "Fetching: https://pittsburghrestaurantweek.com/restaurants/\n",
      "Fetching: https://pittsburghrestaurantweek.com/restaurants/summer-2024-restaurants/\n",
      "Fetching: https://pittsburghrestaurantweek.com/restaurants/winter-2024-restaurants/\n",
      "Fetching: https://pittsburghrestaurantweek.com/restaurants/summer-2023-restaurants/\n",
      "Fetching: https://pittsburghrestaurantweek.com/restaurants/winter-2023-restaurants/\n",
      "Fetching: https://pittsburghrestaurantweek.com/restaurants/summer-2022-restaurants/\n",
      "Fetching: https://pittsburghrestaurantweek.com/restaurants/winter-2022-restaurants/\n",
      "Fetching: https://pittsburghrestaurantweek.com/restaurants/summer-2021-restaurants/\n",
      "Fetching: https://pittsburghrestaurantweek.com/restaurants/winter-2021-restaurants/\n",
      "Fetching: https://pittsburghrestaurantweek.com/restaurants/summer-2020-restaurants/\n",
      "Fetching: https://pittsburghrestaurantweek.com/restaurants/winter-2020-restaurants/\n",
      "Fetching: https://pittsburghrestaurantweek.com/restaurants/summer-2019-restaurants/\n",
      "Fetching: https://pittsburghrestaurantweek.com/restaurants/winter-2019-restaurants/\n",
      "Fetching: https://pittsburghrestaurantweek.com/restaurants/summer-2018-restaurants/\n",
      "Fetching: https://pittsburghrestaurantweek.com/restaurants/winter-2018-restaurants/\n",
      "Fetching: https://pittsburghrestaurantweek.com/restaurants/summer-2017-restaurants/\n",
      "Fetching: https://pittsburghrestaurantweek.com/restaurants/winter-2017-restaurants/\n",
      "Fetching: https://pittsburghrestaurantweek.com/restaurants/summer-2016-restaurants/\n",
      "Fetching: https://pittsburghrestaurantweek.com/restaurants/winter-2016-restaurants/\n",
      "Fetching: https://pittsburghrestaurantweek.com/restaurants/summer-2015-restaurants/\n",
      "Fetching: https://pittsburghrestaurantweek.com/restaurants/winter-2015-restaurants/\n",
      "Fetching: https://pittsburghrestaurantweek.com/restaurants/summer-2014-restaurants/\n",
      "Fetching: https://pittsburghrestaurantweek.com/restaurants/winter-2014-restaurants/\n",
      "Fetching: https://pittsburghrestaurantweek.com/restaurants/summer-2013-restaurants/\n",
      "Fetching: https://pittsburghrestaurantweek.com/restaurants/winter-2013-restaurants/\n",
      "Fetching: https://pittsburghrestaurantweek.com/restaurants/summer-2012-restaurants/\n",
      "Fetching: https://pittsburghrestaurantweek.com/restaurants/winter-2012-restaurants/\n",
      "Data saved to ../raw_documents/Pittsburgh_Restaurant_Week.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import os\n",
    "\n",
    "class Scraper:\n",
    "    def __init__(self):\n",
    "        self.chrome_options = Options()\n",
    "        self.chrome_options.add_argument(\"--headless\")\n",
    "        self.chrome_options.add_argument(\"--disable-gpu\")\n",
    "        self.chrome_options.add_argument(\"--no-sandbox\")\n",
    "        self.chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "        self.driver = webdriver.Chrome(options=self.chrome_options)\n",
    "        self.driver.set_page_load_timeout(10)\n",
    "\n",
    "    def get_html(self, url: str):\n",
    "        \"\"\"Get HTML.\"\"\"\n",
    "        self.driver.get(url)\n",
    "        return self.driver.page_source\n",
    "\n",
    "    def get_soup(self, url: str):\n",
    "        \"\"\"Parse and return BeautifulSoup object.\"\"\"\n",
    "        html = self.get_html(url)\n",
    "        return BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "    def close(self):\n",
    "        self.driver.quit()\n",
    "\n",
    "    def __del__(self):\n",
    "        self.close()\n",
    "\n",
    "def organize_content_by_heading(soup: BeautifulSoup):\n",
    "    \"\"\"Organize content based on headings, ensuring all relevant text is captured.\"\"\"\n",
    "    data = {}\n",
    "    current_h1, current_h2, current_h3, current_h4, current_h5, current_h6 = None, None, None, None, None, None\n",
    "\n",
    "    for tag in soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'p', 'div', 'span']):\n",
    "        if tag.name == 'h1':\n",
    "            current_h1 = tag.get_text(strip=True)\n",
    "            data[current_h1] = {'content': \"\"}\n",
    "            current_h2 = current_h3 = current_h4 = current_h5 = current_h6 = None  # Reset lower headings\n",
    "        elif tag.name == 'h2' and current_h1:\n",
    "            current_h2 = tag.get_text(strip=True)\n",
    "            data[current_h1][current_h2] = {'content': \"\"}\n",
    "            current_h3 = current_h4 = current_h5 = current_h6 = None\n",
    "        elif tag.name == 'h3' and current_h2:\n",
    "            current_h3 = tag.get_text(strip=True)\n",
    "            data[current_h1][current_h2][current_h3] = {'content': \"\"}\n",
    "            current_h4 = current_h5 = current_h6 = None\n",
    "        elif tag.name == 'h4' and current_h3:\n",
    "            current_h4 = tag.get_text(strip=True)\n",
    "            data[current_h1][current_h2][current_h3][current_h4] = {'content': \"\"}\n",
    "            current_h5 = current_h6 = None\n",
    "        elif tag.name == 'h5' and current_h4:\n",
    "            current_h5 = tag.get_text(strip=True)\n",
    "            data[current_h1][current_h2][current_h3][current_h4][current_h5] = {'content': \"\"}\n",
    "            current_h6 = None\n",
    "        elif tag.name == 'h6' and current_h5:\n",
    "            current_h6 = tag.get_text(strip=True)\n",
    "            data[current_h1][current_h2][current_h3][current_h4][current_h5][current_h6] = {'content': \"\"}\n",
    "        else:\n",
    "            content = tag.get_text(strip=True)\n",
    "            if content:\n",
    "                if current_h6:\n",
    "                    data[current_h1][current_h2][current_h3][current_h4][current_h5][current_h6]['content'] += content + \" \"\n",
    "                elif current_h5:\n",
    "                    data[current_h1][current_h2][current_h3][current_h4][current_h5]['content'] += content + \" \"\n",
    "                elif current_h4:\n",
    "                    data[current_h1][current_h2][current_h3][current_h4]['content'] += content + \" \"\n",
    "                elif current_h3:\n",
    "                    data[current_h1][current_h2][current_h3]['content'] += content + \" \"\n",
    "                elif current_h2:\n",
    "                    data[current_h1][current_h2]['content'] += content + \" \"\n",
    "                elif current_h1:\n",
    "                    data[current_h1]['content'] += content + \" \"\n",
    "\n",
    "    return data\n",
    "\n",
    "def save_to_json(data, filename):\n",
    "    \"\"\"Save data to a JSON file, ensuring no overwrite of existing content.\"\"\"\n",
    "    try:\n",
    "        if os.path.exists(filename):\n",
    "            # 读取现有的 JSON 文件并合并\n",
    "            with open(filename, 'r', encoding='utf-8') as f:\n",
    "                existing_data = json.load(f)\n",
    "            # 合并新的数据到现有数据\n",
    "            if isinstance(existing_data, list):\n",
    "                existing_data.extend(data)  # 如果是列表，将数据合并\n",
    "            else:\n",
    "                existing_data.update(data)  # 如果是字典，更新数据\n",
    "        else:\n",
    "            existing_data = data  # 如果文件不存在，则新建\n",
    "\n",
    "        # 保存合并后的数据到文件\n",
    "        with open(filename, 'w', encoding='utf-8') as f:\n",
    "            json.dump(existing_data, f, ensure_ascii=False, indent=4)\n",
    "        print(f\"Data saved to {filename}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error writing to JSON file: {e}\")\n",
    "\n",
    "def scrape_links(scraper_, links):\n",
    "    \"\"\"Scrape multiple links and return organized data.\"\"\"\n",
    "    all_data = []\n",
    "    for link in links:\n",
    "        try:\n",
    "            print(f\"Fetching: {link}\")\n",
    "            soup = scraper_.get_soup(link)\n",
    "            # 提取内容并按照标题层级组织\n",
    "            page_data = organize_content_by_heading(soup)\n",
    "            page_data['url'] = link  # 保存爬取的链接\n",
    "            all_data.append(page_data)\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching {link}: {e}\")\n",
    "\n",
    "    return all_data\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    scraper_ = Scraper()\n",
    "\n",
    "    links = [\n",
    "        \"https://pittsburghrestaurantweek.com/\",\n",
    "        \"https://pittsburghrestaurantweek.com/about/\",\n",
    "        \"https://pittsburghrestaurantweek.com/about/history/\",\n",
    "        \"https://pittsburghrestaurantweek.com/restaurants/\",\n",
    "        \"https://pittsburghrestaurantweek.com/restaurants/summer-2024-restaurants/\",\n",
    "        \"https://pittsburghrestaurantweek.com/restaurants/winter-2024-restaurants/\",\n",
    "        \"https://pittsburghrestaurantweek.com/restaurants/summer-2023-restaurants/\",\n",
    "        \"https://pittsburghrestaurantweek.com/restaurants/winter-2023-restaurants/\",\n",
    "        \"https://pittsburghrestaurantweek.com/restaurants/summer-2022-restaurants/\",\n",
    "        \"https://pittsburghrestaurantweek.com/restaurants/winter-2022-restaurants/\",\n",
    "        \"https://pittsburghrestaurantweek.com/restaurants/summer-2021-restaurants/\",\n",
    "        \"https://pittsburghrestaurantweek.com/restaurants/winter-2021-restaurants/\",\n",
    "        \"https://pittsburghrestaurantweek.com/restaurants/summer-2020-restaurants/\",\n",
    "        \"https://pittsburghrestaurantweek.com/restaurants/winter-2020-restaurants/\",\n",
    "        \"https://pittsburghrestaurantweek.com/restaurants/summer-2019-restaurants/\",\n",
    "        \"https://pittsburghrestaurantweek.com/restaurants/winter-2019-restaurants/\",\n",
    "        \"https://pittsburghrestaurantweek.com/restaurants/summer-2018-restaurants/\",\n",
    "        \"https://pittsburghrestaurantweek.com/restaurants/winter-2018-restaurants/\",\n",
    "        \"https://pittsburghrestaurantweek.com/restaurants/summer-2017-restaurants/\",\n",
    "        \"https://pittsburghrestaurantweek.com/restaurants/winter-2017-restaurants/\",\n",
    "        \"https://pittsburghrestaurantweek.com/restaurants/summer-2016-restaurants/\",\n",
    "        \"https://pittsburghrestaurantweek.com/restaurants/winter-2016-restaurants/\",\n",
    "        \"https://pittsburghrestaurantweek.com/restaurants/summer-2015-restaurants/\",\n",
    "        \"https://pittsburghrestaurantweek.com/restaurants/winter-2015-restaurants/\",\n",
    "        \"https://pittsburghrestaurantweek.com/restaurants/summer-2014-restaurants/\",\n",
    "        \"https://pittsburghrestaurantweek.com/restaurants/winter-2014-restaurants/\",\n",
    "        \"https://pittsburghrestaurantweek.com/restaurants/summer-2013-restaurants/\",\n",
    "        \"https://pittsburghrestaurantweek.com/restaurants/winter-2013-restaurants/\",\n",
    "        \"https://pittsburghrestaurantweek.com/restaurants/summer-2012-restaurants/\",\n",
    "        \"https://pittsburghrestaurantweek.com/restaurants/winter-2012-restaurants/\"\n",
    "       \n",
    "    ]\n",
    "\n",
    "    # 爬取所有页面的内容\n",
    "    scraped_data = scrape_links(scraper_, links)\n",
    "\n",
    "    # 保存到 JSON 文件，追加到现有内容中\n",
    "    save_to_json(scraped_data, \"../raw_documents/Pittsburgh_Restaurant_Week.json\")\n",
    "\n",
    "    scraper_.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Little Italy Days\n",
    "### https://littleitalydays.com/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching: https://littleitalydays.com/\n",
      "Fetching: https://littleitalydays.com/entertainment-schedule/\n",
      "Fetching: https://littleitalydays.com/getting-around/\n",
      "Fetching: https://littleitalydays.com/about-us/\n",
      "Fetching: https://littleitalydays.com/faq/\n",
      "Fetching: https://littleitalydays.com/bloomfield-businesses/\n",
      "Data saved to ../raw_documents/Pittsburgh_Little_Italy_Day.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import os\n",
    "\n",
    "class Scraper:\n",
    "    def __init__(self):\n",
    "        self.chrome_options = Options()\n",
    "        self.chrome_options.add_argument(\"--headless\")\n",
    "        self.chrome_options.add_argument(\"--disable-gpu\")\n",
    "        self.chrome_options.add_argument(\"--no-sandbox\")\n",
    "        self.chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "        self.driver = webdriver.Chrome(options=self.chrome_options)\n",
    "        self.driver.set_page_load_timeout(10)\n",
    "\n",
    "    def get_html(self, url: str):\n",
    "        \"\"\"Get HTML.\"\"\"\n",
    "        self.driver.get(url)\n",
    "        return self.driver.page_source\n",
    "\n",
    "    def get_soup(self, url: str):\n",
    "        \"\"\"Parse and return BeautifulSoup object.\"\"\"\n",
    "        html = self.get_html(url)\n",
    "        return BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "    def close(self):\n",
    "        self.driver.quit()\n",
    "\n",
    "    def __del__(self):\n",
    "        self.close()\n",
    "\n",
    "def organize_content_by_heading(soup: BeautifulSoup):\n",
    "    \"\"\"Organize content based on headings, ensuring all relevant text is captured.\"\"\"\n",
    "    data = {}\n",
    "    current_h1, current_h2, current_h3, current_h4, current_h5, current_h6 = None, None, None, None, None, None\n",
    "\n",
    "    for tag in soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'p', 'div', 'span']):\n",
    "        if tag.name == 'h1':\n",
    "            current_h1 = tag.get_text(strip=True)\n",
    "            data[current_h1] = {'content': \"\"}\n",
    "            current_h2 = current_h3 = current_h4 = current_h5 = current_h6 = None  # Reset lower headings\n",
    "        elif tag.name == 'h2' and current_h1:\n",
    "            current_h2 = tag.get_text(strip=True)\n",
    "            data[current_h1][current_h2] = {'content': \"\"}\n",
    "            current_h3 = current_h4 = current_h5 = current_h6 = None\n",
    "        elif tag.name == 'h3' and current_h2:\n",
    "            current_h3 = tag.get_text(strip=True)\n",
    "            data[current_h1][current_h2][current_h3] = {'content': \"\"}\n",
    "            current_h4 = current_h5 = current_h6 = None\n",
    "        elif tag.name == 'h4' and current_h3:\n",
    "            current_h4 = tag.get_text(strip=True)\n",
    "            data[current_h1][current_h2][current_h3][current_h4] = {'content': \"\"}\n",
    "            current_h5 = current_h6 = None\n",
    "        elif tag.name == 'h5' and current_h4:\n",
    "            current_h5 = tag.get_text(strip=True)\n",
    "            data[current_h1][current_h2][current_h3][current_h4][current_h5] = {'content': \"\"}\n",
    "            current_h6 = None\n",
    "        elif tag.name == 'h6' and current_h5:\n",
    "            current_h6 = tag.get_text(strip=True)\n",
    "            data[current_h1][current_h2][current_h3][current_h4][current_h5][current_h6] = {'content': \"\"}\n",
    "        else:\n",
    "            content = tag.get_text(strip=True)\n",
    "            if content:\n",
    "                if current_h6:\n",
    "                    data[current_h1][current_h2][current_h3][current_h4][current_h5][current_h6]['content'] += content + \" \"\n",
    "                elif current_h5:\n",
    "                    data[current_h1][current_h2][current_h3][current_h4][current_h5]['content'] += content + \" \"\n",
    "                elif current_h4:\n",
    "                    data[current_h1][current_h2][current_h3][current_h4]['content'] += content + \" \"\n",
    "                elif current_h3:\n",
    "                    data[current_h1][current_h2][current_h3]['content'] += content + \" \"\n",
    "                elif current_h2:\n",
    "                    data[current_h1][current_h2]['content'] += content + \" \"\n",
    "                elif current_h1:\n",
    "                    data[current_h1]['content'] += content + \" \"\n",
    "\n",
    "    return data\n",
    "\n",
    "def save_to_json(data, filename):\n",
    "    \"\"\"Save data to a JSON file, ensuring no overwrite of existing content.\"\"\"\n",
    "    try:\n",
    "        if os.path.exists(filename):\n",
    "            # 读取现有的 JSON 文件并合并\n",
    "            with open(filename, 'r', encoding='utf-8') as f:\n",
    "                existing_data = json.load(f)\n",
    "            # 合并新的数据到现有数据\n",
    "            if isinstance(existing_data, list):\n",
    "                existing_data.extend(data)  # 如果是列表，将数据合并\n",
    "            else:\n",
    "                existing_data.update(data)  # 如果是字典，更新数据\n",
    "        else:\n",
    "            existing_data = data  # 如果文件不存在，则新建\n",
    "\n",
    "        # 保存合并后的数据到文件\n",
    "        with open(filename, 'w', encoding='utf-8') as f:\n",
    "            json.dump(existing_data, f, ensure_ascii=False, indent=4)\n",
    "        print(f\"Data saved to {filename}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error writing to JSON file: {e}\")\n",
    "\n",
    "def scrape_links(scraper_, links):\n",
    "    \"\"\"Scrape multiple links and return organized data.\"\"\"\n",
    "    all_data = []\n",
    "    for link in links:\n",
    "        try:\n",
    "            print(f\"Fetching: {link}\")\n",
    "            soup = scraper_.get_soup(link)\n",
    "            # 提取内容并按照标题层级组织\n",
    "            page_data = organize_content_by_heading(soup)\n",
    "            page_data['url'] = link  # 保存爬取的链接\n",
    "            all_data.append(page_data)\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching {link}: {e}\")\n",
    "\n",
    "    return all_data\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    scraper_ = Scraper()\n",
    "\n",
    "    links = [\n",
    "        \"https://littleitalydays.com/\",\n",
    "        \"https://littleitalydays.com/entertainment-schedule/\",\n",
    "        \"https://littleitalydays.com/getting-around/\",\n",
    "        \"https://littleitalydays.com/about-us/\",\n",
    "        \"https://littleitalydays.com/faq/\",\n",
    "        \"https://littleitalydays.com/bloomfield-businesses/\"\n",
    "    ]\n",
    "\n",
    "    # 爬取所有页面的内容\n",
    "    scraped_data = scrape_links(scraper_, links)\n",
    "\n",
    "    # 保存到 JSON 文件，追加到现有内容中\n",
    "    save_to_json(scraped_data, \"../raw_documents/Pittsburgh_Little_Italy_Day.json\")\n",
    "\n",
    "    scraper_.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Banana Split Fest\n",
    "### https://bananasplitfest.com/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching: https://bananasplitfest.com/\n",
      "Fetching: https://bananasplitfest.com/activities/\n",
      "Fetching: https://bananasplitfest.com/events/princess-pageant/\n",
      "Fetching: https://bananasplitfest.com/activities/crafts-games-activities/\n",
      "Fetching: https://bananasplitfest.com/activities/participating-vendors/\n",
      "Fetching: https://bananasplitfest.com/activities/food/\n",
      "Fetching: https://bananasplitfest.com/activities/over-21-area/\n",
      "Fetching: https://bananasplitfest.com/events/\n",
      "Fetching: https://bananasplitfest.com/events/5k-banana-run/\n",
      "Fetching: https://bananasplitfest.com/events/great-american-banana-baking-contest/\n",
      "Fetching: https://bananasplitfest.com/events/banana-challenge/\n",
      "Fetching: https://bananasplitfest.com/events/blood-drive/\n",
      "Fetching: https://bananasplitfest.com/events/cornhole-tournament/\n",
      "Fetching: https://bananasplitfest.com/events/car-show/\n",
      "Fetching: https://bananasplitfest.com/events/yellow-tie-gala/\n",
      "Fetching: https://bananasplitfest.com/activities/entertainment/\n",
      "Fetching: https://bananasplitfest.com/schedule/\n",
      "Fetching: https://bananasplitfest.com/information/parking/\n",
      "Fetching: https://bananasplitfest.com/information/plan-your-visit/\n",
      "Fetching: https://bananasplitfest.com/history/\n",
      "Fetching: https://bananasplitfest.com/information/media/\n",
      "Data saved to ../raw_documents/Pittsburgh_Banana_Split_Festival.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import os\n",
    "\n",
    "class Scraper:\n",
    "    def __init__(self):\n",
    "        self.chrome_options = Options()\n",
    "        self.chrome_options.add_argument(\"--headless\")\n",
    "        self.chrome_options.add_argument(\"--disable-gpu\")\n",
    "        self.chrome_options.add_argument(\"--no-sandbox\")\n",
    "        self.chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "        self.driver = webdriver.Chrome(options=self.chrome_options)\n",
    "        self.driver.set_page_load_timeout(10)\n",
    "\n",
    "    def get_html(self, url: str):\n",
    "        \"\"\"Get HTML.\"\"\"\n",
    "        self.driver.get(url)\n",
    "        return self.driver.page_source\n",
    "\n",
    "    def get_soup(self, url: str):\n",
    "        \"\"\"Parse and return BeautifulSoup object.\"\"\"\n",
    "        html = self.get_html(url)\n",
    "        return BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "    def close(self):\n",
    "        self.driver.quit()\n",
    "\n",
    "    def __del__(self):\n",
    "        self.close()\n",
    "\n",
    "def organize_content_by_heading(soup: BeautifulSoup):\n",
    "    \"\"\"Organize content based on headings, ensuring all relevant text is captured.\"\"\"\n",
    "    data = {}\n",
    "    current_h1, current_h2, current_h3, current_h4, current_h5, current_h6 = None, None, None, None, None, None\n",
    "\n",
    "    for tag in soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'p', 'div', 'span']):\n",
    "        if tag.name == 'h1':\n",
    "            current_h1 = tag.get_text(strip=True)\n",
    "            data[current_h1] = {'content': \"\"}\n",
    "            current_h2 = current_h3 = current_h4 = current_h5 = current_h6 = None  # Reset lower headings\n",
    "        elif tag.name == 'h2' and current_h1:\n",
    "            current_h2 = tag.get_text(strip=True)\n",
    "            data[current_h1][current_h2] = {'content': \"\"}\n",
    "            current_h3 = current_h4 = current_h5 = current_h6 = None\n",
    "        elif tag.name == 'h3' and current_h2:\n",
    "            current_h3 = tag.get_text(strip=True)\n",
    "            data[current_h1][current_h2][current_h3] = {'content': \"\"}\n",
    "            current_h4 = current_h5 = current_h6 = None\n",
    "        elif tag.name == 'h4' and current_h3:\n",
    "            current_h4 = tag.get_text(strip=True)\n",
    "            data[current_h1][current_h2][current_h3][current_h4] = {'content': \"\"}\n",
    "            current_h5 = current_h6 = None\n",
    "        elif tag.name == 'h5' and current_h4:\n",
    "            current_h5 = tag.get_text(strip=True)\n",
    "            data[current_h1][current_h2][current_h3][current_h4][current_h5] = {'content': \"\"}\n",
    "            current_h6 = None\n",
    "        elif tag.name == 'h6' and current_h5:\n",
    "            current_h6 = tag.get_text(strip=True)\n",
    "            data[current_h1][current_h2][current_h3][current_h4][current_h5][current_h6] = {'content': \"\"}\n",
    "        else:\n",
    "            content = tag.get_text(strip=True)\n",
    "            if content:\n",
    "                if current_h6:\n",
    "                    data[current_h1][current_h2][current_h3][current_h4][current_h5][current_h6]['content'] += content + \" \"\n",
    "                elif current_h5:\n",
    "                    data[current_h1][current_h2][current_h3][current_h4][current_h5]['content'] += content + \" \"\n",
    "                elif current_h4:\n",
    "                    data[current_h1][current_h2][current_h3][current_h4]['content'] += content + \" \"\n",
    "                elif current_h3:\n",
    "                    data[current_h1][current_h2][current_h3]['content'] += content + \" \"\n",
    "                elif current_h2:\n",
    "                    data[current_h1][current_h2]['content'] += content + \" \"\n",
    "                elif current_h1:\n",
    "                    data[current_h1]['content'] += content + \" \"\n",
    "\n",
    "    return data\n",
    "\n",
    "def save_to_json(data, filename):\n",
    "    \"\"\"Save data to a JSON file, ensuring no overwrite of existing content.\"\"\"\n",
    "    try:\n",
    "        if os.path.exists(filename):\n",
    "            # 读取现有的 JSON 文件并合并\n",
    "            with open(filename, 'r', encoding='utf-8') as f:\n",
    "                existing_data = json.load(f)\n",
    "            # 合并新的数据到现有数据\n",
    "            if isinstance(existing_data, list):\n",
    "                existing_data.extend(data)  # 如果是列表，将数据合并\n",
    "            else:\n",
    "                existing_data.update(data)  # 如果是字典，更新数据\n",
    "        else:\n",
    "            existing_data = data  # 如果文件不存在，则新建\n",
    "\n",
    "        # 保存合并后的数据到文件\n",
    "        with open(filename, 'w', encoding='utf-8') as f:\n",
    "            json.dump(existing_data, f, ensure_ascii=False, indent=4)\n",
    "        print(f\"Data saved to {filename}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error writing to JSON file: {e}\")\n",
    "\n",
    "def scrape_links(scraper_, links):\n",
    "    \"\"\"Scrape multiple links and return organized data.\"\"\"\n",
    "    all_data = []\n",
    "    for link in links:\n",
    "        try:\n",
    "            print(f\"Fetching: {link}\")\n",
    "            soup = scraper_.get_soup(link)\n",
    "            # 提取内容并按照标题层级组织\n",
    "            page_data = organize_content_by_heading(soup)\n",
    "            page_data['url'] = link  # 保存爬取的链接\n",
    "            all_data.append(page_data)\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching {link}: {e}\")\n",
    "\n",
    "    return all_data\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    scraper_ = Scraper()\n",
    "\n",
    "    links = [\n",
    "        \"https://bananasplitfest.com/\",\n",
    "        \"https://bananasplitfest.com/activities/\",\n",
    "        \"https://bananasplitfest.com/events/princess-pageant/\",\n",
    "        \"https://bananasplitfest.com/activities/crafts-games-activities/\",\n",
    "        \"https://bananasplitfest.com/activities/participating-vendors/\",\n",
    "        \"https://bananasplitfest.com/activities/food/\",\n",
    "        \"https://bananasplitfest.com/activities/over-21-area/\",\n",
    "        \"https://bananasplitfest.com/events/\",\n",
    "        \"https://bananasplitfest.com/events/5k-banana-run/\",\n",
    "        \"https://bananasplitfest.com/events/great-american-banana-baking-contest/\",\n",
    "        \"https://bananasplitfest.com/events/banana-challenge/\",\n",
    "        \"https://bananasplitfest.com/events/blood-drive/\",\n",
    "        \"https://bananasplitfest.com/events/cornhole-tournament/\",\n",
    "        \"https://bananasplitfest.com/events/car-show/\",\n",
    "        \"https://bananasplitfest.com/events/yellow-tie-gala/\",\n",
    "        \"https://bananasplitfest.com/activities/entertainment/\",\n",
    "        \"https://bananasplitfest.com/schedule/\",\n",
    "        \"https://bananasplitfest.com/information/parking/\",\n",
    "        \"https://bananasplitfest.com/information/plan-your-visit/\",\n",
    "        \"https://bananasplitfest.com/history/\",\n",
    "        \"https://bananasplitfest.com/information/media/\"\n",
    "    ]\n",
    "\n",
    "    # 爬取所有页面的内容\n",
    "    scraped_data = scrape_links(scraper_, links)\n",
    "\n",
    "    # 保存到 JSON 文件，追加到现有内容中\n",
    "    save_to_json(scraped_data, \"../raw_documents/Pittsburgh_Banana_Split_Festival.json\")\n",
    "\n",
    "    scraper_.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag_pittsburgh_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
