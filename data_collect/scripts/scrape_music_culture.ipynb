{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Music\n",
    "## Symphony\n",
    "### https://www.pittsburghsymphony.org/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping page 1...\n",
      "Scraping page 2...\n",
      "Scraping page 3...\n",
      "Scraping page 4...\n",
      "Scraping page 5...\n",
      "Pittsburgh Symphony Scraping completed.\n"
     ]
    }
   ],
   "source": [
    "###################################### Events ######################################\n",
    "import json\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "class PittsburghSymphonyScraper:\n",
    "    def __init__(self):\n",
    "        chrome_options = Options()\n",
    "        chrome_options.add_argument(\"--headless\")\n",
    "        chrome_options.add_argument(\"--disable-gpu\")\n",
    "        chrome_options.add_argument(\"--no-sandbox\")\n",
    "        chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "        self.driver = webdriver.Chrome(options=chrome_options)\n",
    "        self.base_url = \"https://www.pittsburghsymphony.org/calendar?page=\"\n",
    "        self.result_file = \"../raw_documents/Pittsburgh_Symphony.json\"\n",
    "\n",
    "    def fetch_page(self, page_num):\n",
    "        url = self.base_url + str(page_num)\n",
    "        try:\n",
    "            self.driver.get(url)\n",
    "            time.sleep(3)  # wait the page to load\n",
    "            page_source = self.driver.page_source\n",
    "            soup = BeautifulSoup(page_source, \"html.parser\")\n",
    "            return soup\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching page {page_num}: {e}\")\n",
    "            return None\n",
    "\n",
    "    def extract_event_info(self, soup):\n",
    "        events = []\n",
    "        event_list = soup.find_all('article', class_='event')\n",
    "        for event in event_list:\n",
    "            try:\n",
    "                title = event.find('h3', class_='title').get_text(strip=True)\n",
    "                time = event.find('time', class_='range').get_text(strip=True)\n",
    "                venue = event.find('div', class_='venue').get_text(strip=True)\n",
    "                organization = event.find('div', class_='organization').get_text(strip=True)\n",
    "\n",
    "                event_info = {\n",
    "                    \"event_name\": title,\n",
    "                    \"event_time\": time,\n",
    "                    \"venue\": venue,\n",
    "                    \"organization\": organization\n",
    "                }\n",
    "                events.append(event_info)\n",
    "            except Exception as e:\n",
    "                print(f\"Error extracting event info: {e}\")\n",
    "        return events\n",
    "\n",
    "    def append_to_json(self, events):    \n",
    "        # write in time\n",
    "        try:\n",
    "            with open(self.result_file, 'a') as f:\n",
    "                for event in events:\n",
    "                    json.dump(event, f, indent=4)\n",
    "                    f.write(\"\\n\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error writing to JSON file: {e}\")\n",
    "\n",
    "    def scrape(self):\n",
    "        for page_num in range(1, 6):\n",
    "            print(f\"Scraping page {page_num}...\")\n",
    "            soup = self.fetch_page(page_num)\n",
    "            if soup:\n",
    "                events = self.extract_event_info(soup)\n",
    "                self.append_to_json(events)\n",
    "\n",
    "    def close(self):\n",
    "        self.driver.quit()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    scraper = PittsburghSymphonyScraper()\n",
    "    scraper.scrape()\n",
    "    scraper.close()\n",
    "    print(\"Pittsburgh Symphony Scraping completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Musicians' data has been appended to the JSON file.\n"
     ]
    }
   ],
   "source": [
    "###################################### Musicians ######################################\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import os\n",
    "\n",
    "url = \"https://www.pittsburghsymphony.org/pso_home/web/musicians\"\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "musicians_data = {}\n",
    "json_file_path = \"../raw_documents/Pittsburgh_Symphony.json\"\n",
    "\n",
    "if os.path.exists(json_file_path):\n",
    "    with open(json_file_path, \"r\") as json_file:\n",
    "        try:\n",
    "            musicians_data = json.load(json_file)\n",
    "        except json.JSONDecodeError:\n",
    "            musicians_data = {}\n",
    "\n",
    "def get_musician_introduction(subpage_url):\n",
    "    try:\n",
    "        subpage_response = requests.get(subpage_url)\n",
    "        subpage_soup = BeautifulSoup(subpage_response.content, \"html.parser\")\n",
    "        bio_text_div = subpage_soup.find(\"div\", class_=\"bio-text\")\n",
    "        if bio_text_div:\n",
    "            return bio_text_div.get_text(strip=True, separator=\" \")\n",
    "    except Exception as e:\n",
    "        print(f\"Error accessing {subpage_url}: {e}\")\n",
    "    return None\n",
    "\n",
    "for section in soup.find_all(\"h3\"):\n",
    "    section_name = section.get_text().strip()\n",
    "    if section_name in musicians_data:\n",
    "        continue\n",
    "    musicians_data[section_name] = []\n",
    "    musician_list = section.find_next(\"p\")\n",
    "    \n",
    "    if musician_list:\n",
    "        for musician in musician_list.find_all(\"a\"):\n",
    "            musician_name = musician.get_text(strip=True)\n",
    "            musician_title = musician_list.get_text(strip=True).split('|')[1].strip() if '|' in musician_list.get_text() else \"\"\n",
    "            musician_data = {\n",
    "                \"name\": musician_name,\n",
    "                \"title\": musician_title\n",
    "            }\n",
    "            \n",
    "            musician_subpage_url = musician.get(\"href\")\n",
    "            if musician_subpage_url:\n",
    "                full_subpage_url = f\"https://www.pittsburghsymphony.org{musician_subpage_url}\"\n",
    "                introduction = get_musician_introduction(full_subpage_url)\n",
    "                if introduction:\n",
    "                    musician_data[\"introduction\"] = introduction\n",
    "            \n",
    "            musicians_data[section_name].append(musician_data)\n",
    "\n",
    "with open(json_file_path, \"w\") as json_file:\n",
    "    json.dump(musicians_data, json_file, indent=4)\n",
    "    json_file.write(\"\\n\") \n",
    "\n",
    "print(\"Musicians' data has been appended to the JSON file.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching: https://www.pittsburghsymphony.org/pso_home/web/visit-landing/frequently-asked-questions\n",
      "Fetching: https://www.pittsburghsymphony.org/pso_home/web/tickets-landing/seating-charts\n",
      "Fetching: https://www.pittsburghsymphony.org/pso_home/web/give-landing/corporate-partnerships/dining-partners\n",
      "Fetching: https://www.pittsburghsymphony.org/pso_home/web/visit-landing/directions-parking-lodging\n",
      "Fetching: https://www.pittsburghsymphony.org/pso_home/web/visit-landing/accessibility-and-information\n",
      "Data saved to ../raw_documents/Music_Culture/1_Symphony_Others.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import os\n",
    "\n",
    "class Scraper:\n",
    "    def __init__(self):\n",
    "        self.chrome_options = Options()\n",
    "        self.chrome_options.add_argument(\"--headless\")\n",
    "        self.chrome_options.add_argument(\"--disable-gpu\")\n",
    "        self.chrome_options.add_argument(\"--no-sandbox\")\n",
    "        self.chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "        self.driver = webdriver.Chrome(options=self.chrome_options)\n",
    "        self.driver.set_page_load_timeout(10)\n",
    "\n",
    "    def get_html(self, url: str):\n",
    "        \"\"\"Get HTML.\"\"\"\n",
    "        self.driver.get(url)\n",
    "        return self.driver.page_source\n",
    "\n",
    "    def get_soup(self, url: str):\n",
    "        \"\"\"Parse and return BeautifulSoup object.\"\"\"\n",
    "        html = self.get_html(url)\n",
    "        return BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "    def close(self):\n",
    "        self.driver.quit()\n",
    "\n",
    "    def __del__(self):\n",
    "        self.close()\n",
    "\n",
    "def organize_content_by_heading(soup: BeautifulSoup):\n",
    "    \"\"\"Organize content based on headings, ensuring all relevant text is captured.\"\"\"\n",
    "    data = {}\n",
    "    current_h1, current_h2, current_h3, current_h4, current_h5, current_h6 = None, None, None, None, None, None\n",
    "\n",
    "    for tag in soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'p', 'div', 'span']):\n",
    "        if tag.name == 'h1':\n",
    "            current_h1 = tag.get_text(strip=True)\n",
    "            data[current_h1] = {'content': \"\"}\n",
    "            current_h2 = current_h3 = current_h4 = current_h5 = current_h6 = None  # Reset lower headings\n",
    "        elif tag.name == 'h2' and current_h1:\n",
    "            current_h2 = tag.get_text(strip=True)\n",
    "            data[current_h1][current_h2] = {'content': \"\"}\n",
    "            current_h3 = current_h4 = current_h5 = current_h6 = None\n",
    "        elif tag.name == 'h3' and current_h2:\n",
    "            current_h3 = tag.get_text(strip=True)\n",
    "            data[current_h1][current_h2][current_h3] = {'content': \"\"}\n",
    "            current_h4 = current_h5 = current_h6 = None\n",
    "        elif tag.name == 'h4' and current_h3:\n",
    "            current_h4 = tag.get_text(strip=True)\n",
    "            data[current_h1][current_h2][current_h3][current_h4] = {'content': \"\"}\n",
    "            current_h5 = current_h6 = None\n",
    "        elif tag.name == 'h5' and current_h4:\n",
    "            current_h5 = tag.get_text(strip=True)\n",
    "            data[current_h1][current_h2][current_h3][current_h4][current_h5] = {'content': \"\"}\n",
    "            current_h6 = None\n",
    "        elif tag.name == 'h6' and current_h5:\n",
    "            current_h6 = tag.get_text(strip=True)\n",
    "            data[current_h1][current_h2][current_h3][current_h4][current_h5][current_h6] = {'content': \"\"}\n",
    "        else:\n",
    "            content = tag.get_text(strip=True)\n",
    "            if content:\n",
    "                if current_h6:\n",
    "                    data[current_h1][current_h2][current_h3][current_h4][current_h5][current_h6]['content'] += content + \" \"\n",
    "                elif current_h5:\n",
    "                    data[current_h1][current_h2][current_h3][current_h4][current_h5]['content'] += content + \" \"\n",
    "                elif current_h4:\n",
    "                    data[current_h1][current_h2][current_h3][current_h4]['content'] += content + \" \"\n",
    "                elif current_h3:\n",
    "                    data[current_h1][current_h2][current_h3]['content'] += content + \" \"\n",
    "                elif current_h2:\n",
    "                    data[current_h1][current_h2]['content'] += content + \" \"\n",
    "                elif current_h1:\n",
    "                    data[current_h1]['content'] += content + \" \"\n",
    "\n",
    "    return data\n",
    "\n",
    "def save_to_json(data, filename):\n",
    "    \"\"\"Save data to a JSON file, ensuring no overwrite of existing content.\"\"\"\n",
    "    try:\n",
    "        if os.path.exists(filename):\n",
    "            with open(filename, 'r', encoding='utf-8') as f:\n",
    "                existing_data = json.load(f)\n",
    "            if isinstance(existing_data, list):\n",
    "                existing_data.extend(data)\n",
    "            else:\n",
    "                existing_data.update(data) \n",
    "        else:\n",
    "            existing_data = data \n",
    "\n",
    "        with open(filename, 'w', encoding='utf-8') as f:\n",
    "            json.dump(existing_data, f, ensure_ascii=False, indent=4)\n",
    "        print(f\"Data saved to {filename}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error writing to JSON file: {e}\")\n",
    "\n",
    "def scrape_links(scraper_, links):\n",
    "    \"\"\"Scrape multiple links and return organized data.\"\"\"\n",
    "    all_data = []\n",
    "    for link in links:\n",
    "        try:\n",
    "            print(f\"Fetching: {link}\")\n",
    "            soup = scraper_.get_soup(link)\n",
    "            page_data = organize_content_by_heading(soup)\n",
    "            page_data['url'] = link  \n",
    "            all_data.append(page_data)\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching {link}: {e}\")\n",
    "\n",
    "    return all_data\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    scraper_ = Scraper()\n",
    "    links = [\n",
    "        \"https://www.pittsburghsymphony.org/pso_home/web/visit-landing/frequently-asked-questions\",\n",
    "        \"https://www.pittsburghsymphony.org/pso_home/web/tickets-landing/seating-charts\",\n",
    "        \"https://www.pittsburghsymphony.org/pso_home/web/give-landing/corporate-partnerships/dining-partners\",\n",
    "        \"https://www.pittsburghsymphony.org/pso_home/web/visit-landing/directions-parking-lodging\",\n",
    "        \"https://www.pittsburghsymphony.org/pso_home/web/visit-landing/accessibility-and-information\"\n",
    "    ]\n",
    "    scraped_data = scrape_links(scraper_, links)\n",
    "    save_to_json(scraped_data, \"../raw_documents/Music_Culture/1_Symphony_Others.json\")\n",
    "\n",
    "    scraper_.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The file has been processed and saved as '1_Symphony_All.txt'\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def flatten_event(event):\n",
    "    \"\"\"Flatten an event entry into a single line of text.\"\"\"\n",
    "    parts = []\n",
    "    if \"event_name\" in event:\n",
    "        parts.append(f\"Event Name: {event['event_name']}\")\n",
    "    if \"event_time\" in event:\n",
    "        parts.append(f\"Event Time: {event['event_time']}\")\n",
    "    if \"venue\" in event:\n",
    "        parts.append(f\"Venue: {event['venue']}\")\n",
    "    if \"organization\" in event:\n",
    "        parts.append(f\"Organization: {event['organization']}\")\n",
    "    return \", \".join(parts)\n",
    "\n",
    "def flatten_section(section_name, section_data):\n",
    "    \"\"\"Flatten a section with subsections into a single line of text.\"\"\"\n",
    "    return f\"{section_name}: \" + \" \".join(section_data)\n",
    "\n",
    "def process_json(data):\n",
    "    \"\"\"Process the JSON data to flatten events and sections.\"\"\"\n",
    "    flattened = []\n",
    "    \n",
    "    for entry in data:\n",
    "        if \"event_name\" in entry:\n",
    "            flattened.append(flatten_event(entry))\n",
    "        else:\n",
    "            for key, value in entry.items():\n",
    "                if isinstance(value, dict):\n",
    "                    for subkey, subvalue in value.items():\n",
    "                        if isinstance(subvalue, str) and subvalue.strip():\n",
    "                            flattened.append(f\"{subkey}: {subvalue}\")\n",
    "                        elif isinstance(subvalue, list):\n",
    "                            flattened.append(flatten_section(subkey, subvalue))\n",
    "                elif isinstance(value, str) and value.strip():\n",
    "                    flattened.append(f\"{key}: {value}\")\n",
    "    \n",
    "    return flattened\n",
    "\n",
    "with open('../raw_documents/Music_Culture/1_Symphony_All.json', 'r', encoding='utf-8') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "flattened_data = process_json(data)\n",
    "\n",
    "with open('../raw_documents/Music_Culture/1_Music_Symphony.txt', 'w', encoding='utf-8') as txt_file:\n",
    "    for line in flattened_data:\n",
    "        txt_file.write(line + '\\n')\n",
    "\n",
    "print(\"The file has been processed and saved as '1_Symphony_All.txt'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been successfully appended.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def format_musician_data(instrument, musician):\n",
    "    \"\"\"Format each musician's data into a sentence.\"\"\"\n",
    "    name = musician['name']\n",
    "    title = musician['title'] if musician['title'] else f\"{instrument} musician\"\n",
    "    return f\"For {instrument}, {name} is {title}.\"\n",
    "\n",
    "def process_json(data, output_file_path):\n",
    "    \"\"\"Process the JSON data and append formatted text to the file.\"\"\"\n",
    "    with open(output_file_path, 'a', encoding='utf-8') as file:\n",
    "        if isinstance(data, list):\n",
    "            for section in data:\n",
    "                for instrument, musicians in section.items():\n",
    "                    for musician in musicians:\n",
    "                        sentence = format_musician_data(instrument, musician)\n",
    "                        file.write(sentence + '\\n')\n",
    "        elif isinstance(data, dict):\n",
    "            for instrument, musicians in data.items():\n",
    "                for musician in musicians:\n",
    "                    sentence = format_musician_data(instrument, musician)\n",
    "                    file.write(sentence + '\\n')\n",
    "\n",
    "with open('../raw_documents/Music_Culture/1_Symphony_All.json', 'r', encoding='utf-8') as file:\n",
    "    data = json.load(file)\n",
    "process_json(data, '../raw_documents/Music_Culture/1_Music_Symphony.txt')\n",
    "print(\"Data has been successfully appended.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text has been successfully written to output.txt.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open('../raw_documents/Music_Culture/1_Symphony_All.json', 'r', encoding='utf-8') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "def flatten_json_content(data):\n",
    "    flattened_text = \"\"\n",
    "    for item in data:\n",
    "        if isinstance(item, str):\n",
    "            flattened_text += item + \": \" \n",
    "        elif isinstance(item, list):\n",
    "            for sentence in item:\n",
    "                flattened_text += sentence + \" \"  \n",
    "        flattened_text += \"\\n\\n\"\n",
    "    return flattened_text.strip()\n",
    "\n",
    "output_text = flatten_json_content(data)\n",
    "with open('output.txt', 'w', encoding='utf-8') as file:\n",
    "    file.write(output_text)\n",
    "print(\"Text has been successfully written to output.txt.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Opera\n",
    "### https://pittsburghopera.org/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching: https://pittsburghopera.org/\n",
      "Fetching: https://pittsburghopera.org/about/mission-history\n",
      "Fetching: https://pittsburghopera.org/about/inclusion-diversity-equity-accessibility-idea/\n",
      "Fetching: https://pittsburghopera.org/calendar?timequery=week&prev=71+&start=1722484800000&end=1725145140000\n",
      "Fetching: https://pittsburghopera.org/calendar?timequery=week&prev=55+&start=1722484800000&end=1725145140000\n",
      "Fetching: https://pittsburghopera.org/calendar?timequery=week&prev=47+&start=1722484800000&end=1725145140000\n",
      "Fetching: https://pittsburghopera.org/calendar?timequery=week&prev=39+&start=1722484800000&end=1725145140000\n",
      "Fetching: https://pittsburghopera.org/calendar?timequery=week&prev=31+&start=1722484800000&end=1725145140000\n",
      "Fetching: https://pittsburghopera.org/calendar?timequery=week&prev=23+&start=1722484800000&end=1725145140000\n",
      "Fetching: https://pittsburghopera.org/calendar?timequery=week&prev=15+&start=1722484800000&end=1725145140000\n",
      "Fetching: https://pittsburghopera.org/calendar?timequery=week&prev=7+&start=1722484800000&end=1725145140000\n",
      "Fetching: https://pittsburghopera.org/calendar?timequery=week&prev=-1+&start=1722484800000&end=1725145140000\n",
      "Fetching: https://pittsburghopera.org/calendar?timequery=week&prev=-9+&start=1722484800000&end=1725145140000\n",
      "Fetching: https://pittsburghopera.org/calendar?timequery=week&prev=-17+&start=1722484800000&end=1725145140000\n",
      "Fetching: https://pittsburghopera.org/calendar?timequery=week&prev=-25+&start=1722484800000&end=1725145140000\n",
      "Fetching: https://pittsburghopera.org/calendar?timequery=week&prev=-33+&start=1722484800000&end=1725145140000\n",
      "Fetching: https://pittsburghopera.org/calendar?timequery=week&prev=-41+&start=1722484800000&end=17251451400000\n",
      "Fetching: https://pittsburghopera.org/calendar?timequery=week&prev=-57+&start=1722484800000&end=1725145140000\n",
      "Fetching: https://pittsburghopera.org/calendar?timequery=week&prev=-97+&start=1722484800000&end=1725145140000\n",
      "Fetching: https://pittsburghopera.org/calendar?timequery=week&prev=-105+&start=1722484800000&end=1725145140000\n",
      "Fetching: https://pittsburghopera.org/calendar?timequery=week&prev=-113+&start=1722484800000&end=1725145140000\n",
      "Fetching: https://pittsburghopera.org/calendar?timequery=week&prev=-121+&start=1722484800000&end=1725145140000\n",
      "Fetching: https://pittsburghopera.org/calendar?timequery=week&prev=-129+&start=1722484800000&end=1725145140000\n",
      "Fetching: https://pittsburghopera.org/calendar?timequery=week&prev=-145+&start=1722484800000&end=1725145140000\n",
      "Fetching: https://pittsburghopera.org/calendar?timequery=week&prev=-153+&start=1722484800000&end=1725145140000\n",
      "Fetching: https://pittsburghopera.org/calendar?timequery=week&prev=-161+&start=1722484800000&end=1725145140000\n",
      "Fetching: https://pittsburghopera.org/calendar?timequery=week&prev=-169+&start=1722484800000&end=1725145140000\n",
      "Fetching: https://pittsburghopera.org/calendar?timequery=month&prev=-169+&start=1743480000000&end=1746053940000\n",
      "Fetching: https://pittsburghopera.org/calendar?timequery=month&prev=-169+&start=1746072000000&end=1748732340000\n",
      "Fetching: https://pittsburghopera.org/season/tosca\n",
      "Fetching: https://pittsburghopera.org/season/cavalleria-rusticana-pagliacci\n",
      "Fetching: https://pittsburghopera.org/season/armida\n",
      "Fetching: https://pittsburghopera.org/season/madama-butterflyhttps://pittsburghopera.org/season/woman-with-eyes-closed\n",
      "Fetching: https://pittsburghopera.org/season/past-seasons\n",
      "Fetching: https://pittsburghopera.org/season/the-barber-of-seville?hsLang=en\n",
      "Fetching: https://pittsburghopera.org/season/the-flying-dutchman?hsLang=en\n",
      "Fetching: https://pittsburghopera.org/season/iphigenie-en-tauride?hsLang=en\n",
      "Fetching: https://pittsburghopera.org/season/proving-up?hsLang=en\n",
      "Fetching: https://pittsburghopera.org/season/la-traviata?hsLang=en\n",
      "Fetching: https://pittsburghopera.org/season/the-passion-of-mary-cardwell-dawson?hsLang=en\n",
      "Fetching: https://pittsburghopera.org/season/past-seasons/rusalka?hsLang=en\n",
      "Fetching: https://pittsburghopera.org/season/past-seasons/the-marriage-of-figaro?hsLang=en\n",
      "Fetching: https://pittsburghopera.org/season/past-seasons/ariodante?hsLang=en\n",
      "Fetching: https://pittsburghopera.org/season/past-seasons/il-trovatore?hsLang=en\n",
      "Fetching: https://pittsburghopera.org/season/past-seasons/denis-katya?hsLang=en\n",
      "Fetching: https://pittsburghopera.org/season/past-seasons/we-shall-not-be-moved?hsLang=en\n",
      "Fetching: https://pittsburghopera.org/season/past-seasons/the-magic-flute?hsLang=en\n",
      "Fetching: https://pittsburghopera.org/season/past-seasons/the-rose-elf?hsLang=en\n",
      "Fetching: https://pittsburghopera.org/season/past-seasons/in-a-grove?hsLang=en\n",
      "Fetching: https://pittsburghopera.org/season/past-seasons/carmen?hsLang=en\n",
      "Fetching: https://pittsburghopera.org/season/past-seasons/blue?hsLang=enhttps://pittsburghopera.org/season/past-seasons/semele?hsLang=en\n",
      "Fetching: https://pittsburghopera.org/season/past-seasons/cosi-fan-tutte?hsLang=en\n",
      "Fetching: https://pittsburghopera.org/season/past-seasons/soldier-songs?hsLang=en\n",
      "Fetching: https://pittsburghopera.org/season/past-seasons/charlie-parkers-yardbird?hsLang=en\n",
      "Fetching: https://pittsburghopera.org/season/past-seasons/last-american-hammer?hsLang=en\n",
      "Fetching: https://pittsburghopera.org/season/past-seasons/alcina?hsLang=en\n",
      "Fetching: https://pittsburghopera.org/season/past-seasons/florencia-en-el-amazonas?hsLang=en\n",
      "Fetching: https://pittsburghopera.org/season/past-seasons/don-giovanni?hsLang=en\n",
      "Fetching: https://pittsburghopera.org/season/past-seasons/madama-butterfly?hsLang=en\n",
      "Fetching: https://pittsburghopera.org/season/past-seasons/afterwards?hsLang=en\n",
      "Fetching: https://pittsburghopera.org/season/past-seasons/glory-denied?hsLang=en\n",
      "Fetching: https://pittsburghopera.org/season/past-seasons/la-boheme?hsLang=en\n",
      "Fetching: https://pittsburghopera.org/season/past-seasons/don-pasquale?hsLang=en\n",
      "Fetching: https://pittsburghopera.org/season/past-seasons/hansel-gretel?hsLang=en\n",
      "Fetching: https://pittsburghopera.org/season/past-seasons/savage-winter?hsLang=en\n",
      "Fetching: https://pittsburghopera.org/season/past-seasons/the-long-walk?hsLang=en\n",
      "Fetching: https://pittsburghopera.org/season/past-seasons/the-elixir-of-love?hsLang=en\n",
      "Fetching: https://pittsburghopera.org/season/past-seasons/moby-dick?hsLang=en\n",
      "Fetching: https://pittsburghopera.org/season/past-seasons/the-marriage-of-figaro?hsLang=en\n",
      "Fetching: https://pittsburghopera.org/season/past-seasons/tosca-2017?hsLang=en\n",
      "Fetching: https://pittsburghopera.org/season/past-seasons/the-summer-king?hsLang=en\n",
      "Fetching: https://pittsburghopera.org/season/past-seasons/turandot?hsLang=en\n",
      "Fetching: https://pittsburghopera.org/season/past-seasons/as-one?hsLang=en\n",
      "Fetching: https://pittsburghopera.org/season/past-seasons/richard-the-lionheart?hsLang=en\n",
      "Fetching: https://pittsburghopera.org/season/past-seasons/salome?hsLang=en\n",
      "Fetching: https://pittsburghopera.org/season/past-seasons/la-traviata-2016?hsLang=en\n",
      "Fetching: https://pittsburghopera.org/season/past-seasons/the-rakes-progress?hsLang=en\n",
      "Fetching: https://pittsburghopera.org/season/past-seasons/twenty-seven?hsLang=en\n",
      "Fetching: https://pittsburghopera.org/season/past-seasons/the-barber-of-seville-2016?hsLang=en\n",
      "Fetching: https://pittsburghopera.org/season/past-seasons/little-women?hsLang=en\n",
      "Fetching: https://pittsburghopera.org/season/past-seasons/cos%C3%AC-fan-tutte-2015?hsLang=en\n",
      "Fetching: https://pittsburghopera.org/season/past-seasons/nabucco?hsLang=en\n",
      "Fetching: https://pittsburghopera.org/season/free-low-cost-events\n",
      "Fetching: https://pittsburghopera.org/season/free-low-cost-events/songshop\n",
      "Fetching: https://pittsburghopera.org/season/free-low-cost-events/opera-up-close\n",
      "Fetching: https://pittsburghopera.org/season/free-low-cost-events/pre-opera-talks\n",
      "Fetching: https://pittsburghopera.org/season/free-low-cost-events/meet-the-artists\n",
      "Fetching: https://pittsburghopera.org/season/free-low-cost-events/community-concerts\n",
      "Fetching: https://pittsburghopera.org/season/free-low-cost-events/allegheny-county-summer-concert-series\n",
      "Fetching: https://pittsburghopera.org/season/free-low-cost-events/wqed-broadcasts\n",
      "Fetching: https://pittsburghopera.org/season/special-events\n",
      "Fetching: https://pittsburghopera.org/season/special-events/diamond-horseshoe-ball\n",
      "Fetching: https://pittsburghopera.org/season/special-events/pittsburgh-opera-fashion-event\n",
      "Fetching: https://pittsburghopera.org/season/special-events/maecenas\n",
      "Fetching: https://pittsburghopera.org/resident-artists/2024-25resident-artists\n",
      "Fetching: https://pittsburghopera.org/resident-artists/faculty-administration/\n",
      "Fetching: https://pittsburghopera.org/resident-artists/auditions/\n",
      "Fetching: https://pittsburghopera.org/resident-artists/history-alumni/\n",
      "Fetching: https://pittsburghopera.org/our-team/orchestra\n",
      "Fetching: https://pittsburghopera.org/our-team/chorus\n",
      "Fetching: https://pittsburghopera.org/facilities/pittsburgh-opera-headquarters/\n",
      "Fetching: https://pittsburghopera.org/facilities/office-hours\n",
      "Fetching: https://pittsburghopera.org/facilities/production-rentals\n",
      "Fetching: https://pittsburghopera.org/facilities/hold-your-event-at-pittsburgh-opera/\n",
      "Data saved to ../raw_documents/Pittsburgh_Opera.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "class Scraper:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.chrome_options = Options()\n",
    "        self.chrome_options.add_argument(\"--headless\")\n",
    "        self.chrome_options.add_argument(\"--disable-gpu\")\n",
    "        self.chrome_options.add_argument(\"--no-sandbox\")\n",
    "        self.chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "        self.driver = webdriver.Chrome(options=self.chrome_options)\n",
    "        self.driver.set_page_load_timeout(10)\n",
    "        self.current_url = None\n",
    "        self.current_domain = None\n",
    "\n",
    "    def set_domain(self, url: str):\n",
    "        \"\"\"set domain name and URL\"\"\"\n",
    "        self.current_domain = url.split(\"/\")[2]  \n",
    "        self.current_url = url\n",
    "\n",
    "    def fetch(self, url: str, raw_html: bool = False):\n",
    "        \"\"\"capture the webpage and choose whether to remove JS or CSS\"\"\"\n",
    "        soup = self.get_soup(url)\n",
    "        if not raw_html:\n",
    "            soup = self.remove_js_css(soup)\n",
    "        return soup, self.get_links(soup), self.get_title(soup)\n",
    "\n",
    "    def get_html(self, url: str):\n",
    "        \"\"\"get HTML\"\"\"\n",
    "        self.set_domain(url) \n",
    "        self.driver.get(url)\n",
    "        return self.driver.page_source\n",
    "\n",
    "    def get_soup(self, url: str):\n",
    "        \"\"\"parse and get BeautifulSoup object\"\"\"\n",
    "        html = self.get_html(url)\n",
    "        return BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "    def get_links(self, soup: BeautifulSoup):\n",
    "        \"\"\"get external links\"\"\"\n",
    "        links = [link.get(\"href\") for link in soup.find_all(\"a\")]\n",
    "        return self.filter_links(links)\n",
    "\n",
    "    def get_title(self, soup: BeautifulSoup):\n",
    "        \"\"\"title\"\"\"\n",
    "        if soup.title is None:\n",
    "            return f\"untitled_{self.get_timestamp()}\"\n",
    "        title = soup.title.string.replace(\" \", \"_\").replace(\"/\", \"__\")\n",
    "        return title.replace(\"\\n\", \"\")\n",
    "\n",
    "    def remove_js_css(self, soup: BeautifulSoup):\n",
    "        \"\"\"remove JS or CSS\"\"\"\n",
    "        for script in soup([\"script\", \"style\"]):\n",
    "            script.extract()  \n",
    "        return soup\n",
    "\n",
    "    def remove_unnecessary_sections(self, soup: BeautifulSoup):\n",
    "        \"\"\"remove header and footer\"\"\"\n",
    "        for tag in [\"header\", \"footer\", \"nav\", \"aside\"]:\n",
    "            for element in soup.find_all(tag):\n",
    "                element.decompose()\n",
    "        return soup\n",
    "\n",
    "    def filter_links(self, links: list):\n",
    "        filtered_links = []\n",
    "        for link in links:\n",
    "            if link is None:\n",
    "                continue\n",
    "            elif link.startswith(\"#\"):\n",
    "                continue \n",
    "            elif link.startswith(\"http\"):\n",
    "                filtered_links.append(link)\n",
    "            elif link.startswith(\"//\"):\n",
    "                filtered_links.append(f\"https:{link}\")\n",
    "            elif link.startswith(\"/\"):\n",
    "                filtered_links.append(f\"https://{self.current_domain}{link}\") \n",
    "            else:\n",
    "                filtered_links.append(f\"{self.current_url}/{link}\")\n",
    "        return filtered_links\n",
    "\n",
    "    def get_timestamp(self):\n",
    "        return datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "    def close(self):\n",
    "        self.driver.quit()\n",
    "\n",
    "    def __del__(self):\n",
    "        self.close()\n",
    "\n",
    "\n",
    "def organize_content_by_heading(soup: BeautifulSoup):\n",
    "    data = {}\n",
    "    current_h2, current_h3, current_h4, current_h5, current_h6 = None, None, None, None, None\n",
    "\n",
    "    for tag in soup.find_all(['h2', 'h3', 'h4', 'h5', 'h6', 'p']):\n",
    "        if tag.name == 'h2':\n",
    "            current_h2 = tag.get_text(strip=True)\n",
    "            data[current_h2] = {}\n",
    "            current_h3 = current_h4 = current_h5 = current_h6 = None\n",
    "        elif tag.name == 'h3' and current_h2:\n",
    "            current_h3 = tag.get_text(strip=True)\n",
    "            data[current_h2][current_h3] = {}\n",
    "            current_h4 = current_h5 = current_h6 = None\n",
    "        elif tag.name == 'h4' and current_h3:\n",
    "            current_h4 = tag.get_text(strip=True)\n",
    "            data[current_h2][current_h3][current_h4] = {}\n",
    "            current_h5 = current_h6 = None\n",
    "        elif tag.name == 'h5' and current_h4:\n",
    "            current_h5 = tag.get_text(strip=True)\n",
    "            data[current_h2][current_h3][current_h4][current_h5] = {}\n",
    "            current_h6 = None\n",
    "        elif tag.name == 'h6' and current_h5:\n",
    "            current_h6 = tag.get_text(strip=True)\n",
    "            data[current_h2][current_h3][current_h4][current_h5][current_h6] = {}\n",
    "        elif tag.name == 'p':\n",
    "            if current_h6:\n",
    "                data[current_h2][current_h3][current_h4][current_h5][current_h6].setdefault('content', []).append(tag.get_text(strip=True))\n",
    "            elif current_h5:\n",
    "                data[current_h2][current_h3][current_h4][current_h5].setdefault('content', []).append(tag.get_text(strip=True))\n",
    "            elif current_h4:\n",
    "                data[current_h2][current_h3][current_h4].setdefault('content', []).append(tag.get_text(strip=True))\n",
    "            elif current_h3:\n",
    "                data[current_h2][current_h3].setdefault('content', []).append(tag.get_text(strip=True))\n",
    "            elif current_h2:\n",
    "                data[current_h2].setdefault('content', []).append(tag.get_text(strip=True))\n",
    "    \n",
    "    return data\n",
    "\n",
    "\n",
    "def save_to_json(data, filename):\n",
    "    try:\n",
    "        with open(filename, 'w', encoding='utf-8') as f:\n",
    "            json.dump(data, f, ensure_ascii=False, indent=4)\n",
    "        print(f\"Data saved to {filename}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error writing to JSON file: {e}\")\n",
    "\n",
    "\n",
    "def scrape_links(scraper_, links):\n",
    "    all_data = []\n",
    "    for link in links:\n",
    "        try:\n",
    "            print(f\"Fetching: {link}\")\n",
    "            soup = scraper_.get_soup(link)\n",
    "            \n",
    "            soup = scraper_.remove_js_css(soup)\n",
    "            soup = scraper_.remove_unnecessary_sections(soup)\n",
    "\n",
    "            page_data = organize_content_by_heading(soup)\n",
    "            page_data[\"url\"] = link  \n",
    "\n",
    "            all_data.append(page_data)\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching {link}: {e}\")\n",
    "    \n",
    "    return all_data\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    scraper_ = Scraper()\n",
    "\n",
    "    links = [\n",
    "        \"https://pittsburghopera.org/calendar?timequery=week&prev=7+&start=1722484800000&end=1725145140000\",\n",
    "        \"https://pittsburghopera.org/calendar?timequery=week&prev=-1+&start=1722484800000&end=1725145140000\",\n",
    "        \"https://pittsburghopera.org/calendar?timequery=week&prev=-9+&start=1722484800000&end=1725145140000\",\n",
    "        \"https://pittsburghopera.org/calendar?timequery=week&prev=-17+&start=1722484800000&end=1725145140000\",\n",
    "        \"https://pittsburghopera.org/calendar?timequery=week&prev=-25+&start=1722484800000&end=1725145140000\",\n",
    "        \"https://pittsburghopera.org/calendar?timequery=week&prev=-33+&start=1722484800000&end=1725145140000\",\n",
    "        \"https://pittsburghopera.org/calendar?timequery=week&prev=-41+&start=1722484800000&end=17251451400000\",\n",
    "        \"https://pittsburghopera.org/calendar?timequery=week&prev=-57+&start=1722484800000&end=1725145140000\",\n",
    "        \"https://pittsburghopera.org/calendar?timequery=week&prev=-97+&start=1722484800000&end=1725145140000\",\n",
    "        \"https://pittsburghopera.org/calendar?timequery=week&prev=-105+&start=1722484800000&end=1725145140000\",\n",
    "        \"https://pittsburghopera.org/calendar?timequery=week&prev=-113+&start=1722484800000&end=1725145140000\",\n",
    "        \"https://pittsburghopera.org/calendar?timequery=week&prev=-121+&start=1722484800000&end=1725145140000\",\n",
    "        \"https://pittsburghopera.org/calendar?timequery=week&prev=-129+&start=1722484800000&end=1725145140000\",\n",
    "        \"https://pittsburghopera.org/calendar?timequery=week&prev=-145+&start=1722484800000&end=1725145140000\",\n",
    "        \"https://pittsburghopera.org/calendar?timequery=week&prev=-153+&start=1722484800000&end=1725145140000\",\n",
    "        \"https://pittsburghopera.org/calendar?timequery=week&prev=-161+&start=1722484800000&end=1725145140000\",\n",
    "        \"https://pittsburghopera.org/calendar?timequery=week&prev=-169+&start=1722484800000&end=1725145140000\",\n",
    "        \"https://pittsburghopera.org/calendar?timequery=month&prev=-169+&start=1743480000000&end=1746053940000\",\n",
    "        \"https://pittsburghopera.org/calendar?timequery=month&prev=-169+&start=1746072000000&end=1748732340000\",\n",
    "    ]\n",
    "\n",
    "    scraped_data = scrape_links(scraper_, links)\n",
    "    save_to_json(scraped_data, \"../raw_documents/Pittsburgh_Opera.json\")\n",
    "    scraper_.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: webdriver-manager in /home/judysun233/anaconda3/envs/rag_pittsburgh_env/lib/python3.11/site-packages (4.0.2)\n",
      "Requirement already satisfied: requests in /home/judysun233/anaconda3/envs/rag_pittsburgh_env/lib/python3.11/site-packages (from webdriver-manager) (2.32.3)\n",
      "Requirement already satisfied: python-dotenv in /home/judysun233/anaconda3/envs/rag_pittsburgh_env/lib/python3.11/site-packages (from webdriver-manager) (1.0.1)\n",
      "Requirement already satisfied: packaging in /home/judysun233/anaconda3/envs/rag_pittsburgh_env/lib/python3.11/site-packages (from webdriver-manager) (24.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/judysun233/anaconda3/envs/rag_pittsburgh_env/lib/python3.11/site-packages (from requests->webdriver-manager) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/judysun233/anaconda3/envs/rag_pittsburgh_env/lib/python3.11/site-packages (from requests->webdriver-manager) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/judysun233/anaconda3/envs/rag_pittsburgh_env/lib/python3.11/site-packages (from requests->webdriver-manager) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/judysun233/anaconda3/envs/rag_pittsburgh_env/lib/python3.11/site-packages (from requests->webdriver-manager) (2024.8.30)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: selenium in /home/judysun233/anaconda3/envs/rag_pittsburgh_env/lib/python3.11/site-packages (4.25.0)\n",
      "Requirement already satisfied: webdriver-manager in /home/judysun233/anaconda3/envs/rag_pittsburgh_env/lib/python3.11/site-packages (4.0.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.26 in /home/judysun233/anaconda3/envs/rag_pittsburgh_env/lib/python3.11/site-packages (from urllib3[socks]<3,>=1.26->selenium) (2.2.3)\n",
      "Requirement already satisfied: trio~=0.17 in /home/judysun233/anaconda3/envs/rag_pittsburgh_env/lib/python3.11/site-packages (from selenium) (0.26.2)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in /home/judysun233/anaconda3/envs/rag_pittsburgh_env/lib/python3.11/site-packages (from selenium) (0.11.1)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in /home/judysun233/anaconda3/envs/rag_pittsburgh_env/lib/python3.11/site-packages (from selenium) (2024.8.30)\n",
      "Requirement already satisfied: typing_extensions~=4.9 in /home/judysun233/anaconda3/envs/rag_pittsburgh_env/lib/python3.11/site-packages (from selenium) (4.12.2)\n",
      "Requirement already satisfied: websocket-client~=1.8 in /home/judysun233/anaconda3/envs/rag_pittsburgh_env/lib/python3.11/site-packages (from selenium) (1.8.0)\n",
      "Requirement already satisfied: requests in /home/judysun233/anaconda3/envs/rag_pittsburgh_env/lib/python3.11/site-packages (from webdriver-manager) (2.32.3)\n",
      "Requirement already satisfied: python-dotenv in /home/judysun233/anaconda3/envs/rag_pittsburgh_env/lib/python3.11/site-packages (from webdriver-manager) (1.0.1)\n",
      "Requirement already satisfied: packaging in /home/judysun233/anaconda3/envs/rag_pittsburgh_env/lib/python3.11/site-packages (from webdriver-manager) (24.1)\n",
      "Requirement already satisfied: attrs>=23.2.0 in /home/judysun233/anaconda3/envs/rag_pittsburgh_env/lib/python3.11/site-packages (from trio~=0.17->selenium) (24.2.0)\n",
      "Requirement already satisfied: sortedcontainers in /home/judysun233/anaconda3/envs/rag_pittsburgh_env/lib/python3.11/site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: idna in /home/judysun233/anaconda3/envs/rag_pittsburgh_env/lib/python3.11/site-packages (from trio~=0.17->selenium) (3.10)\n",
      "Requirement already satisfied: outcome in /home/judysun233/anaconda3/envs/rag_pittsburgh_env/lib/python3.11/site-packages (from trio~=0.17->selenium) (1.3.0.post0)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in /home/judysun233/anaconda3/envs/rag_pittsburgh_env/lib/python3.11/site-packages (from trio~=0.17->selenium) (1.3.1)\n",
      "Requirement already satisfied: wsproto>=0.14 in /home/judysun233/anaconda3/envs/rag_pittsburgh_env/lib/python3.11/site-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
      "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /home/judysun233/anaconda3/envs/rag_pittsburgh_env/lib/python3.11/site-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/judysun233/anaconda3/envs/rag_pittsburgh_env/lib/python3.11/site-packages (from requests->webdriver-manager) (3.4.0)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in /home/judysun233/anaconda3/envs/rag_pittsburgh_env/lib/python3.11/site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install webdriver-manager\n",
    "%pip install --upgrade selenium webdriver-manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "活动信息已成功追加到文件 ../raw_documents/Music_Culture/2_Opera.txt 中！\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "input_file = 'input.txt'\n",
    "output_file = '../raw_documents/Music_Culture/2_Opera.txt'\n",
    "\n",
    "def process_event_block(event_block):\n",
    "    event_name = ''\n",
    "    event_date = ''\n",
    "    event_time = ''\n",
    "    venue = 'No venue'\n",
    "\n",
    "    event_name_match = re.search(r'title\\.textContent\\s*=\\s*\"(.+?)\"', event_block)\n",
    "    if event_name_match:\n",
    "        event_name = event_name_match.group(1).strip()\n",
    "\n",
    "    event_date_match = re.search(r'paragraphDate\\.textContent\\s*=\\s*\"(.+?)\"', event_block)\n",
    "    if event_date_match:\n",
    "        event_date = event_date_match.group(1).strip()\n",
    "\n",
    "    event_time_match = re.search(r'paragraphTime\\.textContent\\s*=\\s*\"(.+?)\"', event_block)\n",
    "    if event_time_match:\n",
    "        event_time = event_time_match.group(1).strip()\n",
    "\n",
    "    venue_match = re.search(r'paragraph\\.innerHTML\\s*=\\s*\\'(.+?)\\'', event_block)\n",
    "    if venue_match:\n",
    "        venue_html = venue_match.group(1).strip()\n",
    "        venue = re.sub(r'<.*?>', '', venue_html)\n",
    "    if venue == '':\n",
    "        venue = None\n",
    "\n",
    "    if venue:\n",
    "        event_detail = f\"Event Name: {event_name}, Event Time: {event_date}{event_time}, Venue: {venue}, Organization: Pittsburgh Opera\"\n",
    "    else:\n",
    "        event_detail = f\"Event Name: {event_name}, Event Time: {event_date}{event_time}, Organization: Pittsburgh Opera\"\n",
    "\n",
    "    return event_detail\n",
    "\n",
    "def append_to_txt_file(event_details):\n",
    "    with open(output_file, 'a', encoding='utf-8') as file:\n",
    "        for event in event_details:\n",
    "            file.write(event + '\\n')\n",
    "\n",
    "def main():\n",
    "    with open(input_file, 'r', encoding='utf-8') as file:\n",
    "        content = file.read()\n",
    "\n",
    "    event_blocks = re.split(r'(?:title\\.textContent)', content)\n",
    "    event_blocks = ['title.textContent' + block for block in event_blocks if block.strip()]\n",
    "\n",
    "    event_details = []\n",
    "    for block in event_blocks:\n",
    "        event_detail = process_event_block(block)\n",
    "        event_details.append(event_detail)\n",
    "\n",
    "    append_to_txt_file(event_details)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import sys\n",
    "import re\n",
    "\n",
    "def extract_text_from_url(url):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()  \n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        title = soup.title.string.strip() if soup.title else 'No Title'\n",
    "\n",
    "        for tag in soup([\"script\", \"style\", \"header\", \"footer\", \"nav\", \"aside\"]):\n",
    "            tag.decompose()\n",
    "\n",
    "        for tag in soup.find_all(class_=re.compile(\".*(nav|footer|menu|social|sidebar).*\", re.I)):\n",
    "            tag.decompose()\n",
    "        for tag in soup.find_all(id=re.compile(\".*(nav|footer|menu|social|sidebar).*\", re.I)):\n",
    "            tag.decompose()\n",
    "\n",
    "        stop_phrases = [\n",
    "            \"Follow us on\", \"Contact Us\", \"Privacy Policy\", \"©\",\n",
    "            \"BUY TICKETS\", \"GIVE NOW\", \"SUBSCRIBE\", \"Site Map\",\n",
    "            \"Board Of Directors Login\"\n",
    "        ]\n",
    "\n",
    "        content = []\n",
    "        for element in soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'p', 'li']):\n",
    "            text = ' '.join(element.stripped_strings)\n",
    "            if text:\n",
    "                if any(phrase.lower() in text.lower() for phrase in stop_phrases):\n",
    "                    continue\n",
    "                content.append(text)\n",
    "\n",
    "        if not content:\n",
    "            print(f\"No content found for {url}\", file=sys.stderr)\n",
    "            return f\"URL: {url}\\nTitle: {title}\\n\\nNo relevant content found.\\n\\n{'-'*80}\\n\\n\"\n",
    "\n",
    "        formatted_paragraphs = []\n",
    "        current_paragraph = ''\n",
    "        for line in content:\n",
    "            if line.endswith(':'):\n",
    "                if current_paragraph:\n",
    "                    formatted_paragraphs.append(current_paragraph.strip('; '))\n",
    "                current_paragraph = line.rstrip(':') + ': '\n",
    "            else:\n",
    "                current_paragraph += line + '; '\n",
    "\n",
    "        if current_paragraph:\n",
    "            formatted_paragraphs.append(current_paragraph.strip('; '))\n",
    "\n",
    "        formatted_text = '\\n\\n'.join(formatted_paragraphs)\n",
    "\n",
    "        return f\"URL: {url}\\nTitle: {title}\\n\\n{formatted_text}\\n\\n{'-'*80}\\n\\n\"\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching {url}: {e}\", file=sys.stderr)\n",
    "        return ''\n",
    "\n",
    "def append_text_to_file(text, file_path):\n",
    "    with open(file_path, 'a', encoding='utf-8') as f:\n",
    "        f.write(text)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    links = [\n",
    "        \"https://pittsburghopera.org/tickets/opera-faqs/\",\n",
    "        \"https://pittsburghopera.org/tickets/subscribe-today\",\n",
    "        \"https://pittsburghopera.org/tickets/season-pass\",\n",
    "        \"https://pittsburghopera.org/tickets/groups-tickets\",\n",
    "        \"https://pittsburghopera.org/tickets/promotions-and-discounts/\",\n",
    "        \"https://pittsburghopera.org/tickets/student-tickets\",\n",
    "        \"https://pittsburghopera.org/tickets/cheap-seats\",\n",
    "        \"https://pittsburghopera.org/tickets/give-the-gift-of-opera/\",\n",
    "        \"https://pittsburghopera.org/tickets/ticket-offices-policies/\",\n",
    "        \"https://pittsburghopera.org/tickets/accessibility/\",\n",
    "        \"https://pittsburghopera.org/tickets/free-rideshare-vouchers/\",\n",
    "        \"https://pittsburghopera.org/tickets/free-childcare-services\"\n",
    "    ]\n",
    "    output_file = '../raw_documents/Music_Culture/2_Opera.txt'\n",
    "\n",
    "    for url in links:\n",
    "        text_content = extract_text_from_url(url)\n",
    "        append_text_to_file(text_content, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned data saved to ../raw_documents/Pittsburgh_Opera_Cleaned.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "def clean_text(text: str):\n",
    "    cleaned_text = re.sub(r'\\n+', ' ', text)\n",
    "    return cleaned_text.strip()\n",
    "\n",
    "def clean_json_file(input_file, output_file):\n",
    "    try:\n",
    "        with open(input_file, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "        for entry in data:\n",
    "            if 'content' in entry:\n",
    "                entry['content'] = clean_text(entry['content'])\n",
    "\n",
    "        with open(output_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "        print(f\"Cleaned data saved to {output_file}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing JSON file: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_file = \"../raw_documents/Pittsburgh_Opera.json\"  \n",
    "    output_file = \"../raw_documents/Pittsburgh_Opera_Cleaned.json\"\n",
    "    clean_json_file(input_file, output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cultural Trust\n",
    "### https://trustarts.org/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping page: https://trustarts.org/\n",
      "Scraping page: https://trustarts.org/calendar?utf8=%E2%9C%93&utf8=%E2%9C%93&genre=All+Genres&organization_id=&start_date=&end_date=2017%2F06%2F14&filter%5Bmin%5D=2024-10-15T13%3A07%3A06-04%3A00&filter%5Bmax%5D=2026-04-15+13%3A07%3A06+-0400&filter%5Bcurrent_page%5D=production\n",
      "Error parsing event: 'NoneType' object has no attribute 'get_text'\n",
      "Error parsing event: 'NoneType' object has no attribute 'get_text'\n",
      "Error parsing event: 'NoneType' object has no attribute 'get_text'\n",
      "Error parsing event: 'NoneType' object has no attribute 'get_text'\n",
      "Error parsing event: 'NoneType' object has no attribute 'get_text'\n",
      "Scraping page: https://trustarts.org/calendar?end_date=722&genre=&order_by=production&page=2\n",
      "Error parsing event: 'NoneType' object has no attribute 'get_text'\n",
      "Error parsing event: 'NoneType' object has no attribute 'get_text'\n",
      "Error parsing event: 'NoneType' object has no attribute 'get_text'\n",
      "Error parsing event: 'NoneType' object has no attribute 'get_text'\n",
      "Error parsing event: 'NoneType' object has no attribute 'get_text'\n",
      "Scraping page: https://trustarts.org/calendar?ad=40&am=broad&askid=2f811b4c-704f-4054-a821-1b9934816698-0-ab_msb&l=sem&o=22837&page=3&q=Byham+Theater+Pittsburgh&qsrc=999\n",
      "Error parsing event: 'NoneType' object has no attribute 'get_text'\n",
      "Error parsing event: 'NoneType' object has no attribute 'get_text'\n",
      "Error parsing event: 'NoneType' object has no attribute 'get_text'\n",
      "Error parsing event: 'NoneType' object has no attribute 'get_text'\n",
      "Error parsing event: 'NoneType' object has no attribute 'get_text'\n",
      "Scraping page: https://trustarts.org/calendar?cluid=3794577&page=4\n",
      "Error parsing event: 'NoneType' object has no attribute 'get_text'\n",
      "Error parsing event: 'NoneType' object has no attribute 'get_text'\n",
      "Error parsing event: 'NoneType' object has no attribute 'get_text'\n",
      "Error parsing event: 'NoneType' object has no attribute 'get_text'\n",
      "Error parsing event: 'NoneType' object has no attribute 'get_text'\n",
      "Scraping page: https://trustarts.org/calendar?end_date=106&genre=&page=5\n",
      "Error parsing event: 'NoneType' object has no attribute 'get_text'\n",
      "Error parsing event: 'NoneType' object has no attribute 'get_text'\n",
      "Error parsing event: 'NoneType' object has no attribute 'get_text'\n",
      "Error parsing event: 'NoneType' object has no attribute 'get_text'\n",
      "Error parsing event: 'NoneType' object has no attribute 'get_text'\n",
      "Scraping page: https://trustarts.org/calendar?cid=Tess_Order&cluid=294&page=6\n",
      "Error parsing event: 'NoneType' object has no attribute 'get_text'\n",
      "Error parsing event: 'NoneType' object has no attribute 'get_text'\n",
      "Error parsing event: 'NoneType' object has no attribute 'get_text'\n",
      "Error parsing event: 'NoneType' object has no attribute 'get_text'\n",
      "Error parsing event: 'NoneType' object has no attribute 'get_text'\n",
      "Scraping page: https://trustarts.org/calendar?ad=102&am=532&an=msn_s&l=sem&o=22837&page=7\n",
      "Error parsing event: 'NoneType' object has no attribute 'get_text'\n",
      "Error parsing event: 'NoneType' object has no attribute 'get_text'\n",
      "Error parsing event: 'NoneType' object has no attribute 'get_text'\n",
      "Error parsing event: 'NoneType' object has no attribute 'get_text'\n",
      "Error parsing event: 'NoneType' object has no attribute 'get_text'\n",
      "Scraping page: https://trustarts.org/calendar?end_date=364&order_by=production&page=8\n",
      "Error parsing event: 'NoneType' object has no attribute 'get_text'\n",
      "Error parsing event: 'NoneType' object has no attribute 'get_text'\n",
      "Error parsing event: 'NoneType' object has no attribute 'get_text'\n",
      "Error parsing event: 'NoneType' object has no attribute 'get_text'\n",
      "Error parsing event: 'NoneType' object has no attribute 'get_text'\n",
      "Scraping page: https://trustarts.org/calendar?ad=102&am=532&an=324&l=sem&page=9&q=594&qsrc=999\n",
      "Error parsing event: 'NoneType' object has no attribute 'get_text'\n",
      "Error parsing event: 'NoneType' object has no attribute 'get_text'\n",
      "Error parsing event: 'NoneType' object has no attribute 'get_text'\n",
      "Error parsing event: 'NoneType' object has no attribute 'get_text'\n",
      "Error parsing event: 'NoneType' object has no attribute 'get_text'\n",
      "Scraping page: https://trustarts.org/calendar?ad=172&am=broad&an=msn_s&l=sem&o=22837&page=10&q=Pittsburgh%2BCultural%2BTrust&qsrc=999\n",
      "Error parsing event: 'NoneType' object has no attribute 'get_text'\n",
      "Error parsing event: 'NoneType' object has no attribute 'get_text'\n",
      "Error parsing event: 'NoneType' object has no attribute 'get_text'\n",
      "Error parsing event: 'NoneType' object has no attribute 'get_text'\n",
      "Error parsing event: 'NoneType' object has no attribute 'get_text'\n",
      "Scraping page: https://trustarts.org/calendar?end_date=&genre=All+Genres&order_by=production&page=11\n",
      "Error parsing event: 'NoneType' object has no attribute 'get_text'\n",
      "Error parsing event: 'NoneType' object has no attribute 'get_text'\n",
      "Error parsing event: 'NoneType' object has no attribute 'get_text'\n",
      "Error parsing event: 'NoneType' object has no attribute 'get_text'\n",
      "Error parsing event: 'NoneType' object has no attribute 'get_text'\n",
      "Scraping page: https://trustarts.org/calendar?cid=tess_order&cluid=830&page=12\n",
      "Error parsing event: 'NoneType' object has no attribute 'get_text'\n",
      "Error parsing event: 'NoneType' object has no attribute 'get_text'\n",
      "Error parsing event: 'NoneType' object has no attribute 'get_text'\n",
      "Error parsing event: 'NoneType' object has no attribute 'get_text'\n",
      "Error parsing event: 'NoneType' object has no attribute 'get_text'\n",
      "Scraping page: https://trustarts.org/calendar?cid=tess_order&cluid=830&page=13\n",
      "Error parsing event: 'NoneType' object has no attribute 'get_text'\n",
      "Error parsing event: 'NoneType' object has no attribute 'get_text'\n",
      "Error parsing event: 'NoneType' object has no attribute 'get_text'\n",
      "Error parsing event: 'NoneType' object has no attribute 'get_text'\n",
      "Error parsing event: 'NoneType' object has no attribute 'get_text'\n",
      "Scraping page: https://trustarts.org/calendar?cid=tess_order&cluid=830&page=14\n",
      "Error parsing event: 'NoneType' object has no attribute 'get_text'\n",
      "Error parsing event: 'NoneType' object has no attribute 'get_text'\n",
      "Error parsing event: 'NoneType' object has no attribute 'get_text'\n",
      "Error parsing event: 'NoneType' object has no attribute 'get_text'\n",
      "Error parsing event: 'NoneType' object has no attribute 'get_text'\n",
      "Scraping page: https://trustarts.org/calendar?cid=12&page=15\n",
      "Error parsing event: 'NoneType' object has no attribute 'get_text'\n",
      "Error parsing event: 'NoneType' object has no attribute 'get_text'\n",
      "Error parsing event: 'NoneType' object has no attribute 'get_text'\n",
      "Error parsing event: 'NoneType' object has no attribute 'get_text'\n",
      "Error parsing event: 'NoneType' object has no attribute 'get_text'\n",
      "Scraping page: https://trustarts.org/calendar?ad=102&page=16&q=Benedum%2BCenter&qsrc=274\n",
      "Error parsing event: 'NoneType' object has no attribute 'get_text'\n",
      "Error parsing event: 'NoneType' object has no attribute 'get_text'\n",
      "Error parsing event: 'NoneType' object has no attribute 'get_text'\n",
      "Error parsing event: 'NoneType' object has no attribute 'get_text'\n",
      "Error parsing event: 'NoneType' object has no attribute 'get_text'\n",
      "Data saved to ../raw_documents/Pittsburgh_Trustarts.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "class Scraper:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.chrome_options = Options()\n",
    "        self.chrome_options.add_argument(\"--headless\")\n",
    "        self.chrome_options.add_argument(\"--disable-gpu\")\n",
    "        self.chrome_options.add_argument(\"--no-sandbox\")\n",
    "        self.chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "        self.driver = webdriver.Chrome(options=self.chrome_options)\n",
    "        self.driver.set_page_load_timeout(10) \n",
    "        self.base_url = \"https://trustarts.org\"\n",
    "\n",
    "    def get_soup(self, url: str):\n",
    "        self.driver.get(url)\n",
    "        time.sleep(2)  \n",
    "        return BeautifulSoup(self.driver.page_source, \"html.parser\")\n",
    "    \n",
    "    def scrape_event(self, soup):\n",
    "        events = []\n",
    "        event_tags = soup.find_all('article', class_='event')\n",
    "\n",
    "        for event_tag in event_tags:\n",
    "            try:\n",
    "                title = event_tag.find('h3', class_='title').get_text(strip=True)\n",
    "                date = event_tag.find('time', class_='range').get_text(strip=True)\n",
    "\n",
    "                venue = event_tag.find('div', class_='venue').get_text(strip=True)\n",
    "                organization = event_tag.find('div', class_='organization').get_text(strip=True)\n",
    "\n",
    "                categories = [cat.get_text(strip=True) for cat in event_tag.find_all('li', class_='category')]\n",
    "\n",
    "                subpage_url = event_tag.find('a')['href']\n",
    "\n",
    "                event_data = {\n",
    "                    \"title\": title,\n",
    "                    \"date\": date,\n",
    "                    \"venue\": venue,\n",
    "                    \"organization\": organization,\n",
    "                    \"category\": categories,\n",
    "                    \"url\": f\"{self.base_url}{subpage_url}\",\n",
    "                }\n",
    "\n",
    "                events.append(event_data)\n",
    "\n",
    "            except AttributeError as e:\n",
    "                print(f\"Error parsing event: {e}\")\n",
    "        return events\n",
    "\n",
    "    def scrape_events_from_pages(self, urls):\n",
    "        all_events = []\n",
    "        for url in urls:\n",
    "            print(f\"Scraping page: {url}\")\n",
    "            soup = self.get_soup(url)\n",
    "            events = self.scrape_event(soup)\n",
    "            all_events.extend(events)\n",
    "        return all_events\n",
    "\n",
    "    def close(self):\n",
    "        self.driver.quit()\n",
    "\n",
    "    def __del__(self):\n",
    "        self.close()\n",
    "\n",
    "\n",
    "def save_to_json(data, filename):\n",
    "    try:\n",
    "        with open(filename, 'w', encoding='utf-8') as f:\n",
    "            json.dump(data, f, ensure_ascii=False, indent=4)\n",
    "        print(f\"Data saved to {filename}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error writing to JSON file: {e}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    scraper = Scraper()\n",
    "    urls = [\n",
    "        \"https://trustarts.org/\",\n",
    "        \"https://trustarts.org/calendar?utf8=%E2%9C%93&utf8=%E2%9C%93&genre=All+Genres&organization_id=&start_date=&end_date=2017%2F06%2F14&filter%5Bmin%5D=2024-10-15T13%3A07%3A06-04%3A00&filter%5Bmax%5D=2026-04-15+13%3A07%3A06+-0400&filter%5Bcurrent_page%5D=production\",\n",
    "        \"https://trustarts.org/calendar?end_date=722&genre=&order_by=production&page=2\",\n",
    "        \"https://trustarts.org/calendar?ad=40&am=broad&askid=2f811b4c-704f-4054-a821-1b9934816698-0-ab_msb&l=sem&o=22837&page=3&q=Byham+Theater+Pittsburgh&qsrc=999\",\n",
    "        \"https://trustarts.org/calendar?cluid=3794577&page=4\",\n",
    "        \"https://trustarts.org/calendar?end_date=106&genre=&page=5\",\n",
    "        \"https://trustarts.org/calendar?cid=Tess_Order&cluid=294&page=6\",\n",
    "        \"https://trustarts.org/calendar?ad=102&am=532&an=msn_s&l=sem&o=22837&page=7\",\n",
    "        \"https://trustarts.org/calendar?end_date=364&order_by=production&page=8\",\n",
    "        \"https://trustarts.org/calendar?ad=102&am=532&an=324&l=sem&page=9&q=594&qsrc=999\",\n",
    "        \"https://trustarts.org/calendar?ad=172&am=broad&an=msn_s&l=sem&o=22837&page=10&q=Pittsburgh%2BCultural%2BTrust&qsrc=999\",\n",
    "        \"https://trustarts.org/calendar?end_date=&genre=All+Genres&order_by=production&page=11\",\n",
    "        \"https://trustarts.org/calendar?cid=tess_order&cluid=830&page=12\",\n",
    "        \"https://trustarts.org/calendar?cid=tess_order&cluid=830&page=13\",\n",
    "        \"https://trustarts.org/calendar?cid=tess_order&cluid=830&page=14\",\n",
    "        \"https://trustarts.org/calendar?cid=12&page=15\",\n",
    "        \"https://trustarts.org/calendar?ad=102&page=16&q=Benedum%2BCenter&qsrc=274\"\n",
    "    ]\n",
    "\n",
    "    scraped_events = scraper.scrape_events_from_pages(urls)\n",
    "    save_to_json(scraped_events, \"../raw_documents/Pittsburgh_Trustarts.json\")\n",
    "    scraper.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching: https://trustarts.org/pct_home/events/series\n",
      "Fetching: https://trustarts.org/pct_home/events/festivals\n",
      "Fetching: https://trustarts.org/pct_home/events/groups\n",
      "Fetching: https://trustarts.org/pct_home/events/university-student-tickets\n",
      "Fetching: https://trustarts.org/pct_home/events/seating-charts\n",
      "Fetching: https://trustarts.org/pct_home/events/faq---ticketing\n",
      "Fetching: https://trustarts.org/pct_home/events/gift-cards\n",
      "Fetching: https://trustarts.org/pct_home/events/official-ticket-source\n",
      "Fetching: https://trustarts.org/pct_home/events/venue-tours\n",
      "Fetching: https://trustarts.org/pct_home/visual-arts#current\n",
      "Fetching: https://trustarts.org/pct_home/visual-arts#upcoming\n",
      "Fetching: https://trustarts.org/pct_home/visual-arts#galleries\n",
      "Fetching: https://trustarts.org/pct_home/engagement\n",
      "Fetching: https://trustarts.org/pct_home/engagement/lullaby-project\n",
      "Fetching: https://trustarts.org/pct_home/engagement/broadway-talk-back-series\n",
      "Fetching: https://trustarts.org/pct_home/engagement/community-classes-with-mr-messado\n",
      "Fetching: https://trustarts.org/pct_home/engagement/cultural-celebrations\n",
      "Fetching: https://trustarts.org/pct_home/visit\n",
      "Fetching: https://trustarts.org/pct_home/about\n",
      "Data saved to ../raw_documents/Pittsburgh_Trustarts_copy.json\n"
     ]
    }
   ],
   "source": [
    "#################### Except events, more information ####################\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import os\n",
    "\n",
    "class Scraper:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.chrome_options = Options()\n",
    "        self.chrome_options.add_argument(\"--headless\")\n",
    "        self.chrome_options.add_argument(\"--disable-gpu\")\n",
    "        self.chrome_options.add_argument(\"--no-sandbox\")\n",
    "        self.chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "        self.driver = webdriver.Chrome(options=self.chrome_options)\n",
    "        self.driver.set_page_load_timeout(10) \n",
    "        self.current_url = None\n",
    "        self.current_domain = None\n",
    "\n",
    "    def set_domain(self, url: str):\n",
    "        \"\"\"Set domain name and URL.\"\"\"\n",
    "        self.current_domain = url.split(\"/\")[2] \n",
    "        self.current_url = url\n",
    "\n",
    "    def fetch(self, url: str, raw_html: bool = False):\n",
    "        \"\"\"Capture the webpage and choose whether to remove JS or CSS.\"\"\"\n",
    "        soup = self.get_soup(url)\n",
    "        if not raw_html:\n",
    "            soup = self.remove_js_css(soup)\n",
    "        return soup, self.get_links(soup), self.get_title(soup)\n",
    "\n",
    "    def get_html(self, url: str):\n",
    "        \"\"\"Get HTML.\"\"\"\n",
    "        self.set_domain(url)\n",
    "        self.driver.get(url)\n",
    "        return self.driver.page_source\n",
    "\n",
    "    def get_soup(self, url: str):\n",
    "        \"\"\"Parse and get BeautifulSoup object.\"\"\"\n",
    "        html = self.get_html(url)\n",
    "        return BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "    def get_links(self, soup: BeautifulSoup):\n",
    "        \"\"\"Get external links.\"\"\"\n",
    "        links = [link.get(\"href\") for link in soup.find_all(\"a\")]\n",
    "        return self.filter_links(links)\n",
    "\n",
    "    def get_title(self, soup: BeautifulSoup):\n",
    "        \"\"\"Get the title.\"\"\"\n",
    "        if soup.title is None:\n",
    "            return f\"untitled_{self.get_timestamp()}\"\n",
    "        title = soup.title.string.replace(\" \", \"_\").replace(\"/\", \"__\")\n",
    "        return title.replace(\"\\n\", \"\")\n",
    "\n",
    "    def remove_js_css(self, soup: BeautifulSoup):\n",
    "        \"\"\"Remove JS and CSS.\"\"\"\n",
    "        for script in soup([\"script\", \"style\"]):\n",
    "            script.extract() \n",
    "        return soup\n",
    "\n",
    "    def remove_unnecessary_sections(self, soup: BeautifulSoup):\n",
    "        \"\"\"Remove unnecessary sections such as header, footer, nav, etc.\"\"\"\n",
    "        for tag in [\"header\", \"footer\", \"nav\", \"aside\"]:\n",
    "            for element in soup.find_all(tag):\n",
    "                element.decompose()\n",
    "        return soup\n",
    "\n",
    "    def filter_links(self, links: list):\n",
    "        filtered_links = []\n",
    "        for link in links:\n",
    "            if link is None:\n",
    "                continue\n",
    "            elif link.startswith(\"#\"):\n",
    "                continue\n",
    "            elif link.startswith(\"http\"):\n",
    "                filtered_links.append(link)\n",
    "            elif link.startswith(\"//\"):\n",
    "                filtered_links.append(f\"https:{link}\")\n",
    "            elif link.startswith(\"/\"):\n",
    "                filtered_links.append(f\"https://{self.current_domain}{link}\")\n",
    "            else:\n",
    "                filtered_links.append(f\"{self.current_url}/{link}\")\n",
    "        return filtered_links\n",
    "\n",
    "    def get_timestamp(self):\n",
    "        return datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "    def close(self):\n",
    "        self.driver.quit()\n",
    "\n",
    "    def __del__(self):\n",
    "        self.close()\n",
    "\n",
    "\n",
    "def organize_content_by_heading(soup: BeautifulSoup):\n",
    "    \"\"\"Organize content based on headings, ensuring all relevant text is captured.\"\"\"\n",
    "    data = {}\n",
    "    current_h2, current_h3, current_h4, current_h5, current_h6 = None, None, None, None, None\n",
    "\n",
    "    for tag in soup.find_all(['h2', 'h3', 'h4', 'h5', 'h6', 'p', 'div', 'span', 'article']):\n",
    "        if tag.name == 'h2':\n",
    "            current_h2 = tag.get_text(strip=True)\n",
    "            data[current_h2] = {}\n",
    "            current_h3 = current_h4 = current_h5 = current_h6 = None \n",
    "        elif tag.name == 'h3' and current_h2:\n",
    "            current_h3 = tag.get_text(strip=True)\n",
    "            data[current_h2][current_h3] = {}\n",
    "            current_h4 = current_h5 = current_h6 = None\n",
    "        elif tag.name == 'h4' and current_h3:\n",
    "            current_h4 = tag.get_text(strip=True)\n",
    "            data[current_h2][current_h3][current_h4] = {}\n",
    "            current_h5 = current_h6 = None\n",
    "        elif tag.name == 'h5' and current_h4:\n",
    "            current_h5 = tag.get_text(strip=True)\n",
    "            data[current_h2][current_h3][current_h4][current_h5] = {}\n",
    "            current_h6 = None\n",
    "        elif tag.name == 'h6' and current_h5:\n",
    "            current_h6 = tag.get_text(strip=True)\n",
    "            data[current_h2][current_h3][current_h4][current_h5][current_h6] = {}\n",
    "        elif tag.name in ['p', 'div', 'span', 'article']:\n",
    "            content = tag.get_text(strip=True)\n",
    "            if content:\n",
    "                if current_h6:\n",
    "                    data[current_h2][current_h3][current_h4][current_h5][current_h6].setdefault('content', []).append(content)\n",
    "                elif current_h5:\n",
    "                    data[current_h2][current_h3][current_h4][current_h5].setdefault('content', []).append(content)\n",
    "                elif current_h4:\n",
    "                    data[current_h2][current_h3][current_h4].setdefault('content', []).append(content)\n",
    "                elif current_h3:\n",
    "                    data[current_h2][current_h3].setdefault('content', []).append(content)\n",
    "                elif current_h2:\n",
    "                    data[current_h2].setdefault('content', []).append(content)\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def save_to_json(data, filename):\n",
    "    \"\"\"Save data to a JSON file, ensuring no overwrite of existing content.\"\"\"\n",
    "    try:\n",
    "        if os.path.exists(filename):\n",
    "            with open(filename, 'r', encoding='utf-8') as f:\n",
    "                existing_data = json.load(f)\n",
    "            if isinstance(existing_data, list):\n",
    "                existing_data.extend(data)\n",
    "            else:\n",
    "                existing_data.update(data) \n",
    "        else:\n",
    "            existing_data = data \n",
    "\n",
    "        with open(filename, 'w', encoding='utf-8') as f:\n",
    "            json.dump(existing_data, f, ensure_ascii=False, indent=4)\n",
    "        print(f\"Data saved to {filename}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error writing to JSON file: {e}\")\n",
    "\n",
    "\n",
    "def scrape_links(scraper_, links):\n",
    "    all_data = []\n",
    "    for link in links:\n",
    "        try:\n",
    "            print(f\"Fetching: {link}\")\n",
    "            soup = scraper_.get_soup(link)\n",
    "\n",
    "            soup = scraper_.remove_js_css(soup)\n",
    "            soup = scraper_.remove_unnecessary_sections(soup)\n",
    "\n",
    "            page_data = organize_content_by_heading(soup)\n",
    "            page_data[\"url\"] = link\n",
    "\n",
    "            all_data.append(page_data)\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching {link}: {e}\")\n",
    "\n",
    "    return all_data\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    scraper_ = Scraper()\n",
    "\n",
    "    links = [\n",
    "        \"https://trustarts.org/pct_home/events/series\",\n",
    "        \"https://trustarts.org/pct_home/events/festivals\",\n",
    "        \"https://trustarts.org/pct_home/events/groups\",\n",
    "        \"https://trustarts.org/pct_home/events/university-student-tickets\",\n",
    "        \"https://trustarts.org/pct_home/events/seating-charts\",\n",
    "        \"https://trustarts.org/pct_home/events/faq---ticketing\",\n",
    "        \"https://trustarts.org/pct_home/events/gift-cards\",\n",
    "        \"https://trustarts.org/pct_home/events/official-ticket-source\",\n",
    "        \"https://trustarts.org/pct_home/events/venue-tours\",\n",
    "        \"https://trustarts.org/pct_home/visual-arts#current\",\n",
    "        \"https://trustarts.org/pct_home/visual-arts#upcoming\",\n",
    "        \"https://trustarts.org/pct_home/visual-arts#galleries\",\n",
    "        \"https://trustarts.org/pct_home/engagement\",\n",
    "        \"https://trustarts.org/pct_home/engagement/lullaby-project\",\n",
    "        \"https://trustarts.org/pct_home/engagement/broadway-talk-back-series\",\n",
    "        \"https://trustarts.org/pct_home/engagement/community-classes-with-mr-messado\",\n",
    "        \"https://trustarts.org/pct_home/engagement/cultural-celebrations\",\n",
    "        \"https://trustarts.org/pct_home/visit\",\n",
    "        \"https://trustarts.org/pct_home/about\"\n",
    "    ]\n",
    "\n",
    "    scraped_data = scrape_links(scraper_, links)\n",
    "    save_to_json(scraped_data, \"../raw_documents/Pittsburgh_Trustarts_copy.json\")\n",
    "    scraper_.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fetching https://trustarts.org: 403 Client Error: Forbidden for url: https://trustarts.org/\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin, urlparse\n",
    "import re\n",
    "\n",
    "visited = set()\n",
    "\n",
    "def is_valid_url(url, base_url):\n",
    "    return url.startswith(base_url)\n",
    "\n",
    "def extract_subpages(url, base_url, current_depth, max_depth):\n",
    "    if current_depth > max_depth or url in visited:\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/85.0.4183.121 Safari/537.36'\n",
    "        }\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status() \n",
    "        visited.add(url)  \n",
    "        \n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        print(url) \n",
    "    \n",
    "        for link in soup.find_all('a', href=True):\n",
    "            subpage_url = link['href']\n",
    "            subpage_url = urljoin(base_url, subpage_url)\n",
    "            \n",
    "            if is_valid_url(subpage_url, base_url) and subpage_url not in visited:\n",
    "                extract_subpages(subpage_url, base_url, current_depth + 1, max_depth)\n",
    "\n",
    "        time.sleep(5)\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching {url}: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # base_url = \"https://trustarts.org/pct_home/\"\n",
    "    base_url = \"https://trustarts.org/\"\n",
    "    start_urls = [\n",
    "        \"https://trustarts.org/\"\n",
    "        # \"https://trustarts.org/pct_home/events\",\n",
    "        # \"https://trustarts.org/pct_home/visual-arts\",\n",
    "        # \"https://trustarts.org/pct_home/engagement\",\n",
    "        # \"https://trustarts.org/pct_home/support\",\n",
    "        # \"https://trustarts.org/pct_home/visit\",\n",
    "        # \"https://trustarts.org/pct_home/about\"\n",
    "    ]\n",
    "    \n",
    "    max_depth = 5 \n",
    "    for start_url in start_urls:\n",
    "        extract_subpages(start_url, base_url, current_depth=0, max_depth=max_depth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Museum\n",
    "## Carnegie Museum\n",
    "### https://carnegiemuseums.org/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching: https://carnegiemuseums.org/events/\n",
      "Fetching: https://carnegiemuseums.org/events/page/2/\n",
      "Fetching: https://carnegiemuseums.org/events/page/3/\n",
      "Fetching: https://carnegiemuseums.org/events/page/4/\n",
      "Fetching: https://carnegiemuseums.org/events/page/5/\n",
      "Data saved to ../raw_documents/Pittsburgh_Carnegie_Museums.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "class Scraper:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.chrome_options = Options()\n",
    "        self.chrome_options.add_argument(\"--headless\")\n",
    "        self.chrome_options.add_argument(\"--disable-gpu\")\n",
    "        self.chrome_options.add_argument(\"--no-sandbox\")\n",
    "        self.chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "        self.driver = webdriver.Chrome(options=self.chrome_options)\n",
    "        self.driver.set_page_load_timeout(20) \n",
    "        self.current_url = None\n",
    "        self.current_domain = None\n",
    "\n",
    "    def set_domain(self, url: str):\n",
    "        \"\"\"set domain name and URL\"\"\"\n",
    "        self.current_domain = url.split(\"/\")[2]\n",
    "        self.current_url = url\n",
    "\n",
    "    def fetch(self, url: str):\n",
    "        \"\"\"capture the webpage\"\"\"\n",
    "        soup = self.get_soup(url)\n",
    "        return soup\n",
    "\n",
    "    def get_html(self, url: str):\n",
    "        \"\"\"get HTML\"\"\"\n",
    "        self.set_domain(url)\n",
    "        self.driver.get(url)\n",
    "        return self.driver.page_source\n",
    "\n",
    "    def get_soup(self, url: str):\n",
    "        \"\"\"parse and get BeautifulSoup object\"\"\"\n",
    "        html = self.get_html(url)\n",
    "        return BeautifulSoup(html, \"html.parser\")\n",
    "    \n",
    "    def remove_unnecessary_sections(self, soup: BeautifulSoup):\n",
    "        \"\"\"remove header and footer\"\"\"\n",
    "        for tag in [\"header\", \"footer\", \"nav\", \"aside\"]:\n",
    "            for element in soup.find_all(tag):\n",
    "                element.decompose()\n",
    "        return soup\n",
    "\n",
    "    def close(self):\n",
    "        self.driver.quit()\n",
    "\n",
    "    def __del__(self):\n",
    "        self.close()\n",
    "\n",
    "def scrape_links(scraper_, links):\n",
    "    all_data = []\n",
    "    for link in links:\n",
    "        try:\n",
    "            print(f\"Fetching: {link}\")\n",
    "            soup = scraper_.get_soup(link)\n",
    "            events = soup.find_all('article', class_='event-card')\n",
    "\n",
    "            for event in events:\n",
    "                page_data = {}\n",
    "\n",
    "                event_name_tag = event.find('h2')\n",
    "                event_name = event_name_tag.text.strip() if event_name_tag else \"No Event Name\"\n",
    "                page_data[\"event_name\"] = event_name\n",
    "\n",
    "                event_start = event.get('data-event-start', 'No Start Date')\n",
    "                event_end = event.get('data-event-end', 'No End Date')\n",
    "                page_data[\"event_start\"] = event_start\n",
    "                page_data[\"event_end\"] = event_end\n",
    "\n",
    "                venue_tag = event.find('a', class_='event-card__venue-tag')\n",
    "                venue = venue_tag.text.strip() if venue_tag else \"No Venue\"\n",
    "                page_data[\"venue\"] = venue\n",
    "\n",
    "                event_type_tag = event.find('a', class_='event-card__event-type')\n",
    "                event_type = event_type_tag.text.strip() if event_type_tag else \"No Event Type\"\n",
    "                page_data[\"event_type\"] = event_type\n",
    "\n",
    "                featured_flag_tag = event.find('span', class_='event-card__featured-flag')\n",
    "                featured_flag = \"Featured\" if featured_flag_tag else \"Not Featured\"\n",
    "                page_data[\"featured_flag\"] = featured_flag\n",
    "\n",
    "                all_data.append(page_data)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching {link}: {e}\")\n",
    "    \n",
    "    return all_data\n",
    "\n",
    "def save_to_json(data, filename):\n",
    "    try:\n",
    "        with open(filename, 'w', encoding='utf-8') as f:\n",
    "            json.dump(data, f, ensure_ascii=False, indent=4)\n",
    "        print(f\"Data saved to {filename}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error writing to JSON file: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    scraper_ = Scraper()\n",
    "\n",
    "    links = [\n",
    "        \"https://carnegiemuseums.org/events/\",\n",
    "        \"https://carnegiemuseums.org/events/page/2/\",\n",
    "        \"https://carnegiemuseums.org/events/page/3/\",\n",
    "        \"https://carnegiemuseums.org/events/page/4/\",\n",
    "        \"https://carnegiemuseums.org/events/page/5/\"\n",
    "    ]\n",
    "\n",
    "    scraped_data = scrape_links(scraper_, links)\n",
    "    save_to_json(scraped_data, \"../raw_documents/Pittsburgh_Carnegie_Museums.json\")\n",
    "    scraper_.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching: https://carnegieart.org/about/our-story/\n",
      "Data appended to ../raw_documents/Pittsburgh_Carnegie_Museums.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import os\n",
    "\n",
    "class Scraper:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.chrome_options = Options()\n",
    "        self.chrome_options.add_argument(\"--headless\")\n",
    "        self.chrome_options.add_argument(\"--disable-gpu\")\n",
    "        self.chrome_options.add_argument(\"--no-sandbox\")\n",
    "        self.chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "        self.driver = webdriver.Chrome(options=self.chrome_options)\n",
    "        self.driver.set_page_load_timeout(20) \n",
    "        self.current_url = None\n",
    "        self.current_domain = None\n",
    "\n",
    "    def set_domain(self, url: str):\n",
    "        \"\"\"set domain name and URL\"\"\"\n",
    "        self.current_domain = url.split(\"/\")[2]\n",
    "        self.current_url = url\n",
    "\n",
    "    def fetch(self, url: str):\n",
    "        \"\"\"capture the webpage\"\"\"\n",
    "        soup = self.get_soup(url)\n",
    "        return soup\n",
    "\n",
    "    def get_html(self, url: str):\n",
    "        \"\"\"get HTML\"\"\"\n",
    "        self.set_domain(url)\n",
    "        self.driver.get(url)\n",
    "        return self.driver.page_source\n",
    "\n",
    "    def get_soup(self, url: str):\n",
    "        \"\"\"parse and get BeautifulSoup object\"\"\"\n",
    "        html = self.get_html(url)\n",
    "        return BeautifulSoup(html, \"html.parser\")\n",
    "    \n",
    "    def remove_unnecessary_sections(self, soup: BeautifulSoup):\n",
    "        \"\"\"remove header and footer\"\"\"\n",
    "        for tag in [\"header\", \"footer\", \"nav\", \"aside\"]:\n",
    "            for element in soup.find_all(tag):\n",
    "                element.decompose()\n",
    "        return soup\n",
    "\n",
    "    def close(self):\n",
    "        self.driver.quit()\n",
    "\n",
    "    def __del__(self):\n",
    "        self.close()\n",
    "\n",
    "\n",
    "def scrape_page(scraper_, link):\n",
    "    try:\n",
    "        print(f\"Fetching: {link}\")\n",
    "        soup = scraper_.get_soup(link)\n",
    "        scraper_.remove_unnecessary_sections(soup)\n",
    "\n",
    "        page_data = []\n",
    "        sections = soup.find_all(['h2', 'p'])\n",
    "        current_title = None\n",
    "\n",
    "        for element in sections:\n",
    "            if element.name == 'h2':\n",
    "                current_title = element.text.strip()\n",
    "            elif element.name == 'p' and current_title:\n",
    "                page_data.append({\n",
    "                    \"title\": current_title,\n",
    "                    \"content\": element.text.strip()\n",
    "                })\n",
    "\n",
    "        return page_data\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching {link}: {e}\")\n",
    "        return []\n",
    "\n",
    "\n",
    "def append_to_json_file(data, filename):\n",
    "    try:\n",
    "        if os.path.exists(filename):\n",
    "            with open(filename, 'r', encoding='utf-8') as f:\n",
    "                existing_data = json.load(f)\n",
    "        else:\n",
    "            existing_data = []\n",
    "        existing_data.extend(data)\n",
    "\n",
    "        with open(filename, 'w', encoding='utf-8') as f:\n",
    "            json.dump(existing_data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "        print(f\"Data appended to {filename}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error writing to JSON file: {e}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    scraper_ = Scraper()\n",
    "\n",
    "    link = \"https://carnegieart.org/about/our-story/\"\n",
    "\n",
    "    scraped_data = scrape_page(scraper_, link)\n",
    "    append_to_json_file(scraped_data, \"../raw_documents/Pittsburgh_Carnegie_Museums.json\")\n",
    "    scraper_.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching: https://carnegieart.org/art/whats-on-view/\n",
      "Data appended to ../raw_documents/Pittsburgh_Carnegie_Museums.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "class Scraper:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.chrome_options = Options()\n",
    "        self.chrome_options.add_argument(\"--headless\")\n",
    "        self.chrome_options.add_argument(\"--disable-gpu\")\n",
    "        self.chrome_options.add_argument(\"--no-sandbox\")\n",
    "        self.chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "        self.driver = webdriver.Chrome(options=self.chrome_options)\n",
    "        self.driver.set_page_load_timeout(20)  \n",
    "        self.current_url = None\n",
    "        self.current_domain = None\n",
    "\n",
    "    def set_domain(self, url: str):\n",
    "        \"\"\"set domain name and URL\"\"\"\n",
    "        self.current_domain = url.split(\"/\")[2]\n",
    "        self.current_url = url\n",
    "\n",
    "    def fetch(self, url: str):\n",
    "        \"\"\"capture the webpage\"\"\"\n",
    "        soup = self.get_soup(url)\n",
    "        return soup\n",
    "\n",
    "    def get_html(self, url: str):\n",
    "        \"\"\"get HTML\"\"\"\n",
    "        self.set_domain(url)\n",
    "        self.driver.get(url)\n",
    "        return self.driver.page_source\n",
    "\n",
    "    def get_soup(self, url: str):\n",
    "        \"\"\"parse and get BeautifulSoup object\"\"\"\n",
    "        html = self.get_html(url)\n",
    "        return BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "    def close(self):\n",
    "        self.driver.quit()\n",
    "\n",
    "    def __del__(self):\n",
    "        self.close()\n",
    "\n",
    "def scrape_page(scraper_, link):\n",
    "    try:\n",
    "        print(f\"Fetching: {link}\")\n",
    "        soup = scraper_.get_soup(link)\n",
    "\n",
    "        page_data = []\n",
    "        events = soup.find_all('div', class_='cmoa-grid-item')\n",
    "\n",
    "        for event in events:\n",
    "            event_data = {}\n",
    "            event_name_tag = event.find('a', class_='font-bold')\n",
    "            event_name = event_name_tag.text.strip() if event_name_tag else \"No Event Name\"\n",
    "            event_data[\"event_name\"] = event_name\n",
    "\n",
    "            time_tag = event.find('div', class_='break-words')\n",
    "            event_time = time_tag.text.strip() if time_tag else \"No Time Information\"\n",
    "            event_data[\"time\"] = event_time\n",
    "\n",
    "            location_tag = event.find('ul', class_='metadata')\n",
    "            if location_tag:\n",
    "                location = location_tag.find('li').text.strip()\n",
    "            else:\n",
    "                location = \"No Location\"\n",
    "            event_data[\"location\"] = location\n",
    "\n",
    "            page_data.append(event_data)\n",
    "\n",
    "        return page_data\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching {link}: {e}\")\n",
    "        return []\n",
    "\n",
    "def append_to_json_file(data, filename):\n",
    "    try:\n",
    "        if os.path.exists(filename):\n",
    "            with open(filename, 'r', encoding='utf-8') as f:\n",
    "                existing_data = json.load(f)\n",
    "        else:\n",
    "            existing_data = []\n",
    "\n",
    "        existing_data.extend(data)\n",
    "\n",
    "        with open(filename, 'w', encoding='utf-8') as f:\n",
    "            json.dump(existing_data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "        print(f\"Data appended to {filename}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error writing to JSON file: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    scraper_ = Scraper()\n",
    "    link = \"https://carnegieart.org/art/whats-on-view/\"\n",
    "    scraped_data = scrape_page(scraper_, link)\n",
    "    append_to_json_file(scraped_data, \"../raw_documents/Pittsburgh_Carnegie_Museums.json\")\n",
    "    scraper_.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching: https://carnegieart.org/art/whats-on-view/page/2/\n",
      "Data appended to ../raw_documents/Pittsburgh_Carnegie_Museums.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "class Scraper:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.chrome_options = Options()\n",
    "        self.chrome_options.add_argument(\"--headless\")\n",
    "        self.chrome_options.add_argument(\"--disable-gpu\")\n",
    "        self.chrome_options.add_argument(\"--no-sandbox\")\n",
    "        self.chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "        self.driver = webdriver.Chrome(options=self.chrome_options)\n",
    "        self.driver.set_page_load_timeout(20)  \n",
    "        self.current_url = None\n",
    "        self.current_domain = None\n",
    "\n",
    "    def set_domain(self, url: str):\n",
    "        \"\"\"set domain name and URL\"\"\"\n",
    "        self.current_domain = url.split(\"/\")[2]\n",
    "        self.current_url = url\n",
    "\n",
    "    def fetch(self, url: str):\n",
    "        \"\"\"capture the webpage\"\"\"\n",
    "        soup = self.get_soup(url)\n",
    "        return soup\n",
    "\n",
    "    def get_html(self, url: str):\n",
    "        \"\"\"get HTML\"\"\"\n",
    "        self.set_domain(url)\n",
    "        self.driver.get(url)\n",
    "        return self.driver.page_source\n",
    "\n",
    "    def get_soup(self, url: str):\n",
    "        \"\"\"parse and get BeautifulSoup object\"\"\"\n",
    "        html = self.get_html(url)\n",
    "        return BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "    def close(self):\n",
    "        self.driver.quit()\n",
    "\n",
    "    def __del__(self):\n",
    "        self.close()\n",
    "\n",
    "def scrape_page(scraper_, link):\n",
    "    try:\n",
    "        print(f\"Fetching: {link}\")\n",
    "        soup = scraper_.get_soup(link)\n",
    "\n",
    "        page_data = []\n",
    "        events = soup.find_all('div', class_='cmoa-grid-item')\n",
    "\n",
    "        for event in events:\n",
    "            event_data = {}\n",
    "\n",
    "            event_name_tag = event.find('a', class_='font-bold')\n",
    "            event_name = event_name_tag.text.strip() if event_name_tag else \"No Event Name\"\n",
    "            event_data[\"event_name\"] = event_name\n",
    "\n",
    "            time_tag = event.find('div', class_='break-words')\n",
    "            event_time = time_tag.text.strip() if time_tag else \"No Time Information\"\n",
    "            event_data[\"time\"] = event_time\n",
    "\n",
    "            location_tag = event.find('ul', class_='metadata')\n",
    "            if location_tag:\n",
    "                location = location_tag.find('li').text.strip()\n",
    "            else:\n",
    "                location = \"No Location\"\n",
    "            event_data[\"location\"] = location\n",
    "\n",
    "            page_data.append(event_data)\n",
    "\n",
    "        return page_data\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching {link}: {e}\")\n",
    "        return []\n",
    "\n",
    "def append_to_json_file(data, filename):\n",
    "    try:\n",
    "        if os.path.exists(filename):\n",
    "            with open(filename, 'r', encoding='utf-8') as f:\n",
    "                existing_data = json.load(f)\n",
    "        else:\n",
    "            existing_data = []\n",
    "\n",
    "        existing_data.extend(data)\n",
    "\n",
    "        with open(filename, 'w', encoding='utf-8') as f:\n",
    "            json.dump(existing_data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "        print(f\"Data appended to {filename}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error writing to JSON file: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    scraper_ = Scraper()\n",
    "    link = \"https://carnegieart.org/art/whats-on-view/page/2/\"\n",
    "    scraped_data = scrape_page(scraper_, link)\n",
    "    append_to_json_file(scraped_data, \"../raw_documents/Pittsburgh_Carnegie_Museums.json\")\n",
    "    scraper_.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching: https://carnegieart.org/art/whats-on-view/page/3/\n",
      "Data appended to ../raw_documents/Pittsburgh_Carnegie_Museums.json\n",
      "Fetching: https://carnegieart.org/art/whats-on-view/page/4/\n",
      "Data appended to ../raw_documents/Pittsburgh_Carnegie_Museums.json\n",
      "Fetching: https://carnegieart.org/art/whats-on-view/page/5/\n",
      "Data appended to ../raw_documents/Pittsburgh_Carnegie_Museums.json\n",
      "Fetching: https://carnegieart.org/art/whats-on-view/page/6/\n",
      "Data appended to ../raw_documents/Pittsburgh_Carnegie_Museums.json\n",
      "Fetching: https://carnegieart.org/art/whats-on-view/page/7/\n",
      "Data appended to ../raw_documents/Pittsburgh_Carnegie_Museums.json\n",
      "Fetching: https://carnegieart.org/art/whats-on-view/page/8/\n",
      "Data appended to ../raw_documents/Pittsburgh_Carnegie_Museums.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "class Scraper:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.chrome_options = Options()\n",
    "        self.chrome_options.add_argument(\"--headless\")\n",
    "        self.chrome_options.add_argument(\"--disable-gpu\")\n",
    "        self.chrome_options.add_argument(\"--no-sandbox\")\n",
    "        self.chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "        self.driver = webdriver.Chrome(options=self.chrome_options)\n",
    "        self.driver.set_page_load_timeout(20) \n",
    "        self.current_url = None\n",
    "        self.current_domain = None\n",
    "\n",
    "    def set_domain(self, url: str):\n",
    "        \"\"\"set domain name and URL\"\"\"\n",
    "        self.current_domain = url.split(\"/\")[2]\n",
    "        self.current_url = url\n",
    "\n",
    "    def fetch(self, url: str):\n",
    "        \"\"\"capture the webpage\"\"\"\n",
    "        soup = self.get_soup(url)\n",
    "        return soup\n",
    "\n",
    "    def get_html(self, url: str):\n",
    "        \"\"\"get HTML\"\"\"\n",
    "        self.set_domain(url)\n",
    "        self.driver.get(url)\n",
    "        return self.driver.page_source\n",
    "\n",
    "    def get_soup(self, url: str):\n",
    "        \"\"\"parse and get BeautifulSoup object\"\"\"\n",
    "        html = self.get_html(url)\n",
    "        return BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "    def close(self):\n",
    "        self.driver.quit()\n",
    "\n",
    "    def __del__(self):\n",
    "        self.close()\n",
    "\n",
    "def scrape_page(scraper_, link):\n",
    "    try:\n",
    "        print(f\"Fetching: {link}\")\n",
    "        soup = scraper_.get_soup(link)\n",
    "\n",
    "        page_data = []\n",
    "        events = soup.find_all('div', class_='cmoa-grid-item')\n",
    "\n",
    "        for event in events:\n",
    "            event_data = {}\n",
    "            event_name_tag = event.find('a', class_='font-bold')\n",
    "            event_name = event_name_tag.text.strip() if event_name_tag else \"No Event Name\"\n",
    "            event_data[\"event_name\"] = event_name\n",
    "\n",
    "            time_tag = event.find('div', class_='break-words')\n",
    "            event_time = time_tag.text.strip() if time_tag else \"No Time Information\"\n",
    "            event_data[\"time\"] = event_time\n",
    "\n",
    "            location_tag = event.find('ul', class_='metadata')\n",
    "            if location_tag:\n",
    "                location = location_tag.find('li').text.strip()\n",
    "            else:\n",
    "                location = \"No Location\"\n",
    "            event_data[\"location\"] = location\n",
    "\n",
    "            page_data.append(event_data)\n",
    "\n",
    "        return page_data\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching {link}: {e}\")\n",
    "        return []\n",
    "\n",
    "def append_to_json_file(data, filename):\n",
    "    try:\n",
    "        if os.path.exists(filename):\n",
    "            with open(filename, 'r', encoding='utf-8') as f:\n",
    "                existing_data = json.load(f)\n",
    "        else:\n",
    "            existing_data = []\n",
    "        existing_data.extend(data)\n",
    "\n",
    "        with open(filename, 'w', encoding='utf-8') as f:\n",
    "            json.dump(existing_data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "        print(f\"Data appended to {filename}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error writing to JSON file: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    scraper_ = Scraper()\n",
    "\n",
    "    link = \"https://carnegieart.org/art/whats-on-view/page/3/\"\n",
    "    scraped_data = scrape_page(scraper_, link)\n",
    "    append_to_json_file(scraped_data, \"../raw_documents/Pittsburgh_Carnegie_Museums.json\")\n",
    "\n",
    "    link = \"https://carnegieart.org/art/whats-on-view/page/4/\"\n",
    "    scraped_data = scrape_page(scraper_, link)\n",
    "    append_to_json_file(scraped_data, \"../raw_documents/Pittsburgh_Carnegie_Museums.json\")\n",
    "\n",
    "    link = \"https://carnegieart.org/art/whats-on-view/page/5/\"\n",
    "    scraped_data = scrape_page(scraper_, link)\n",
    "    append_to_json_file(scraped_data, \"../raw_documents/Pittsburgh_Carnegie_Museums.json\")\n",
    "\n",
    "    link = \"https://carnegieart.org/art/whats-on-view/page/6/\"\n",
    "    scraped_data = scrape_page(scraper_, link)\n",
    "    append_to_json_file(scraped_data, \"../raw_documents/Pittsburgh_Carnegie_Museums.json\")\n",
    "\n",
    "    link = \"https://carnegieart.org/art/whats-on-view/page/7/\"\n",
    "    scraped_data = scrape_page(scraper_, link)\n",
    "    append_to_json_file(scraped_data, \"../raw_documents/Pittsburgh_Carnegie_Museums.json\")\n",
    "\n",
    "    link = \"https://carnegieart.org/art/whats-on-view/page/8/\"\n",
    "    scraped_data = scrape_page(scraper_, link)\n",
    "    append_to_json_file(scraped_data, \"../raw_documents/Pittsburgh_Carnegie_Museums.json\")\n",
    "\n",
    "    scraper_.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching: https://www.warhol.org/calendar/\n",
      "Fetching: https://www.warhol.org/calendar/?date=2024-11-01&days=30&0=#calendar-header\n",
      "Fetching: https://www.warhol.org/calendar/?date=2024-12-01&days=31&0=#calendar-header\n",
      "Fetching: https://www.warhol.org/calendar/?date=2025-01-01&days=31&0=#calendar-header\n",
      "Fetching: https://www.warhol.org/calendar/?date=2025-02-01&days=28&0=#calendar-header\n",
      "Fetching: https://www.warhol.org/calendar/?date=2025-03-01&days=31&0=#calendar-header\n",
      "Fetching: https://www.warhol.org/calendar/?date=2025-04-01&days=30&0=#calendar-header\n",
      "Fetching: https://www.warhol.org/calendar/?date=2025-05-01&days=31&0=#calendar-header\n",
      "Fetching: https://www.warhol.org/calendar/?date=2025-06-01&days=30&0=#calendar-header\n",
      "Fetching: https://www.warhol.org/calendar/?date=2025-07-01&days=31&0=#calendar-header\n",
      "Fetching: https://www.warhol.org/calendar/?date=2025-08-01&days=31&0=#calendar-header\n",
      "Fetching: https://www.warhol.org/calendar/?date=2025-09-01&days=30&0=#calendar-header\n",
      "Fetching: https://www.warhol.org/calendar/?date=2025-10-01&days=31&0=#calendar-header\n",
      "Fetching: https://www.warhol.org/calendar/?date=2025-11-01&days=30&0=#calendar-header\n",
      "Fetching: https://www.warhol.org/calendar/?date=2025-12-01&days=31&0=#calendar-header\n",
      "Data appended to ../raw_documents/Pittsburgh_Carnegie_Museums.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "\n",
    "class Scraper:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.chrome_options = Options()\n",
    "        self.chrome_options.add_argument(\"--headless\")\n",
    "        self.chrome_options.add_argument(\"--disable-gpu\")\n",
    "        self.chrome_options.add_argument(\"--no-sandbox\")\n",
    "        self.chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "        self.driver = webdriver.Chrome(options=self.chrome_options)\n",
    "        self.driver.set_page_load_timeout(30) \n",
    "        self.current_url = None\n",
    "        self.current_domain = None\n",
    "\n",
    "    def set_domain(self, url: str):\n",
    "        \"\"\"Set domain name and URL\"\"\"\n",
    "        self.current_domain = url.split(\"/\")[2]\n",
    "        self.current_url = url\n",
    "\n",
    "    def fetch(self, url: str):\n",
    "        \"\"\"Capture the webpage\"\"\"\n",
    "        soup = self.get_soup(url)\n",
    "        return soup\n",
    "\n",
    "    def get_html(self, url: str):\n",
    "        \"\"\"Get HTML content\"\"\"\n",
    "        self.set_domain(url)\n",
    "        self.driver.get(url)\n",
    "        try:\n",
    "            WebDriverWait(self.driver, 30).until(EC.presence_of_element_located((By.CSS_SELECTOR, \"div.item\")))\n",
    "        except Exception as e:\n",
    "            print(f\"Error while waiting for the page to load: {e}\")\n",
    "        return self.driver.page_source\n",
    "\n",
    "    def get_soup(self, url: str):\n",
    "        html = self.get_html(url)\n",
    "        return BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "    def close(self):\n",
    "        self.driver.quit()\n",
    "\n",
    "    def __del__(self):\n",
    "        self.close()\n",
    "\n",
    "def scrape_page(scraper_, link):\n",
    "    try:\n",
    "        print(f\"Fetching: {link}\")\n",
    "        soup = scraper_.get_soup(link)\n",
    "\n",
    "        page_data = []\n",
    "\n",
    "        events = soup.find_all('div', class_='item')\n",
    "\n",
    "        for event in events:\n",
    "            event_data = {}\n",
    "            event_name_tag = event.find('h3')\n",
    "            event_name = event_name_tag.text.strip() if event_name_tag else 'No Event Name'\n",
    "            event_data['event_name'] = event_name\n",
    "\n",
    "            time_tag = event.find('time')\n",
    "            event_time = time_tag.text.strip() if time_tag else 'No Time Information'\n",
    "            event_data['time'] = event_time\n",
    "\n",
    "            location_tag = event.find('span', class_='screen-reader-text', string='Location:')\n",
    "            if location_tag:\n",
    "                location_parts = location_tag.find_parent('p').contents\n",
    "                location = ''.join([str(part).strip() for part in location_parts if isinstance(part, str)]).replace('<br>', ', ')\n",
    "                location = location.replace('Location:', '').strip()\n",
    "            else:\n",
    "                location = 'No Location'\n",
    "            event_data['location'] = location\n",
    "\n",
    "            event_type_list = event.find_all('li')\n",
    "            event_type = ', '.join([et.text.strip() for et in event_type_list if et])\n",
    "            event_data['event_type'] = event_type\n",
    "\n",
    "            audience_tag = event.find('h4', string=lambda x: 'Audience' in x)\n",
    "            if audience_tag:\n",
    "                audience_list = audience_tag.find_next('ul').find_all('li')\n",
    "                audience = ', '.join([a.text.strip() for a in audience_list if a])\n",
    "            else:\n",
    "                audience = 'No Audience Information'\n",
    "            event_data['audience'] = audience\n",
    "\n",
    "            page_data.append(event_data)\n",
    "\n",
    "        return page_data\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching {link}: {e}\")\n",
    "        return []\n",
    "        \n",
    "def scrape_multiple_pages(scraper_, links):\n",
    "    all_data = []\n",
    "    \n",
    "    for link in links:\n",
    "        data = scrape_page(scraper_, link)\n",
    "        all_data.extend(data)\n",
    "\n",
    "    return all_data\n",
    "\n",
    "def append_to_json_file(data, filename):\n",
    "    try:\n",
    "        if os.path.exists(filename):\n",
    "            with open(filename, 'r', encoding='utf-8') as f:\n",
    "                existing_data = json.load(f)\n",
    "        else:\n",
    "            existing_data = []\n",
    "\n",
    "        existing_data.extend(data)\n",
    "\n",
    "        with open(filename, 'w', encoding='utf-8') as f:\n",
    "            json.dump(existing_data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "        print(f\"Data appended to {filename}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error writing to JSON file: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    scraper_ = Scraper()\n",
    "    links = [\n",
    "        \"https://www.warhol.org/calendar/\",\n",
    "        \"https://www.warhol.org/calendar/?date=2024-11-01&days=30&0=#calendar-header\",\n",
    "        \"https://www.warhol.org/calendar/?date=2024-12-01&days=31&0=#calendar-header\",\n",
    "        \"https://www.warhol.org/calendar/?date=2025-01-01&days=31&0=#calendar-header\",\n",
    "        \"https://www.warhol.org/calendar/?date=2025-02-01&days=28&0=#calendar-header\",\n",
    "        \"https://www.warhol.org/calendar/?date=2025-03-01&days=31&0=#calendar-header\",\n",
    "        \"https://www.warhol.org/calendar/?date=2025-04-01&days=30&0=#calendar-header\",\n",
    "        \"https://www.warhol.org/calendar/?date=2025-05-01&days=31&0=#calendar-header\",\n",
    "        \"https://www.warhol.org/calendar/?date=2025-06-01&days=30&0=#calendar-header\",\n",
    "        \"https://www.warhol.org/calendar/?date=2025-07-01&days=31&0=#calendar-header\",\n",
    "        \"https://www.warhol.org/calendar/?date=2025-08-01&days=31&0=#calendar-header\",\n",
    "        \"https://www.warhol.org/calendar/?date=2025-09-01&days=30&0=#calendar-header\",\n",
    "        \"https://www.warhol.org/calendar/?date=2025-10-01&days=31&0=#calendar-header\",\n",
    "        \"https://www.warhol.org/calendar/?date=2025-11-01&days=30&0=#calendar-header\",\n",
    "        \"https://www.warhol.org/calendar/?date=2025-12-01&days=31&0=#calendar-header\"\n",
    "    ]\n",
    "\n",
    "    scraped_data = scrape_multiple_pages(scraper_, links)\n",
    "    append_to_json_file(scraped_data, \"../raw_documents/Pittsburgh_Carnegie_Museums.json\")\n",
    "    scraper_.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Heinz History Center\n",
    "### https://www.heinzhistorycenter.org/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching: https://www.heinzhistorycenter.org/events/https://www.heinzhistorycenter.org/\n",
      "Fetching: https://www.heinzhistorycenter.org/research/detre-library-archives/\n",
      "Fetching: https://www.heinzhistorycenter.org/whats-on/exhibits/\n",
      "Data saved to ../raw_documents/Heinz_History_Center.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import os\n",
    "\n",
    "class Scraper:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.chrome_options = Options()\n",
    "        self.chrome_options.add_argument(\"--headless\")\n",
    "        self.chrome_options.add_argument(\"--disable-gpu\")\n",
    "        self.chrome_options.add_argument(\"--no-sandbox\")\n",
    "        self.chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "        self.driver = webdriver.Chrome(options=self.chrome_options)\n",
    "        self.driver.set_page_load_timeout(10)\n",
    "        self.current_url = None\n",
    "        self.current_domain = None\n",
    "\n",
    "    def set_domain(self, url: str):\n",
    "        \"\"\"Set domain name and URL.\"\"\"\n",
    "        self.current_domain = url.split(\"/\")[2]  \n",
    "        self.current_url = url\n",
    "\n",
    "    def fetch(self, url: str, raw_html: bool = False):\n",
    "        \"\"\"Capture the webpage and choose whether to remove JS or CSS.\"\"\"\n",
    "        soup = self.get_soup(url)\n",
    "        if not raw_html:\n",
    "            soup = self.remove_js_css(soup)\n",
    "        return soup, self.get_links(soup), self.get_title(soup)\n",
    "\n",
    "    def get_html(self, url: str):\n",
    "        \"\"\"Get HTML.\"\"\"\n",
    "        self.set_domain(url)\n",
    "        self.driver.get(url)\n",
    "        return self.driver.page_source\n",
    "\n",
    "    def get_soup(self, url: str):\n",
    "        \"\"\"Parse and get BeautifulSoup object.\"\"\"\n",
    "        html = self.get_html(url)\n",
    "        return BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "    def get_links(self, soup: BeautifulSoup):\n",
    "        \"\"\"Get external links.\"\"\"\n",
    "        links = [link.get(\"href\") for link in soup.find_all(\"a\")]\n",
    "        return self.filter_links(links)\n",
    "\n",
    "    def remove_js_css(self, soup: BeautifulSoup):\n",
    "        \"\"\"Remove JS and CSS.\"\"\"\n",
    "        for script in soup([\"script\", \"style\"]):\n",
    "            script.extract() \n",
    "        return soup\n",
    "\n",
    "    def remove_unnecessary_sections(self, soup: BeautifulSoup):\n",
    "        \"\"\"Remove unnecessary sections such as header, footer, nav, etc.\"\"\"\n",
    "        for tag in [\"header\", \"footer\", \"nav\", \"aside\"]:\n",
    "            for element in soup.find_all(tag):\n",
    "                element.decompose()\n",
    "        return soup\n",
    "\n",
    "    def filter_links(self, links: list):\n",
    "        filtered_links = []\n",
    "        for link in links:\n",
    "            if link is None:\n",
    "                continue\n",
    "            elif link.startswith(\"#\"):\n",
    "                continue\n",
    "            elif link.startswith(\"http\"):\n",
    "                filtered_links.append(link)\n",
    "            elif link.startswith(\"//\"):\n",
    "                filtered_links.append(f\"https:{link}\")\n",
    "            elif link.startswith(\"/\"):\n",
    "                filtered_links.append(f\"https://{self.current_domain}{link}\")\n",
    "            else:\n",
    "                filtered_links.append(f\"{self.current_url}/{link}\")\n",
    "        return filtered_links\n",
    "\n",
    "    def close(self):\n",
    "        self.driver.quit()\n",
    "\n",
    "    def __del__(self):\n",
    "        self.close()\n",
    "\n",
    "\n",
    "def extract_event_data(soup: BeautifulSoup):\n",
    "    \"\"\"Extract event data from the soup.\"\"\"\n",
    "    events = []\n",
    "    event_cards = soup.find_all(\"div\", class_=\"card_body\")\n",
    "    for card in event_cards:\n",
    "        event = {}\n",
    "        event_name_tag = card.find(\"h3\", class_=\"card_title\")\n",
    "        event['event_name'] = event_name_tag.get_text(strip=True) if event_name_tag else \"No Event Name\"\n",
    "\n",
    "        time_tag = card.find(\"span\", class_=\"card_time\")\n",
    "        event['time'] = time_tag.get_text(strip=True) if time_tag else \"No Time Information\"\n",
    "\n",
    "        location_tag = card.find(\"span\", class_=\"card_location\")\n",
    "        event['location'] = location_tag.get_text(strip=True) if location_tag else \"No Location\"\n",
    "\n",
    "        description_tag = card.find(\"div\", class_=\"card_description\")\n",
    "        event['description'] = description_tag.get_text(strip=True) if description_tag else \"No Description\"\n",
    "\n",
    "        events.append(event)\n",
    "\n",
    "    return events\n",
    "\n",
    "\n",
    "def save_to_json(data, filename):\n",
    "    \"\"\"Save data to a JSON file, ensuring no overwrite of existing content.\"\"\"\n",
    "    try:\n",
    "        if os.path.exists(filename):\n",
    "            with open(filename, 'r', encoding='utf-8') as f:\n",
    "                existing_data = json.load(f)\n",
    "\n",
    "            if isinstance(existing_data, list):\n",
    "                existing_data.extend(data) \n",
    "            else:\n",
    "                existing_data.update(data)  \n",
    "        else:\n",
    "            existing_data = data \n",
    "\n",
    "        with open(filename, 'w', encoding='utf-8') as f:\n",
    "            json.dump(existing_data, f, ensure_ascii=False, indent=4)\n",
    "        print(f\"Data saved to {filename}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error writing to JSON file: {e}\")\n",
    "\n",
    "\n",
    "def scrape_links(scraper_, links):\n",
    "    all_data = []\n",
    "    for link in links:\n",
    "        try:\n",
    "            print(f\"Fetching: {link}\")\n",
    "            soup = scraper_.get_soup(link)\n",
    "\n",
    "            soup = scraper_.remove_js_css(soup)\n",
    "            soup = scraper_.remove_unnecessary_sections(soup)\n",
    "\n",
    "            page_data = extract_event_data(soup)\n",
    "            all_data.extend(page_data)\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching {link}: {e}\")\n",
    "\n",
    "    return all_data\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    scraper_ = Scraper()\n",
    "\n",
    "    links = [\n",
    "        \"https://www.heinzhistorycenter.org/events/\"\n",
    "        \"https://www.heinzhistorycenter.org/\",\n",
    "        \"https://www.heinzhistorycenter.org/research/detre-library-archives/\",\n",
    "        \"https://www.heinzhistorycenter.org/whats-on/exhibits/\",\n",
    "\n",
    "    ]\n",
    "\n",
    "    scraped_data = scrape_links(scraper_, links)\n",
    "    save_to_json(scraped_data, \"../raw_documents/Heinz_History_Center.json\")\n",
    "    scraper_.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching: https://www.heinzhistorycenter.org/whats-on/sports-museum/exhibits/\n",
      "Fetching: https://www.heinzhistorycenter.org/whats-on/fort-pitt/\n",
      "Fetching: https://www.heinzhistorycenter.org/whats-on/meadowcroft/exhibits/\n",
      "Fetching: https://www.heinzhistorycenter.org/whats-on/exhibits/past-exhibits/\n",
      "Data saved to ../raw_documents/Heinz_History_Center.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import os\n",
    "\n",
    "class Scraper:\n",
    "    def __init__(self):\n",
    "        self.chrome_options = Options()\n",
    "        self.chrome_options.add_argument(\"--headless\")\n",
    "        self.chrome_options.add_argument(\"--disable-gpu\")\n",
    "        self.chrome_options.add_argument(\"--no-sandbox\")\n",
    "        self.chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "        self.driver = webdriver.Chrome(options=self.chrome_options)\n",
    "        self.driver.set_page_load_timeout(10)\n",
    "\n",
    "    def get_html(self, url: str):\n",
    "        \"\"\"Get HTML.\"\"\"\n",
    "        self.driver.get(url)\n",
    "        return self.driver.page_source\n",
    "\n",
    "    def get_soup(self, url: str):\n",
    "        \"\"\"Parse and return BeautifulSoup object.\"\"\"\n",
    "        html = self.get_html(url)\n",
    "        return BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "    def close(self):\n",
    "        self.driver.quit()\n",
    "\n",
    "    def __del__(self):\n",
    "        self.close()\n",
    "\n",
    "def organize_content_by_heading(soup: BeautifulSoup):\n",
    "    \"\"\"Organize content based on headings, ensuring all relevant text is captured.\"\"\"\n",
    "    data = {}\n",
    "    current_h1, current_h2, current_h3, current_h4, current_h5, current_h6 = None, None, None, None, None, None\n",
    "\n",
    "    for tag in soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'p', 'div', 'span']):\n",
    "        if tag.name == 'h1':\n",
    "            current_h1 = tag.get_text(strip=True)\n",
    "            data[current_h1] = {'content': \"\"}\n",
    "            current_h2 = current_h3 = current_h4 = current_h5 = current_h6 = None  # Reset lower headings\n",
    "        elif tag.name == 'h2' and current_h1:\n",
    "            current_h2 = tag.get_text(strip=True)\n",
    "            data[current_h1][current_h2] = {'content': \"\"}\n",
    "            current_h3 = current_h4 = current_h5 = current_h6 = None\n",
    "        elif tag.name == 'h3' and current_h2:\n",
    "            current_h3 = tag.get_text(strip=True)\n",
    "            data[current_h1][current_h2][current_h3] = {'content': \"\"}\n",
    "            current_h4 = current_h5 = current_h6 = None\n",
    "        elif tag.name == 'h4' and current_h3:\n",
    "            current_h4 = tag.get_text(strip=True)\n",
    "            data[current_h1][current_h2][current_h3][current_h4] = {'content': \"\"}\n",
    "            current_h5 = current_h6 = None\n",
    "        elif tag.name == 'h5' and current_h4:\n",
    "            current_h5 = tag.get_text(strip=True)\n",
    "            data[current_h1][current_h2][current_h3][current_h4][current_h5] = {'content': \"\"}\n",
    "            current_h6 = None\n",
    "        elif tag.name == 'h6' and current_h5:\n",
    "            current_h6 = tag.get_text(strip=True)\n",
    "            data[current_h1][current_h2][current_h3][current_h4][current_h5][current_h6] = {'content': \"\"}\n",
    "        else:\n",
    "            content = tag.get_text(strip=True)\n",
    "            if content:\n",
    "                if current_h6:\n",
    "                    data[current_h1][current_h2][current_h3][current_h4][current_h5][current_h6]['content'] += content + \" \"\n",
    "                elif current_h5:\n",
    "                    data[current_h1][current_h2][current_h3][current_h4][current_h5]['content'] += content + \" \"\n",
    "                elif current_h4:\n",
    "                    data[current_h1][current_h2][current_h3][current_h4]['content'] += content + \" \"\n",
    "                elif current_h3:\n",
    "                    data[current_h1][current_h2][current_h3]['content'] += content + \" \"\n",
    "                elif current_h2:\n",
    "                    data[current_h1][current_h2]['content'] += content + \" \"\n",
    "                elif current_h1:\n",
    "                    data[current_h1]['content'] += content + \" \"\n",
    "\n",
    "    return data\n",
    "\n",
    "def save_to_json(data, filename):\n",
    "    \"\"\"Save data to a JSON file, ensuring no overwrite of existing content.\"\"\"\n",
    "    try:\n",
    "        if os.path.exists(filename):\n",
    "            with open(filename, 'r', encoding='utf-8') as f:\n",
    "                existing_data = json.load(f)\n",
    "            if isinstance(existing_data, list):\n",
    "                existing_data.extend(data)\n",
    "            else:\n",
    "                existing_data.update(data) \n",
    "        else:\n",
    "            existing_data = data\n",
    "\n",
    "        with open(filename, 'w', encoding='utf-8') as f:\n",
    "            json.dump(existing_data, f, ensure_ascii=False, indent=4)\n",
    "        print(f\"Data saved to {filename}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error writing to JSON file: {e}\")\n",
    "\n",
    "def scrape_links(scraper_, links):\n",
    "    \"\"\"Scrape multiple links and return organized data.\"\"\"\n",
    "    all_data = []\n",
    "    for link in links:\n",
    "        try:\n",
    "            print(f\"Fetching: {link}\")\n",
    "            soup = scraper_.get_soup(link)\n",
    "            page_data = organize_content_by_heading(soup)\n",
    "            page_data['url'] = link  \n",
    "            all_data.append(page_data)\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching {link}: {e}\")\n",
    "\n",
    "    return all_data\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    scraper_ = Scraper()\n",
    "\n",
    "    links = [\n",
    "        \"https://www.heinzhistorycenter.org/whats-on/sports-museum/exhibits/\",\n",
    "        \"https://www.heinzhistorycenter.org/whats-on/fort-pitt/\",\n",
    "        \"https://www.heinzhistorycenter.org/whats-on/meadowcroft/exhibits/\",\n",
    "        \"https://www.heinzhistorycenter.org/whats-on/exhibits/past-exhibits/\"\n",
    "    ]\n",
    "\n",
    "    scraped_data = scrape_links(scraper_, links)\n",
    "    save_to_json(scraped_data, \"../raw_documents/Heinz_History_Center.json\")\n",
    "    scraper_.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Frick\n",
    "### https://www.thefrickpittsburgh.org/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching: https://www.thefrickpittsburgh.org/calendar?search=1&page=1&search_date_from=10%2F01%2F2024&search_date_to=3%2F31%2F2025\n",
      "Fetching: https://www.thefrickpittsburgh.org/calendar?search=1&page=2&search_date_from=10%2F01%2F2024&search_date_to=3%2F31%2F2025\n",
      "Fetching: https://www.thefrickpittsburgh.org/stories\n",
      "Fetching: https://www.thefrickpittsburgh.org/exhibitions\n",
      "Fetching: https://www.thefrickpittsburgh.org/plan-your-visit\n",
      "Fetching: https://www.thefrickpittsburgh.org/mission\n",
      "Fetching: https://www.thefrickpittsburgh.org/collection\n",
      "Fetching: https://collection.thefrickpittsburgh.org/objects?query=object_type%3A%22Carriage%22+OR+object_type%3A%22Cars+and+Carriages%22&limit=40\n",
      "Fetching: https://collection.thefrickpittsburgh.org/objects?query=object_type%3A%22Costume%22&limit=40\n",
      "Fetching: https://collection.thefrickpittsburgh.org/objects?query=object_type%3A%22Decorative+Arts%22&limit=40\n",
      "Fetching: https://collection.thefrickpittsburgh.org/objects?query=object_type%3A%22Painting%22&limit=40\n",
      "Fetching: https://collection.thefrickpittsburgh.org/objects?query=object_type%3A%22Photography%22&limit=40\n",
      "Fetching: https://collection.thefrickpittsburgh.org/objects?query=object_type%3A%22Works+on+Paper%2FDrawing%22+OR+object_type%3A%22Works+on+Paper%2FPrint%22&limit=40\n",
      "Data saved to ../raw_documents/Pittsburgh_Frick.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import os\n",
    "\n",
    "class Scraper:\n",
    "    def __init__(self):\n",
    "        self.chrome_options = Options()\n",
    "        self.chrome_options.add_argument(\"--headless\")\n",
    "        self.chrome_options.add_argument(\"--disable-gpu\")\n",
    "        self.chrome_options.add_argument(\"--no-sandbox\")\n",
    "        self.chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "        self.driver = webdriver.Chrome(options=self.chrome_options)\n",
    "        self.driver.set_page_load_timeout(10)\n",
    "\n",
    "    def get_html(self, url: str):\n",
    "        \"\"\"Get HTML.\"\"\"\n",
    "        self.driver.get(url)\n",
    "        return self.driver.page_source\n",
    "\n",
    "    def get_soup(self, url: str):\n",
    "        \"\"\"Parse and return BeautifulSoup object.\"\"\"\n",
    "        html = self.get_html(url)\n",
    "        return BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "    def close(self):\n",
    "        self.driver.quit()\n",
    "\n",
    "    def __del__(self):\n",
    "        self.close()\n",
    "\n",
    "def organize_content_by_heading(soup: BeautifulSoup):\n",
    "    \"\"\"Organize content based on headings, ensuring all relevant text is captured.\"\"\"\n",
    "    data = {}\n",
    "    current_h1, current_h2, current_h3, current_h4, current_h5, current_h6 = None, None, None, None, None, None\n",
    "\n",
    "    for tag in soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'p', 'div', 'span']):\n",
    "        if tag.name == 'h1':\n",
    "            current_h1 = tag.get_text(strip=True)\n",
    "            data[current_h1] = {'content': \"\"}\n",
    "            current_h2 = current_h3 = current_h4 = current_h5 = current_h6 = None \n",
    "        elif tag.name == 'h2' and current_h1:\n",
    "            current_h2 = tag.get_text(strip=True)\n",
    "            data[current_h1][current_h2] = {'content': \"\"}\n",
    "            current_h3 = current_h4 = current_h5 = current_h6 = None\n",
    "        elif tag.name == 'h3' and current_h2:\n",
    "            current_h3 = tag.get_text(strip=True)\n",
    "            data[current_h1][current_h2][current_h3] = {'content': \"\"}\n",
    "            current_h4 = current_h5 = current_h6 = None\n",
    "        elif tag.name == 'h4' and current_h3:\n",
    "            current_h4 = tag.get_text(strip=True)\n",
    "            data[current_h1][current_h2][current_h3][current_h4] = {'content': \"\"}\n",
    "            current_h5 = current_h6 = None\n",
    "        elif tag.name == 'h5' and current_h4:\n",
    "            current_h5 = tag.get_text(strip=True)\n",
    "            data[current_h1][current_h2][current_h3][current_h4][current_h5] = {'content': \"\"}\n",
    "            current_h6 = None\n",
    "        elif tag.name == 'h6' and current_h5:\n",
    "            current_h6 = tag.get_text(strip=True)\n",
    "            data[current_h1][current_h2][current_h3][current_h4][current_h5][current_h6] = {'content': \"\"}\n",
    "        else:\n",
    "            content = tag.get_text(strip=True)\n",
    "            if content:\n",
    "                if current_h6:\n",
    "                    data[current_h1][current_h2][current_h3][current_h4][current_h5][current_h6]['content'] += content + \" \"\n",
    "                elif current_h5:\n",
    "                    data[current_h1][current_h2][current_h3][current_h4][current_h5]['content'] += content + \" \"\n",
    "                elif current_h4:\n",
    "                    data[current_h1][current_h2][current_h3][current_h4]['content'] += content + \" \"\n",
    "                elif current_h3:\n",
    "                    data[current_h1][current_h2][current_h3]['content'] += content + \" \"\n",
    "                elif current_h2:\n",
    "                    data[current_h1][current_h2]['content'] += content + \" \"\n",
    "                elif current_h1:\n",
    "                    data[current_h1]['content'] += content + \" \"\n",
    "\n",
    "    return data\n",
    "\n",
    "def save_to_json(data, filename):\n",
    "    \"\"\"Save data to a JSON file, ensuring no overwrite of existing content.\"\"\"\n",
    "    try:\n",
    "        if os.path.exists(filename):\n",
    "            with open(filename, 'r', encoding='utf-8') as f:\n",
    "                existing_data = json.load(f)\n",
    "            if isinstance(existing_data, list):\n",
    "                existing_data.extend(data) \n",
    "            else:\n",
    "                existing_data.update(data) \n",
    "        else:\n",
    "            existing_data = data\n",
    "\n",
    "        with open(filename, 'w', encoding='utf-8') as f:\n",
    "            json.dump(existing_data, f, ensure_ascii=False, indent=4)\n",
    "        print(f\"Data saved to {filename}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error writing to JSON file: {e}\")\n",
    "\n",
    "def scrape_links(scraper_, links):\n",
    "    \"\"\"Scrape multiple links and return organized data.\"\"\"\n",
    "    all_data = []\n",
    "    for link in links:\n",
    "        try:\n",
    "            print(f\"Fetching: {link}\")\n",
    "            soup = scraper_.get_soup(link)\n",
    "            page_data = organize_content_by_heading(soup)\n",
    "            page_data['url'] = link  \n",
    "            all_data.append(page_data)\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching {link}: {e}\")\n",
    "\n",
    "    return all_data\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    scraper_ = Scraper()\n",
    "\n",
    "    links = [\n",
    "        \"https://www.thefrickpittsburgh.org/calendar?search=1&page=1&search_date_from=10%2F01%2F2024&search_date_to=3%2F31%2F2025\",\n",
    "        \"https://www.thefrickpittsburgh.org/calendar?search=1&page=2&search_date_from=10%2F01%2F2024&search_date_to=3%2F31%2F2025\",\n",
    "        \"https://www.thefrickpittsburgh.org/stories\",\n",
    "        \"https://www.thefrickpittsburgh.org/exhibitions\",\n",
    "        \"https://www.thefrickpittsburgh.org/plan-your-visit\",\n",
    "        \"https://www.thefrickpittsburgh.org/mission\",\n",
    "        \"https://www.thefrickpittsburgh.org/collection\",\n",
    "        \"https://collection.thefrickpittsburgh.org/objects?query=object_type%3A%22Carriage%22+OR+object_type%3A%22Cars+and+Carriages%22&limit=40\",\n",
    "        \"https://collection.thefrickpittsburgh.org/objects?query=object_type%3A%22Costume%22&limit=40\",\n",
    "        \"https://collection.thefrickpittsburgh.org/objects?query=object_type%3A%22Decorative+Arts%22&limit=40\",\n",
    "        \"https://collection.thefrickpittsburgh.org/objects?query=object_type%3A%22Painting%22&limit=40\",\n",
    "        \"https://collection.thefrickpittsburgh.org/objects?query=object_type%3A%22Photography%22&limit=40\",\n",
    "        \"https://collection.thefrickpittsburgh.org/objects?query=object_type%3A%22Works+on+Paper%2FDrawing%22+OR+object_type%3A%22Works+on+Paper%2FPrint%22&limit=40\"\n",
    "    ]\n",
    "    scraped_data = scrape_links(scraper_, links)\n",
    "    save_to_json(scraped_data, \"../raw_documents/Pittsburgh_Frick.json\")\n",
    "    scraper_.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Food\n",
    "## Food Festivals\n",
    "### https://www.visitpittsburgh.com/events-festivals/food-festivals/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching: https://www.visitpittsburgh.com/events-festivals/food-festivals/\n",
      "Fetching: https://www.visitpittsburgh.com/blog/\n",
      "Fetching: https://www.visitpittsburgh.com/plan-your-trip/\n",
      "Fetching: https://www.visitpittsburgh.com/events-festivals/\n",
      "Fetching: https://www.visitpittsburgh.com/events-festivals/?page=2\n",
      "Fetching: https://www.visitpittsburgh.com/events-festivals/?page=3\n",
      "Fetching: https://www.visitpittsburgh.com/events-festivals/?page=4\n",
      "Fetching: https://www.visitpittsburgh.com/events-festivals/?page=5\n",
      "Fetching: https://www.visitpittsburgh.com/events-festivals/?page=6\n",
      "Fetching: https://www.visitpittsburgh.com/events-festivals/?page=7\n",
      "Fetching: https://www.visitpittsburgh.com/events-festivals/?page=8\n",
      "Fetching: https://www.visitpittsburgh.com/events-festivals/?page=9\n",
      "Fetching: https://www.visitpittsburgh.com/events-festivals/?page=10\n",
      "Fetching: https://www.visitpittsburgh.com/events-festivals/?page=11\n",
      "Fetching: https://www.visitpittsburgh.com/events-festivals/?page=12\n",
      "Fetching: https://www.visitpittsburgh.com/events-festivals/?page=13\n",
      "Error fetching https://www.visitpittsburgh.com/events-festivals/?page=13: Message: timeout: Timed out receiving message from renderer: 8.279\n",
      "  (Session info: chrome=129.0.6668.100)\n",
      "Stacktrace:\n",
      "#0 0x55d1201d40ca <unknown>\n",
      "#1 0x55d11fed2a80 <unknown>\n",
      "#2 0x55d11febb073 <unknown>\n",
      "#3 0x55d11febad91 <unknown>\n",
      "#4 0x55d11feb8e58 <unknown>\n",
      "#5 0x55d11feb963f <unknown>\n",
      "#6 0x55d11fec8b57 <unknown>\n",
      "#7 0x55d11fedecdd <unknown>\n",
      "#8 0x55d11fee408b <unknown>\n",
      "#9 0x55d11feb9cf7 <unknown>\n",
      "#10 0x55d11fedeb4c <unknown>\n",
      "#11 0x55d11ff64f1e <unknown>\n",
      "#12 0x55d11ff45933 <unknown>\n",
      "#13 0x55d11ff13e3d <unknown>\n",
      "#14 0x55d11ff1492e <unknown>\n",
      "#15 0x55d12019fc8f <unknown>\n",
      "#16 0x55d1201a3d50 <unknown>\n",
      "#17 0x55d12018d837 <unknown>\n",
      "#18 0x55d1201a44c1 <unknown>\n",
      "#19 0x55d120174d1e <unknown>\n",
      "#20 0x55d1201c32b8 <unknown>\n",
      "#21 0x55d1201c34ba <unknown>\n",
      "#22 0x55d1201d2d5d <unknown>\n",
      "#23 0x7fcd4b8c4ac3 <unknown>\n",
      "\n",
      "Fetching: https://www.visitpittsburgh.com/events-festivals/?page=14\n",
      "Error fetching https://www.visitpittsburgh.com/events-festivals/?page=14: Message: timeout: Timed out receiving message from renderer: 10.000\n",
      "  (Session info: chrome=129.0.6668.100)\n",
      "Stacktrace:\n",
      "#0 0x55d1201d40ca <unknown>\n",
      "#1 0x55d11fed2a80 <unknown>\n",
      "#2 0x55d11febb073 <unknown>\n",
      "#3 0x55d11febad91 <unknown>\n",
      "#4 0x55d11feb8e58 <unknown>\n",
      "#5 0x55d11feb963f <unknown>\n",
      "#6 0x55d11fec8b57 <unknown>\n",
      "#7 0x55d11fedecdd <unknown>\n",
      "#8 0x55d11fee408b <unknown>\n",
      "#9 0x55d11feb9cf7 <unknown>\n",
      "#10 0x55d11fedeb4c <unknown>\n",
      "#11 0x55d11ff64a82 <unknown>\n",
      "#12 0x55d11ff45933 <unknown>\n",
      "#13 0x55d11ff13e3d <unknown>\n",
      "#14 0x55d11ff1492e <unknown>\n",
      "#15 0x55d12019fc8f <unknown>\n",
      "#16 0x55d1201a3d50 <unknown>\n",
      "#17 0x55d12018d837 <unknown>\n",
      "#18 0x55d1201a44c1 <unknown>\n",
      "#19 0x55d120174d1e <unknown>\n",
      "#20 0x55d1201c32b8 <unknown>\n",
      "#21 0x55d1201c34ba <unknown>\n",
      "#22 0x55d1201d2d5d <unknown>\n",
      "#23 0x7fcd4b8c4ac3 <unknown>\n",
      "\n",
      "Fetching: https://www.visitpittsburgh.com/events-festivals/?page=15\n",
      "Error fetching https://www.visitpittsburgh.com/events-festivals/?page=15: Message: timeout: Timed out receiving message from renderer: 10.000\n",
      "  (Session info: chrome=129.0.6668.100)\n",
      "Stacktrace:\n",
      "#0 0x55d1201d40ca <unknown>\n",
      "#1 0x55d11fed2a80 <unknown>\n",
      "#2 0x55d11febb073 <unknown>\n",
      "#3 0x55d11febad91 <unknown>\n",
      "#4 0x55d11feb8e58 <unknown>\n",
      "#5 0x55d11feb963f <unknown>\n",
      "#6 0x55d11fec8b57 <unknown>\n",
      "#7 0x55d11fedecdd <unknown>\n",
      "#8 0x55d11fee408b <unknown>\n",
      "#9 0x55d11feb9cf7 <unknown>\n",
      "#10 0x55d11fedeb4c <unknown>\n",
      "#11 0x55d11ff64a82 <unknown>\n",
      "#12 0x55d11ff45933 <unknown>\n",
      "#13 0x55d11ff13e3d <unknown>\n",
      "#14 0x55d11ff1492e <unknown>\n",
      "#15 0x55d12019fc8f <unknown>\n",
      "#16 0x55d1201a3d50 <unknown>\n",
      "#17 0x55d12018d837 <unknown>\n",
      "#18 0x55d1201a44c1 <unknown>\n",
      "#19 0x55d120174d1e <unknown>\n",
      "#20 0x55d1201c32b8 <unknown>\n",
      "#21 0x55d1201c34ba <unknown>\n",
      "#22 0x55d1201d2d5d <unknown>\n",
      "#23 0x7fcd4b8c4ac3 <unknown>\n",
      "\n",
      "Fetching: https://www.visitpittsburgh.com/events-festivals/?page=16\n",
      "Error fetching https://www.visitpittsburgh.com/events-festivals/?page=16: Message: timeout: Timed out receiving message from renderer: 10.000\n",
      "  (Session info: chrome=129.0.6668.100)\n",
      "Stacktrace:\n",
      "#0 0x55d1201d40ca <unknown>\n",
      "#1 0x55d11fed2a80 <unknown>\n",
      "#2 0x55d11febb073 <unknown>\n",
      "#3 0x55d11febad91 <unknown>\n",
      "#4 0x55d11feb8e58 <unknown>\n",
      "#5 0x55d11feb963f <unknown>\n",
      "#6 0x55d11fec8b57 <unknown>\n",
      "#7 0x55d11fedecdd <unknown>\n",
      "#8 0x55d11fee408b <unknown>\n",
      "#9 0x55d11feb9cf7 <unknown>\n",
      "#10 0x55d11fedeb4c <unknown>\n",
      "#11 0x55d11ff64a82 <unknown>\n",
      "#12 0x55d11ff45933 <unknown>\n",
      "#13 0x55d11ff13e3d <unknown>\n",
      "#14 0x55d11ff1492e <unknown>\n",
      "#15 0x55d12019fc8f <unknown>\n",
      "#16 0x55d1201a3d50 <unknown>\n",
      "#17 0x55d12018d837 <unknown>\n",
      "#18 0x55d1201a44c1 <unknown>\n",
      "#19 0x55d120174d1e <unknown>\n",
      "#20 0x55d1201c32b8 <unknown>\n",
      "#21 0x55d1201c34ba <unknown>\n",
      "#22 0x55d1201d2d5d <unknown>\n",
      "#23 0x7fcd4b8c4ac3 <unknown>\n",
      "\n",
      "Fetching: https://www.visitpittsburgh.com/things-to-do/\n",
      "Error fetching https://www.visitpittsburgh.com/things-to-do/: Message: timeout: Timed out receiving message from renderer: 10.000\n",
      "  (Session info: chrome=129.0.6668.100)\n",
      "Stacktrace:\n",
      "#0 0x55d1201d40ca <unknown>\n",
      "#1 0x55d11fed2a80 <unknown>\n",
      "#2 0x55d11febb073 <unknown>\n",
      "#3 0x55d11febad91 <unknown>\n",
      "#4 0x55d11feb8e58 <unknown>\n",
      "#5 0x55d11feb963f <unknown>\n",
      "#6 0x55d11fec8b57 <unknown>\n",
      "#7 0x55d11fedecdd <unknown>\n",
      "#8 0x55d11fee408b <unknown>\n",
      "#9 0x55d11feb9cf7 <unknown>\n",
      "#10 0x55d11fedeb4c <unknown>\n",
      "#11 0x55d11ff64a82 <unknown>\n",
      "#12 0x55d11ff45933 <unknown>\n",
      "#13 0x55d11ff13e3d <unknown>\n",
      "#14 0x55d11ff1492e <unknown>\n",
      "#15 0x55d12019fc8f <unknown>\n",
      "#16 0x55d1201a3d50 <unknown>\n",
      "#17 0x55d12018d837 <unknown>\n",
      "#18 0x55d1201a44c1 <unknown>\n",
      "#19 0x55d120174d1e <unknown>\n",
      "#20 0x55d1201c32b8 <unknown>\n",
      "#21 0x55d1201c34ba <unknown>\n",
      "#22 0x55d1201d2d5d <unknown>\n",
      "#23 0x7fcd4b8c4ac3 <unknown>\n",
      "\n",
      "Fetching: https://www.visitpittsburgh.com/restaurants-culinary/\n",
      "Error fetching https://www.visitpittsburgh.com/restaurants-culinary/: Message: timeout: Timed out receiving message from renderer: 10.000\n",
      "  (Session info: chrome=129.0.6668.100)\n",
      "Stacktrace:\n",
      "#0 0x55d1201d40ca <unknown>\n",
      "#1 0x55d11fed2a80 <unknown>\n",
      "#2 0x55d11febb073 <unknown>\n",
      "#3 0x55d11febad91 <unknown>\n",
      "#4 0x55d11feb8e58 <unknown>\n",
      "#5 0x55d11feb963f <unknown>\n",
      "#6 0x55d11fec8b57 <unknown>\n",
      "#7 0x55d11fedecdd <unknown>\n",
      "#8 0x55d11fee408b <unknown>\n",
      "#9 0x55d11feb9cf7 <unknown>\n",
      "#10 0x55d11fedeb4c <unknown>\n",
      "#11 0x55d11ff64a82 <unknown>\n",
      "#12 0x55d11ff45933 <unknown>\n",
      "#13 0x55d11ff13e3d <unknown>\n",
      "#14 0x55d11ff1492e <unknown>\n",
      "#15 0x55d12019fc8f <unknown>\n",
      "#16 0x55d1201a3d50 <unknown>\n",
      "#17 0x55d12018d837 <unknown>\n",
      "#18 0x55d1201a44c1 <unknown>\n",
      "#19 0x55d120174d1e <unknown>\n",
      "#20 0x55d1201c32b8 <unknown>\n",
      "#21 0x55d1201c34ba <unknown>\n",
      "#22 0x55d1201d2d5d <unknown>\n",
      "#23 0x7fcd4b8c4ac3 <unknown>\n",
      "\n",
      "Fetching: https://www.visitpittsburgh.com/restaurants-culinary/craft-breweries/\n",
      "Error fetching https://www.visitpittsburgh.com/restaurants-culinary/craft-breweries/: Message: timeout: Timed out receiving message from renderer: 10.000\n",
      "  (Session info: chrome=129.0.6668.100)\n",
      "Stacktrace:\n",
      "#0 0x55d1201d40ca <unknown>\n",
      "#1 0x55d11fed2a80 <unknown>\n",
      "#2 0x55d11febb073 <unknown>\n",
      "#3 0x55d11febad91 <unknown>\n",
      "#4 0x55d11feb8e58 <unknown>\n",
      "#5 0x55d11feb963f <unknown>\n",
      "#6 0x55d11fec8b57 <unknown>\n",
      "#7 0x55d11fedecdd <unknown>\n",
      "#8 0x55d11fee408b <unknown>\n",
      "#9 0x55d11feb9cf7 <unknown>\n",
      "#10 0x55d11fedeb4c <unknown>\n",
      "#11 0x55d11ff64a82 <unknown>\n",
      "#12 0x55d11ff45933 <unknown>\n",
      "#13 0x55d11ff13e3d <unknown>\n",
      "#14 0x55d11ff1492e <unknown>\n",
      "#15 0x55d12019fc8f <unknown>\n",
      "#16 0x55d1201a3d50 <unknown>\n",
      "#17 0x55d12018d837 <unknown>\n",
      "#18 0x55d1201a44c1 <unknown>\n",
      "#19 0x55d120174d1e <unknown>\n",
      "#20 0x55d1201c32b8 <unknown>\n",
      "#21 0x55d1201c34ba <unknown>\n",
      "#22 0x55d1201d2d5d <unknown>\n",
      "#23 0x7fcd4b8c4ac3 <unknown>\n",
      "\n",
      "Fetching: https://www.visitpittsburgh.com/things-to-do/family-fun/\n",
      "Error fetching https://www.visitpittsburgh.com/things-to-do/family-fun/: Message: timeout: Timed out receiving message from renderer: 10.000\n",
      "  (Session info: chrome=129.0.6668.100)\n",
      "Stacktrace:\n",
      "#0 0x55d1201d40ca <unknown>\n",
      "#1 0x55d11fed2a80 <unknown>\n",
      "#2 0x55d11febb073 <unknown>\n",
      "#3 0x55d11febad91 <unknown>\n",
      "#4 0x55d11feb8e58 <unknown>\n",
      "#5 0x55d11feb963f <unknown>\n",
      "#6 0x55d11fec8b57 <unknown>\n",
      "#7 0x55d11fedecdd <unknown>\n",
      "#8 0x55d11fee408b <unknown>\n",
      "#9 0x55d11feb9cf7 <unknown>\n",
      "#10 0x55d11fedeb4c <unknown>\n",
      "#11 0x55d11ff64a82 <unknown>\n",
      "#12 0x55d11ff45933 <unknown>\n",
      "#13 0x55d11ff13e3d <unknown>\n",
      "#14 0x55d11ff1492e <unknown>\n",
      "#15 0x55d12019fc8f <unknown>\n",
      "#16 0x55d1201a3d50 <unknown>\n",
      "#17 0x55d12018d837 <unknown>\n",
      "#18 0x55d1201a44c1 <unknown>\n",
      "#19 0x55d120174d1e <unknown>\n",
      "#20 0x55d1201c32b8 <unknown>\n",
      "#21 0x55d1201c34ba <unknown>\n",
      "#22 0x55d1201d2d5d <unknown>\n",
      "#23 0x7fcd4b8c4ac3 <unknown>\n",
      "\n",
      "Fetching: https://www.visitpittsburgh.com/things-to-do/pittsburgh-sports-teams/\n",
      "Error fetching https://www.visitpittsburgh.com/things-to-do/pittsburgh-sports-teams/: Message: timeout: Timed out receiving message from renderer: 10.000\n",
      "  (Session info: chrome=129.0.6668.100)\n",
      "Stacktrace:\n",
      "#0 0x55d1201d40ca <unknown>\n",
      "#1 0x55d11fed2a80 <unknown>\n",
      "#2 0x55d11febb073 <unknown>\n",
      "#3 0x55d11febad91 <unknown>\n",
      "#4 0x55d11feb8e58 <unknown>\n",
      "#5 0x55d11feb963f <unknown>\n",
      "#6 0x55d11fec8b57 <unknown>\n",
      "#7 0x55d11fedecdd <unknown>\n",
      "#8 0x55d11fee408b <unknown>\n",
      "#9 0x55d11feb9cf7 <unknown>\n",
      "#10 0x55d11fedeb4c <unknown>\n",
      "#11 0x55d11ff64a82 <unknown>\n",
      "#12 0x55d11ff45933 <unknown>\n",
      "#13 0x55d11ff13e3d <unknown>\n",
      "#14 0x55d11ff1492e <unknown>\n",
      "#15 0x55d12019fc8f <unknown>\n",
      "#16 0x55d1201a3d50 <unknown>\n",
      "#17 0x55d12018d837 <unknown>\n",
      "#18 0x55d1201a44c1 <unknown>\n",
      "#19 0x55d120174d1e <unknown>\n",
      "#20 0x55d1201c32b8 <unknown>\n",
      "#21 0x55d1201c34ba <unknown>\n",
      "#22 0x55d1201d2d5d <unknown>\n",
      "#23 0x7fcd4b8c4ac3 <unknown>\n",
      "\n",
      "Fetching: https://www.visitpittsburgh.com/things-to-do/arts-culture/\n",
      "Error fetching https://www.visitpittsburgh.com/things-to-do/arts-culture/: Message: timeout: Timed out receiving message from renderer: 10.000\n",
      "  (Session info: chrome=129.0.6668.100)\n",
      "Stacktrace:\n",
      "#0 0x55d1201d40ca <unknown>\n",
      "#1 0x55d11fed2a80 <unknown>\n",
      "#2 0x55d11febb073 <unknown>\n",
      "#3 0x55d11febad91 <unknown>\n",
      "#4 0x55d11feb8e58 <unknown>\n",
      "#5 0x55d11feb963f <unknown>\n",
      "#6 0x55d11fec8b57 <unknown>\n",
      "#7 0x55d11fedecdd <unknown>\n",
      "#8 0x55d11fee408b <unknown>\n",
      "#9 0x55d11feb9cf7 <unknown>\n",
      "#10 0x55d11fedeb4c <unknown>\n",
      "#11 0x55d11ff64a82 <unknown>\n",
      "#12 0x55d11ff45933 <unknown>\n",
      "#13 0x55d11ff13e3d <unknown>\n",
      "#14 0x55d11ff1492e <unknown>\n",
      "#15 0x55d12019fc8f <unknown>\n",
      "#16 0x55d1201a3d50 <unknown>\n",
      "#17 0x55d12018d837 <unknown>\n",
      "#18 0x55d1201a44c1 <unknown>\n",
      "#19 0x55d120174d1e <unknown>\n",
      "#20 0x55d1201c32b8 <unknown>\n",
      "#21 0x55d1201c34ba <unknown>\n",
      "#22 0x55d1201d2d5d <unknown>\n",
      "#23 0x7fcd4b8c4ac3 <unknown>\n",
      "\n",
      "Fetching: https://www.visitpittsburgh.com/meetings-and-events/\n",
      "Error fetching https://www.visitpittsburgh.com/meetings-and-events/: Message: timeout: Timed out receiving message from renderer: 10.000\n",
      "  (Session info: chrome=129.0.6668.100)\n",
      "Stacktrace:\n",
      "#0 0x55d1201d40ca <unknown>\n",
      "#1 0x55d11fed2a80 <unknown>\n",
      "#2 0x55d11febb073 <unknown>\n",
      "#3 0x55d11febad91 <unknown>\n",
      "#4 0x55d11feb8e58 <unknown>\n",
      "#5 0x55d11feb963f <unknown>\n",
      "#6 0x55d11fec8b57 <unknown>\n",
      "#7 0x55d11fedecdd <unknown>\n",
      "#8 0x55d11fee408b <unknown>\n",
      "#9 0x55d11feb9cf7 <unknown>\n",
      "#10 0x55d11fedeb4c <unknown>\n",
      "#11 0x55d11ff64a82 <unknown>\n",
      "#12 0x55d11ff45933 <unknown>\n",
      "#13 0x55d11ff13e3d <unknown>\n",
      "#14 0x55d11ff1492e <unknown>\n",
      "#15 0x55d12019fc8f <unknown>\n",
      "#16 0x55d1201a3d50 <unknown>\n",
      "#17 0x55d12018d837 <unknown>\n",
      "#18 0x55d1201a44c1 <unknown>\n",
      "#19 0x55d120174d1e <unknown>\n",
      "#20 0x55d1201c32b8 <unknown>\n",
      "#21 0x55d1201c34ba <unknown>\n",
      "#22 0x55d1201d2d5d <unknown>\n",
      "#23 0x7fcd4b8c4ac3 <unknown>\n",
      "\n",
      "Fetching: https://www.visitpittsburgh.com/events-festivals/annual-events/\n",
      "Error fetching https://www.visitpittsburgh.com/events-festivals/annual-events/: Message: timeout: Timed out receiving message from renderer: 10.000\n",
      "  (Session info: chrome=129.0.6668.100)\n",
      "Stacktrace:\n",
      "#0 0x55d1201d40ca <unknown>\n",
      "#1 0x55d11fed2a80 <unknown>\n",
      "#2 0x55d11febb073 <unknown>\n",
      "#3 0x55d11febad91 <unknown>\n",
      "#4 0x55d11feb8e58 <unknown>\n",
      "#5 0x55d11feb963f <unknown>\n",
      "#6 0x55d11fec8b57 <unknown>\n",
      "#7 0x55d11fedecdd <unknown>\n",
      "#8 0x55d11fee408b <unknown>\n",
      "#9 0x55d11feb9cf7 <unknown>\n",
      "#10 0x55d11fedeb4c <unknown>\n",
      "#11 0x55d11ff64a82 <unknown>\n",
      "#12 0x55d11ff45933 <unknown>\n",
      "#13 0x55d11ff13e3d <unknown>\n",
      "#14 0x55d11ff1492e <unknown>\n",
      "#15 0x55d12019fc8f <unknown>\n",
      "#16 0x55d1201a3d50 <unknown>\n",
      "#17 0x55d12018d837 <unknown>\n",
      "#18 0x55d1201a44c1 <unknown>\n",
      "#19 0x55d120174d1e <unknown>\n",
      "#20 0x55d1201c32b8 <unknown>\n",
      "#21 0x55d1201c34ba <unknown>\n",
      "#22 0x55d1201d2d5d <unknown>\n",
      "#23 0x7fcd4b8c4ac3 <unknown>\n",
      "\n",
      "Fetching: https://www.visitpittsburgh.com/events-festivals/holidays/\n",
      "Error fetching https://www.visitpittsburgh.com/events-festivals/holidays/: Message: timeout: Timed out receiving message from renderer: 10.000\n",
      "  (Session info: chrome=129.0.6668.100)\n",
      "Stacktrace:\n",
      "#0 0x55d1201d40ca <unknown>\n",
      "#1 0x55d11fed2a80 <unknown>\n",
      "#2 0x55d11febb073 <unknown>\n",
      "#3 0x55d11febad91 <unknown>\n",
      "#4 0x55d11feb8e58 <unknown>\n",
      "#5 0x55d11feb963f <unknown>\n",
      "#6 0x55d11fec8b57 <unknown>\n",
      "#7 0x55d11fedecdd <unknown>\n",
      "#8 0x55d11fee408b <unknown>\n",
      "#9 0x55d11feb9cf7 <unknown>\n",
      "#10 0x55d11fedeb4c <unknown>\n",
      "#11 0x55d11ff64a82 <unknown>\n",
      "#12 0x55d11ff45933 <unknown>\n",
      "#13 0x55d11ff13e3d <unknown>\n",
      "#14 0x55d11ff1492e <unknown>\n",
      "#15 0x55d12019fc8f <unknown>\n",
      "#16 0x55d1201a3d50 <unknown>\n",
      "#17 0x55d12018d837 <unknown>\n",
      "#18 0x55d1201a44c1 <unknown>\n",
      "#19 0x55d120174d1e <unknown>\n",
      "#20 0x55d1201c32b8 <unknown>\n",
      "#21 0x55d1201c34ba <unknown>\n",
      "#22 0x55d1201d2d5d <unknown>\n",
      "#23 0x7fcd4b8c4ac3 <unknown>\n",
      "\n",
      "Fetching: https://www.visitpittsburgh.com/events-festivals/halloween-events/\n",
      "Error fetching https://www.visitpittsburgh.com/events-festivals/halloween-events/: Message: timeout: Timed out receiving message from renderer: 10.000\n",
      "  (Session info: chrome=129.0.6668.100)\n",
      "Stacktrace:\n",
      "#0 0x55d1201d40ca <unknown>\n",
      "#1 0x55d11fed2a80 <unknown>\n",
      "#2 0x55d11febb073 <unknown>\n",
      "#3 0x55d11febad91 <unknown>\n",
      "#4 0x55d11feb8e58 <unknown>\n",
      "#5 0x55d11feb963f <unknown>\n",
      "#6 0x55d11fec8b57 <unknown>\n",
      "#7 0x55d11fedecdd <unknown>\n",
      "#8 0x55d11fee408b <unknown>\n",
      "#9 0x55d11feb9cf7 <unknown>\n",
      "#10 0x55d11fedeb4c <unknown>\n",
      "#11 0x55d11ff64a82 <unknown>\n",
      "#12 0x55d11ff45933 <unknown>\n",
      "#13 0x55d11ff13e3d <unknown>\n",
      "#14 0x55d11ff1492e <unknown>\n",
      "#15 0x55d12019fc8f <unknown>\n",
      "#16 0x55d1201a3d50 <unknown>\n",
      "#17 0x55d12018d837 <unknown>\n",
      "#18 0x55d1201a44c1 <unknown>\n",
      "#19 0x55d120174d1e <unknown>\n",
      "#20 0x55d1201c32b8 <unknown>\n",
      "#21 0x55d1201c34ba <unknown>\n",
      "#22 0x55d1201d2d5d <unknown>\n",
      "#23 0x7fcd4b8c4ac3 <unknown>\n",
      "\n",
      "Fetching: https://www.visitpittsburgh.com/events-festivals/film-festivals/\n",
      "Error fetching https://www.visitpittsburgh.com/events-festivals/film-festivals/: Message: timeout: Timed out receiving message from renderer: 10.000\n",
      "  (Session info: chrome=129.0.6668.100)\n",
      "Stacktrace:\n",
      "#0 0x55d1201d40ca <unknown>\n",
      "#1 0x55d11fed2a80 <unknown>\n",
      "#2 0x55d11febb073 <unknown>\n",
      "#3 0x55d11febad91 <unknown>\n",
      "#4 0x55d11feb8e58 <unknown>\n",
      "#5 0x55d11feb963f <unknown>\n",
      "#6 0x55d11fec8b57 <unknown>\n",
      "#7 0x55d11fedecdd <unknown>\n",
      "#8 0x55d11fee408b <unknown>\n",
      "#9 0x55d11feb9cf7 <unknown>\n",
      "#10 0x55d11fedeb4c <unknown>\n",
      "#11 0x55d11ff64a82 <unknown>\n",
      "#12 0x55d11ff45933 <unknown>\n",
      "#13 0x55d11ff13e3d <unknown>\n",
      "#14 0x55d11ff1492e <unknown>\n",
      "#15 0x55d12019fc8f <unknown>\n",
      "#16 0x55d1201a3d50 <unknown>\n",
      "#17 0x55d12018d837 <unknown>\n",
      "#18 0x55d1201a44c1 <unknown>\n",
      "#19 0x55d120174d1e <unknown>\n",
      "#20 0x55d1201c32b8 <unknown>\n",
      "#21 0x55d1201c34ba <unknown>\n",
      "#22 0x55d1201d2d5d <unknown>\n",
      "#23 0x7fcd4b8c4ac3 <unknown>\n",
      "\n",
      "Fetching: https://www.visitpittsburgh.com/events-festivals/food-festivals/soul-food-festival/\n",
      "Error fetching https://www.visitpittsburgh.com/events-festivals/food-festivals/soul-food-festival/: Message: timeout: Timed out receiving message from renderer: 10.000\n",
      "  (Session info: chrome=129.0.6668.100)\n",
      "Stacktrace:\n",
      "#0 0x55d1201d40ca <unknown>\n",
      "#1 0x55d11fed2a80 <unknown>\n",
      "#2 0x55d11febb073 <unknown>\n",
      "#3 0x55d11febad91 <unknown>\n",
      "#4 0x55d11feb8e58 <unknown>\n",
      "#5 0x55d11feb963f <unknown>\n",
      "#6 0x55d11fec8b57 <unknown>\n",
      "#7 0x55d11fedecdd <unknown>\n",
      "#8 0x55d11fee408b <unknown>\n",
      "#9 0x55d11feb9cf7 <unknown>\n",
      "#10 0x55d11fedeb4c <unknown>\n",
      "#11 0x55d11ff64a82 <unknown>\n",
      "#12 0x55d11ff45933 <unknown>\n",
      "#13 0x55d11ff13e3d <unknown>\n",
      "#14 0x55d11ff1492e <unknown>\n",
      "#15 0x55d12019fc8f <unknown>\n",
      "#16 0x55d1201a3d50 <unknown>\n",
      "#17 0x55d12018d837 <unknown>\n",
      "#18 0x55d1201a44c1 <unknown>\n",
      "#19 0x55d120174d1e <unknown>\n",
      "#20 0x55d1201c32b8 <unknown>\n",
      "#21 0x55d1201c34ba <unknown>\n",
      "#22 0x55d1201d2d5d <unknown>\n",
      "#23 0x7fcd4b8c4ac3 <unknown>\n",
      "\n",
      "Fetching: https://www.visitpittsburgh.com/events-festivals/food-festivals/the-original-pittsburgh-taco-festival/\n",
      "Error fetching https://www.visitpittsburgh.com/events-festivals/food-festivals/the-original-pittsburgh-taco-festival/: Message: timeout: Timed out receiving message from renderer: 10.000\n",
      "  (Session info: chrome=129.0.6668.100)\n",
      "Stacktrace:\n",
      "#0 0x55d1201d40ca <unknown>\n",
      "#1 0x55d11fed2a80 <unknown>\n",
      "#2 0x55d11febb073 <unknown>\n",
      "#3 0x55d11febad91 <unknown>\n",
      "#4 0x55d11feb8e58 <unknown>\n",
      "#5 0x55d11feb963f <unknown>\n",
      "#6 0x55d11fec8b57 <unknown>\n",
      "#7 0x55d11fedecdd <unknown>\n",
      "#8 0x55d11fee408b <unknown>\n",
      "#9 0x55d11feb9cf7 <unknown>\n",
      "#10 0x55d11fedeb4c <unknown>\n",
      "#11 0x55d11ff64a82 <unknown>\n",
      "#12 0x55d11ff45933 <unknown>\n",
      "#13 0x55d11ff13e3d <unknown>\n",
      "#14 0x55d11ff1492e <unknown>\n",
      "#15 0x55d12019fc8f <unknown>\n",
      "#16 0x55d1201a3d50 <unknown>\n",
      "#17 0x55d12018d837 <unknown>\n",
      "#18 0x55d1201a44c1 <unknown>\n",
      "#19 0x55d120174d1e <unknown>\n",
      "#20 0x55d1201c32b8 <unknown>\n",
      "#21 0x55d1201c34ba <unknown>\n",
      "#22 0x55d1201d2d5d <unknown>\n",
      "#23 0x7fcd4b8c4ac3 <unknown>\n",
      "\n",
      "Fetching: https://www.visitpittsburgh.com/blog/top-beer-festivals-to-attend-in-pittsburgh-this-fall/\n",
      "Error fetching https://www.visitpittsburgh.com/blog/top-beer-festivals-to-attend-in-pittsburgh-this-fall/: Message: timeout: Timed out receiving message from renderer: 10.000\n",
      "  (Session info: chrome=129.0.6668.100)\n",
      "Stacktrace:\n",
      "#0 0x55d1201d40ca <unknown>\n",
      "#1 0x55d11fed2a80 <unknown>\n",
      "#2 0x55d11febb073 <unknown>\n",
      "#3 0x55d11febad91 <unknown>\n",
      "#4 0x55d11feb8e58 <unknown>\n",
      "#5 0x55d11feb963f <unknown>\n",
      "#6 0x55d11fec8b57 <unknown>\n",
      "#7 0x55d11fedecdd <unknown>\n",
      "#8 0x55d11fee408b <unknown>\n",
      "#9 0x55d11feb9cf7 <unknown>\n",
      "#10 0x55d11fedeb4c <unknown>\n",
      "#11 0x55d11ff64a82 <unknown>\n",
      "#12 0x55d11ff45933 <unknown>\n",
      "#13 0x55d11ff13e3d <unknown>\n",
      "#14 0x55d11ff1492e <unknown>\n",
      "#15 0x55d12019fc8f <unknown>\n",
      "#16 0x55d1201a3d50 <unknown>\n",
      "#17 0x55d12018d837 <unknown>\n",
      "#18 0x55d1201a44c1 <unknown>\n",
      "#19 0x55d120174d1e <unknown>\n",
      "#20 0x55d1201c32b8 <unknown>\n",
      "#21 0x55d1201c34ba <unknown>\n",
      "#22 0x55d1201d2d5d <unknown>\n",
      "#23 0x7fcd4b8c4ac3 <unknown>\n",
      "\n",
      "Data saved to ../raw_documents/Pittsburgh_Food_Festivals.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import os\n",
    "\n",
    "class Scraper:\n",
    "    def __init__(self):\n",
    "        self.chrome_options = Options()\n",
    "        self.chrome_options.add_argument(\"--headless\")\n",
    "        self.chrome_options.add_argument(\"--disable-gpu\")\n",
    "        self.chrome_options.add_argument(\"--no-sandbox\")\n",
    "        self.chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "        self.driver = webdriver.Chrome(options=self.chrome_options)\n",
    "        self.driver.set_page_load_timeout(10)\n",
    "\n",
    "    def get_html(self, url: str):\n",
    "        \"\"\"Get HTML.\"\"\"\n",
    "        self.driver.get(url)\n",
    "        return self.driver.page_source\n",
    "\n",
    "    def get_soup(self, url: str):\n",
    "        \"\"\"Parse and return BeautifulSoup object.\"\"\"\n",
    "        html = self.get_html(url)\n",
    "        return BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "    def close(self):\n",
    "        self.driver.quit()\n",
    "\n",
    "    def __del__(self):\n",
    "        self.close()\n",
    "\n",
    "def organize_content_by_heading(soup: BeautifulSoup):\n",
    "    \"\"\"Organize content based on headings, ensuring all relevant text is captured.\"\"\"\n",
    "    data = {}\n",
    "    current_h1, current_h2, current_h3, current_h4, current_h5, current_h6 = None, None, None, None, None, None\n",
    "\n",
    "    for tag in soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'p', 'div', 'span']):\n",
    "        if tag.name == 'h1':\n",
    "            current_h1 = tag.get_text(strip=True)\n",
    "            data[current_h1] = {'content': \"\"}\n",
    "            current_h2 = current_h3 = current_h4 = current_h5 = current_h6 = None  # Reset lower headings\n",
    "        elif tag.name == 'h2' and current_h1:\n",
    "            current_h2 = tag.get_text(strip=True)\n",
    "            data[current_h1][current_h2] = {'content': \"\"}\n",
    "            current_h3 = current_h4 = current_h5 = current_h6 = None\n",
    "        elif tag.name == 'h3' and current_h2:\n",
    "            current_h3 = tag.get_text(strip=True)\n",
    "            data[current_h1][current_h2][current_h3] = {'content': \"\"}\n",
    "            current_h4 = current_h5 = current_h6 = None\n",
    "        elif tag.name == 'h4' and current_h3:\n",
    "            current_h4 = tag.get_text(strip=True)\n",
    "            data[current_h1][current_h2][current_h3][current_h4] = {'content': \"\"}\n",
    "            current_h5 = current_h6 = None\n",
    "        elif tag.name == 'h5' and current_h4:\n",
    "            current_h5 = tag.get_text(strip=True)\n",
    "            data[current_h1][current_h2][current_h3][current_h4][current_h5] = {'content': \"\"}\n",
    "            current_h6 = None\n",
    "        elif tag.name == 'h6' and current_h5:\n",
    "            current_h6 = tag.get_text(strip=True)\n",
    "            data[current_h1][current_h2][current_h3][current_h4][current_h5][current_h6] = {'content': \"\"}\n",
    "        else:\n",
    "            content = tag.get_text(strip=True)\n",
    "            if content:\n",
    "                if current_h6:\n",
    "                    data[current_h1][current_h2][current_h3][current_h4][current_h5][current_h6]['content'] += content + \" \"\n",
    "                elif current_h5:\n",
    "                    data[current_h1][current_h2][current_h3][current_h4][current_h5]['content'] += content + \" \"\n",
    "                elif current_h4:\n",
    "                    data[current_h1][current_h2][current_h3][current_h4]['content'] += content + \" \"\n",
    "                elif current_h3:\n",
    "                    data[current_h1][current_h2][current_h3]['content'] += content + \" \"\n",
    "                elif current_h2:\n",
    "                    data[current_h1][current_h2]['content'] += content + \" \"\n",
    "                elif current_h1:\n",
    "                    data[current_h1]['content'] += content + \" \"\n",
    "\n",
    "    return data\n",
    "\n",
    "def save_to_json(data, filename):\n",
    "    \"\"\"Save data to a JSON file, ensuring no overwrite of existing content.\"\"\"\n",
    "    try:\n",
    "        if os.path.exists(filename):\n",
    "            with open(filename, 'r', encoding='utf-8') as f:\n",
    "                existing_data = json.load(f)\n",
    "            if isinstance(existing_data, list):\n",
    "                existing_data.extend(data) \n",
    "            else:\n",
    "                existing_data.update(data)  \n",
    "        else:\n",
    "            existing_data = data  \n",
    "\n",
    "        with open(filename, 'w', encoding='utf-8') as f:\n",
    "            json.dump(existing_data, f, ensure_ascii=False, indent=4)\n",
    "        print(f\"Data saved to {filename}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error writing to JSON file: {e}\")\n",
    "\n",
    "def scrape_links(scraper_, links):\n",
    "    \"\"\"Scrape multiple links and return organized data.\"\"\"\n",
    "    all_data = []\n",
    "    for link in links:\n",
    "        try:\n",
    "            print(f\"Fetching: {link}\")\n",
    "            soup = scraper_.get_soup(link)\n",
    "            page_data = organize_content_by_heading(soup)\n",
    "            page_data['url'] = link \n",
    "            all_data.append(page_data)\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching {link}: {e}\")\n",
    "\n",
    "    return all_data\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    scraper_ = Scraper()\n",
    "\n",
    "    links = [\n",
    "        \"https://www.visitpittsburgh.com/events-festivals/food-festivals/\",\n",
    "        \"https://www.visitpittsburgh.com/blog/\",\n",
    "        \"https://www.visitpittsburgh.com/plan-your-trip/\",\n",
    "        \"https://www.visitpittsburgh.com/events-festivals/\",\n",
    "        \"https://www.visitpittsburgh.com/events-festivals/?page=2\",\n",
    "        \"https://www.visitpittsburgh.com/events-festivals/?page=3\",\n",
    "        \"https://www.visitpittsburgh.com/events-festivals/?page=4\",\n",
    "        \"https://www.visitpittsburgh.com/events-festivals/?page=5\",\n",
    "        \"https://www.visitpittsburgh.com/events-festivals/?page=6\",\n",
    "        \"https://www.visitpittsburgh.com/events-festivals/?page=7\",\n",
    "        \"https://www.visitpittsburgh.com/events-festivals/?page=8\",\n",
    "        \"https://www.visitpittsburgh.com/events-festivals/?page=9\",\n",
    "        \"https://www.visitpittsburgh.com/events-festivals/?page=10\",\n",
    "        \"https://www.visitpittsburgh.com/events-festivals/?page=11\",\n",
    "        \"https://www.visitpittsburgh.com/events-festivals/?page=12\",\n",
    "        \"https://www.visitpittsburgh.com/events-festivals/?page=13\",\n",
    "        \"https://www.visitpittsburgh.com/events-festivals/?page=14\",\n",
    "        \"https://www.visitpittsburgh.com/events-festivals/?page=15\",\n",
    "        \"https://www.visitpittsburgh.com/events-festivals/?page=16\",\n",
    "        \"https://www.visitpittsburgh.com/things-to-do/\",\n",
    "        \"https://www.visitpittsburgh.com/restaurants-culinary/\",\n",
    "        \"https://www.visitpittsburgh.com/restaurants-culinary/craft-breweries/\",\n",
    "        \"https://www.visitpittsburgh.com/things-to-do/family-fun/\",\n",
    "        \"https://www.visitpittsburgh.com/things-to-do/pittsburgh-sports-teams/\",\n",
    "        \"https://www.visitpittsburgh.com/things-to-do/arts-culture/\",\n",
    "        \"https://www.visitpittsburgh.com/meetings-and-events/\",\n",
    "        \"https://www.visitpittsburgh.com/events-festivals/annual-events/\",\n",
    "        \"https://www.visitpittsburgh.com/events-festivals/holidays/\",\n",
    "        \"https://www.visitpittsburgh.com/events-festivals/halloween-events/\",\n",
    "        \"https://www.visitpittsburgh.com/events-festivals/film-festivals/\",\n",
    "        \"https://www.visitpittsburgh.com/events-festivals/food-festivals/soul-food-festival/\",\n",
    "        \"https://www.visitpittsburgh.com/events-festivals/food-festivals/the-original-pittsburgh-taco-festival/\",\n",
    "        \"https://www.visitpittsburgh.com/blog/top-beer-festivals-to-attend-in-pittsburgh-this-fall/\"\n",
    "       \n",
    "    ]\n",
    "    scraped_data = scrape_links(scraper_, links)\n",
    "    save_to_json(scraped_data, \"../raw_documents/Pittsburgh_Food_Festivals.json\")\n",
    "    scraper_.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching: https://www.visitpittsburgh.com/events-festivals/?page=14\n",
      "Fetching: https://www.visitpittsburgh.com/events-festivals/?page=15\n",
      "Fetching: https://www.visitpittsburgh.com/events-festivals/?page=16\n",
      "Fetching: https://www.visitpittsburgh.com/things-to-do/\n",
      "Fetching: https://www.visitpittsburgh.com/restaurants-culinary/\n",
      "Fetching: https://www.visitpittsburgh.com/restaurants-culinary/craft-breweries/\n",
      "Fetching: https://www.visitpittsburgh.com/things-to-do/family-fun/\n",
      "Fetching: https://www.visitpittsburgh.com/things-to-do/pittsburgh-sports-teams/\n",
      "Fetching: https://www.visitpittsburgh.com/things-to-do/arts-culture/\n",
      "Fetching: https://www.visitpittsburgh.com/meetings-and-events/\n",
      "Fetching: https://www.visitpittsburgh.com/events-festivals/annual-events/\n",
      "Fetching: https://www.visitpittsburgh.com/events-festivals/holidays/\n",
      "Fetching: https://www.visitpittsburgh.com/events-festivals/halloween-events/\n",
      "Fetching: https://www.visitpittsburgh.com/events-festivals/film-festivals/\n",
      "Fetching: https://www.visitpittsburgh.com/events-festivals/food-festivals/soul-food-festival/\n",
      "Fetching: https://www.visitpittsburgh.com/events-festivals/food-festivals/the-original-pittsburgh-taco-festival/\n",
      "Fetching: https://www.visitpittsburgh.com/blog/top-beer-festivals-to-attend-in-pittsburgh-this-fall/\n",
      "Data saved to ../raw_documents/Pittsburgh_Food_Festivals.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import os\n",
    "\n",
    "class Scraper:\n",
    "    def __init__(self):\n",
    "        self.chrome_options = Options()\n",
    "        self.chrome_options.add_argument(\"--headless\")\n",
    "        self.chrome_options.add_argument(\"--disable-gpu\")\n",
    "        self.chrome_options.add_argument(\"--no-sandbox\")\n",
    "        self.chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "        self.driver = webdriver.Chrome(options=self.chrome_options)\n",
    "        self.driver.set_page_load_timeout(10)\n",
    "\n",
    "    def get_html(self, url: str):\n",
    "        \"\"\"Get HTML with explicit wait for dynamic content.\"\"\"\n",
    "        self.driver.get(url)\n",
    "        \n",
    "        try:\n",
    "            WebDriverWait(self.driver, 10).until(\n",
    "                EC.presence_of_element_located((By.TAG_NAME, \"body\")) \n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"Error while waiting for page to load: {e}\")\n",
    "        \n",
    "        return self.driver.page_source\n",
    "\n",
    "    def get_soup(self, url: str):\n",
    "        html = self.get_html(url)\n",
    "        return BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "    def close(self):\n",
    "        self.driver.quit()\n",
    "\n",
    "    def __del__(self):\n",
    "        self.close()\n",
    "\n",
    "def organize_content_by_heading(soup: BeautifulSoup):\n",
    "\n",
    "    data = {}\n",
    "    current_h1, current_h2, current_h3, current_h4, current_h5, current_h6 = None, None, None, None, None, None\n",
    "\n",
    "    for tag in soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'p', 'div', 'span']):\n",
    "        if tag.name == 'h1':\n",
    "            current_h1 = tag.get_text(strip=True)\n",
    "            data[current_h1] = {'content': \"\"}\n",
    "            current_h2 = current_h3 = current_h4 = current_h5 = current_h6 = None\n",
    "        elif tag.name == 'h2' and current_h1:\n",
    "            current_h2 = tag.get_text(strip=True)\n",
    "            data[current_h1][current_h2] = {'content': \"\"}\n",
    "            current_h3 = current_h4 = current_h5 = current_h6 = None\n",
    "        elif tag.name == 'h3' and current_h2:\n",
    "            current_h3 = tag.get_text(strip=True)\n",
    "            data[current_h1][current_h2][current_h3] = {'content': \"\"}\n",
    "            current_h4 = current_h5 = current_h6 = None\n",
    "        elif tag.name == 'h4' and current_h3:\n",
    "            current_h4 = tag.get_text(strip=True)\n",
    "            data[current_h1][current_h2][current_h3][current_h4] = {'content': \"\"}\n",
    "            current_h5 = current_h6 = None\n",
    "        elif tag.name == 'h5' and current_h4:\n",
    "            current_h5 = tag.get_text(strip=True)\n",
    "            data[current_h1][current_h2][current_h3][current_h4][current_h5] = {'content': \"\"}\n",
    "            current_h6 = None\n",
    "        elif tag.name == 'h6' and current_h5:\n",
    "            current_h6 = tag.get_text(strip=True)\n",
    "            data[current_h1][current_h2][current_h3][current_h4][current_h5][current_h6] = {'content': \"\"}\n",
    "        else:\n",
    "            content = tag.get_text(strip=True)\n",
    "            if content:\n",
    "                if current_h6:\n",
    "                    data[current_h1][current_h2][current_h3][current_h4][current_h5][current_h6]['content'] += content + \" \"\n",
    "                elif current_h5:\n",
    "                    data[current_h1][current_h2][current_h3][current_h4][current_h5]['content'] += content + \" \"\n",
    "                elif current_h4:\n",
    "                    data[current_h1][current_h2][current_h3][current_h4]['content'] += content + \" \"\n",
    "                elif current_h3:\n",
    "                    data[current_h1][current_h2][current_h3]['content'] += content + \" \"\n",
    "                elif current_h2:\n",
    "                    data[current_h1][current_h2]['content'] += content + \" \"\n",
    "                elif current_h1:\n",
    "                    data[current_h1]['content'] += content + \" \"\n",
    "\n",
    "    return data\n",
    "\n",
    "def save_to_json(data, filename):\n",
    "    \"\"\"Save data to a JSON file, ensuring no overwrite of existing content.\"\"\"\n",
    "    try:\n",
    "        if os.path.exists(filename):\n",
    "            with open(filename, 'r', encoding='utf-8') as f:\n",
    "                existing_data = json.load(f)\n",
    "            if isinstance(existing_data, list):\n",
    "                existing_data.extend(data)  \n",
    "            else:\n",
    "                existing_data.update(data) \n",
    "        else:\n",
    "            existing_data = data \n",
    "        with open(filename, 'w', encoding='utf-8') as f:\n",
    "            json.dump(existing_data, f, ensure_ascii=False, indent=4)\n",
    "        print(f\"Data saved to {filename}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error writing to JSON file: {e}\")\n",
    "\n",
    "def scrape_links(scraper_, links):\n",
    "    \"\"\"Scrape multiple links and return organized data.\"\"\"\n",
    "    all_data = []\n",
    "    for link in links:\n",
    "        try:\n",
    "            print(f\"Fetching: {link}\")\n",
    "            soup = scraper_.get_soup(link)\n",
    "            page_data = organize_content_by_heading(soup)\n",
    "            page_data['url'] = link \n",
    "            all_data.append(page_data)\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching {link}: {e}\")\n",
    "    return all_data\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    scraper_ = Scraper()\n",
    "\n",
    "    links = [\n",
    "        \"https://www.visitpittsburgh.com/events-festivals/?page=14\",\n",
    "        \"https://www.visitpittsburgh.com/events-festivals/?page=15\",\n",
    "        \"https://www.visitpittsburgh.com/events-festivals/?page=16\",\n",
    "        \"https://www.visitpittsburgh.com/things-to-do/\",\n",
    "        \"https://www.visitpittsburgh.com/restaurants-culinary/\",\n",
    "        \"https://www.visitpittsburgh.com/restaurants-culinary/craft-breweries/\",\n",
    "        \"https://www.visitpittsburgh.com/things-to-do/family-fun/\",\n",
    "        \"https://www.visitpittsburgh.com/things-to-do/pittsburgh-sports-teams/\",\n",
    "        \"https://www.visitpittsburgh.com/things-to-do/arts-culture/\",\n",
    "        \"https://www.visitpittsburgh.com/meetings-and-events/\",\n",
    "        \"https://www.visitpittsburgh.com/events-festivals/annual-events/\",\n",
    "        \"https://www.visitpittsburgh.com/events-festivals/holidays/\",\n",
    "        \"https://www.visitpittsburgh.com/events-festivals/halloween-events/\",\n",
    "        \"https://www.visitpittsburgh.com/events-festivals/film-festivals/\",\n",
    "        \"https://www.visitpittsburgh.com/events-festivals/food-festivals/soul-food-festival/\",\n",
    "        \"https://www.visitpittsburgh.com/events-festivals/food-festivals/the-original-pittsburgh-taco-festival/\",\n",
    "        \"https://www.visitpittsburgh.com/blog/top-beer-festivals-to-attend-in-pittsburgh-this-fall/\"\n",
    "    ]\n",
    "\n",
    "    scraped_data = scrape_links(scraper_, links)\n",
    "    save_to_json(scraped_data, \"../raw_documents/Pittsburgh_Food_Festivals.json\")\n",
    "    scraper_.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Picklesburgh\n",
    "### https://www.picklesburgh.com/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching: https://www.picklesburgh.com/\n",
      "Fetching: https://www.picklesburgh.com/vendors/\n",
      "Fetching: https://www.picklesburgh.com/entertainment/\n",
      "Fetching: https://www.picklesburgh.com/games/\n",
      "Fetching: https://www.picklesburgh.com/festival-schedule/lil-gherkins-activity-area/\n",
      "Fetching: https://www.picklesburgh.com/taste-of-picklesburgh/\n",
      "Fetching: https://www.picklesburgh.com/news/\n",
      "Fetching: https://www.picklesburgh.com/accessibility/\n",
      "Fetching: https://www.picklesburgh.com/visit/getting-here/\n",
      "Data saved to ../raw_documents/Picklesburgh.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import os\n",
    "\n",
    "class Scraper:\n",
    "    def __init__(self):\n",
    "        self.chrome_options = Options()\n",
    "        self.chrome_options.add_argument(\"--headless\")\n",
    "        self.chrome_options.add_argument(\"--disable-gpu\")\n",
    "        self.chrome_options.add_argument(\"--no-sandbox\")\n",
    "        self.chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "        self.driver = webdriver.Chrome(options=self.chrome_options)\n",
    "        self.driver.set_page_load_timeout(10)\n",
    "\n",
    "    def get_html(self, url: str):\n",
    "        \"\"\"Get HTML.\"\"\"\n",
    "        self.driver.get(url)\n",
    "        return self.driver.page_source\n",
    "\n",
    "    def get_soup(self, url: str):\n",
    "        \"\"\"Parse and return BeautifulSoup object.\"\"\"\n",
    "        html = self.get_html(url)\n",
    "        return BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "    def close(self):\n",
    "        self.driver.quit()\n",
    "\n",
    "    def __del__(self):\n",
    "        self.close()\n",
    "\n",
    "def organize_content_by_heading(soup: BeautifulSoup):\n",
    "    \"\"\"Organize content based on headings, ensuring all relevant text is captured.\"\"\"\n",
    "    data = {}\n",
    "    current_h1, current_h2, current_h3, current_h4, current_h5, current_h6 = None, None, None, None, None, None\n",
    "\n",
    "    for tag in soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'p', 'div', 'span']):\n",
    "        if tag.name == 'h1':\n",
    "            current_h1 = tag.get_text(strip=True)\n",
    "            data[current_h1] = {'content': \"\"}\n",
    "            current_h2 = current_h3 = current_h4 = current_h5 = current_h6 = None  # Reset lower headings\n",
    "        elif tag.name == 'h2' and current_h1:\n",
    "            current_h2 = tag.get_text(strip=True)\n",
    "            data[current_h1][current_h2] = {'content': \"\"}\n",
    "            current_h3 = current_h4 = current_h5 = current_h6 = None\n",
    "        elif tag.name == 'h3' and current_h2:\n",
    "            current_h3 = tag.get_text(strip=True)\n",
    "            data[current_h1][current_h2][current_h3] = {'content': \"\"}\n",
    "            current_h4 = current_h5 = current_h6 = None\n",
    "        elif tag.name == 'h4' and current_h3:\n",
    "            current_h4 = tag.get_text(strip=True)\n",
    "            data[current_h1][current_h2][current_h3][current_h4] = {'content': \"\"}\n",
    "            current_h5 = current_h6 = None\n",
    "        elif tag.name == 'h5' and current_h4:\n",
    "            current_h5 = tag.get_text(strip=True)\n",
    "            data[current_h1][current_h2][current_h3][current_h4][current_h5] = {'content': \"\"}\n",
    "            current_h6 = None\n",
    "        elif tag.name == 'h6' and current_h5:\n",
    "            current_h6 = tag.get_text(strip=True)\n",
    "            data[current_h1][current_h2][current_h3][current_h4][current_h5][current_h6] = {'content': \"\"}\n",
    "        else:\n",
    "            content = tag.get_text(strip=True)\n",
    "            if content:\n",
    "                if current_h6:\n",
    "                    data[current_h1][current_h2][current_h3][current_h4][current_h5][current_h6]['content'] += content + \" \"\n",
    "                elif current_h5:\n",
    "                    data[current_h1][current_h2][current_h3][current_h4][current_h5]['content'] += content + \" \"\n",
    "                elif current_h4:\n",
    "                    data[current_h1][current_h2][current_h3][current_h4]['content'] += content + \" \"\n",
    "                elif current_h3:\n",
    "                    data[current_h1][current_h2][current_h3]['content'] += content + \" \"\n",
    "                elif current_h2:\n",
    "                    data[current_h1][current_h2]['content'] += content + \" \"\n",
    "                elif current_h1:\n",
    "                    data[current_h1]['content'] += content + \" \"\n",
    "\n",
    "    return data\n",
    "\n",
    "def save_to_json(data, filename):\n",
    "    \"\"\"Save data to a JSON file, ensuring no overwrite of existing content.\"\"\"\n",
    "    try:\n",
    "        if os.path.exists(filename):\n",
    "            with open(filename, 'r', encoding='utf-8') as f:\n",
    "                existing_data = json.load(f)\n",
    "            if isinstance(existing_data, list):\n",
    "                existing_data.extend(data)\n",
    "            else:\n",
    "                existing_data.update(data)  \n",
    "        else:\n",
    "            existing_data = data  \n",
    "\n",
    "        with open(filename, 'w', encoding='utf-8') as f:\n",
    "            json.dump(existing_data, f, ensure_ascii=False, indent=4)\n",
    "        print(f\"Data saved to {filename}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error writing to JSON file: {e}\")\n",
    "\n",
    "def scrape_links(scraper_, links):\n",
    "    all_data = []\n",
    "    for link in links:\n",
    "        try:\n",
    "            print(f\"Fetching: {link}\")\n",
    "            soup = scraper_.get_soup(link)\n",
    "            page_data = organize_content_by_heading(soup)\n",
    "            page_data['url'] = link \n",
    "            all_data.append(page_data)\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching {link}: {e}\")\n",
    "    return all_data\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    scraper_ = Scraper()\n",
    "\n",
    "    links = [\n",
    "        \"https://www.picklesburgh.com/\",\n",
    "        \"https://www.picklesburgh.com/vendors/\",\n",
    "        \"https://www.picklesburgh.com/entertainment/\",\n",
    "        \"https://www.picklesburgh.com/games/\",\n",
    "        \"https://www.picklesburgh.com/festival-schedule/lil-gherkins-activity-area/\",\n",
    "        \"https://www.picklesburgh.com/taste-of-picklesburgh/\",\n",
    "        \"https://www.picklesburgh.com/news/\",\n",
    "        \"https://www.picklesburgh.com/accessibility/\",\n",
    "        \"https://www.picklesburgh.com/visit/getting-here/\"\n",
    "       \n",
    "    ]\n",
    "    scraped_data = scrape_links(scraper_, links)\n",
    "    save_to_json(scraped_data, \"../raw_documents/Picklesburgh.json\")\n",
    "    scraper_.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error fetching https://www.picklesburgh.com/: 403 Client Error: Forbidden for url: https://www.picklesburgh.com/\n",
      "Error fetching https://www.picklesburgh.com/vendors/: 403 Client Error: Forbidden for url: https://www.picklesburgh.com/vendors/\n",
      "Error fetching https://www.picklesburgh.com/entertainment/: 403 Client Error: Forbidden for url: https://www.picklesburgh.com/entertainment/\n",
      "Error fetching https://www.picklesburgh.com/games/: 403 Client Error: Forbidden for url: https://www.picklesburgh.com/games/\n",
      "Error fetching https://www.picklesburgh.com/festival-schedule/lil-gherkins-activity-area/: 403 Client Error: Forbidden for url: https://www.picklesburgh.com/festival-schedule/lil-gherkins-activity-area/\n",
      "Error fetching https://www.picklesburgh.com/taste-of-picklesburgh/: 403 Client Error: Forbidden for url: https://www.picklesburgh.com/taste-of-picklesburgh/\n",
      "Error fetching https://www.picklesburgh.com/news/: 403 Client Error: Forbidden for url: https://www.picklesburgh.com/news/\n",
      "Error fetching https://www.picklesburgh.com/accessibility/: 403 Client Error: Forbidden for url: https://www.picklesburgh.com/accessibility/\n",
      "Error fetching https://www.picklesburgh.com/visit/getting-here/: 403 Client Error: Forbidden for url: https://www.picklesburgh.com/visit/getting-here/\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import sys\n",
    "import re\n",
    "\n",
    "def extract_text_from_url(url):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status() \n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        title = soup.title.string.strip() if soup.title else 'No Title'\n",
    "        for tag in soup([\"script\", \"style\", \"header\", \"footer\", \"nav\", \"aside\"]):\n",
    "            tag.decompose()\n",
    "        for tag in soup.find_all(class_=re.compile(\".*(nav|footer|menu|social|sidebar).*\", re.I)):\n",
    "            tag.decompose()\n",
    "        for tag in soup.find_all(id=re.compile(\".*(nav|footer|menu|social|sidebar).*\", re.I)):\n",
    "            tag.decompose()\n",
    "\n",
    "        stop_phrases = [\n",
    "            \"Follow us on\", \"Contact Us\", \"Privacy Policy\", \"©\",\n",
    "            \"BUY TICKETS\", \"GIVE NOW\", \"SUBSCRIBE\", \"Site Map\",\n",
    "            \"Board Of Directors Login\"\n",
    "        ]\n",
    "\n",
    "        content = []\n",
    "        for element in soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'p', 'li']):\n",
    "            text = ' '.join(element.stripped_strings)\n",
    "            if text:\n",
    "                if any(phrase.lower() in text.lower() for phrase in stop_phrases):\n",
    "                    continue  \n",
    "                content.append(text)\n",
    "\n",
    "        if not content:\n",
    "            print(f\"No content found for {url}\", file=sys.stderr)\n",
    "            return f\"URL: {url}\\nTitle: {title}\\n\\nNo relevant content found.\\n\\n{'-'*80}\\n\\n\"\n",
    "\n",
    "        formatted_paragraphs = []\n",
    "        current_paragraph = ''\n",
    "        for line in content:\n",
    "            if line.endswith(':'):\n",
    "                if current_paragraph:\n",
    "                    formatted_paragraphs.append(current_paragraph.strip('; '))\n",
    "                current_paragraph = line.rstrip(':') + ': '\n",
    "            else:\n",
    "                current_paragraph += line + '; '\n",
    "\n",
    "        if current_paragraph:\n",
    "            formatted_paragraphs.append(current_paragraph.strip('; '))\n",
    "\n",
    "        formatted_text = '\\n\\n'.join(formatted_paragraphs)\n",
    "\n",
    "        return f\"URL: {url}\\nTitle: {title}\\n\\n{formatted_text}\\n\\n{'-'*80}\\n\\n\"\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching {url}: {e}\", file=sys.stderr)\n",
    "        return ''\n",
    "\n",
    "def append_text_to_file(text, file_path):\n",
    "    with open(file_path, 'a', encoding='utf-8') as f:\n",
    "        f.write(text)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    links = [\n",
    "        \"https://www.picklesburgh.com/\",\n",
    "        \"https://www.picklesburgh.com/vendors/\",\n",
    "        \"https://www.picklesburgh.com/entertainment/\",\n",
    "        \"https://www.picklesburgh.com/games/\",\n",
    "        \"https://www.picklesburgh.com/festival-schedule/lil-gherkins-activity-area/\",\n",
    "        \"https://www.picklesburgh.com/taste-of-picklesburgh/\",\n",
    "        \"https://www.picklesburgh.com/news/\",\n",
    "        \"https://www.picklesburgh.com/accessibility/\",\n",
    "        \"https://www.picklesburgh.com/visit/getting-here/\"\n",
    "    ]\n",
    "    output_file = '../raw_documents/Music_Culture/7_Picklesburgh.txt'\n",
    "\n",
    "    for url in links:\n",
    "        text_content = extract_text_from_url(url)\n",
    "        append_text_to_file(text_content, output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pittsburgh Taco Fest\n",
    "### https://www.pghtacofest.com/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching: https://www.pghtacofest.com/\n",
      "Fetching: https://www.pghtacofest.com/about\n",
      "Fetching: https://www.pghtacofest.com/vendors\n",
      "Fetching: https://www.pghtacofest.com/faqs\n",
      "Data saved to ../raw_documents/Pittsburgh_Taco_Festival.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import os\n",
    "\n",
    "class Scraper:\n",
    "    def __init__(self):\n",
    "        self.chrome_options = Options()\n",
    "        self.chrome_options.add_argument(\"--headless\")\n",
    "        self.chrome_options.add_argument(\"--disable-gpu\")\n",
    "        self.chrome_options.add_argument(\"--no-sandbox\")\n",
    "        self.chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "        self.driver = webdriver.Chrome(options=self.chrome_options)\n",
    "        self.driver.set_page_load_timeout(10)\n",
    "\n",
    "    def get_html(self, url: str):\n",
    "        \"\"\"Get HTML.\"\"\"\n",
    "        self.driver.get(url)\n",
    "        return self.driver.page_source\n",
    "\n",
    "    def get_soup(self, url: str):\n",
    "        \"\"\"Parse and return BeautifulSoup object.\"\"\"\n",
    "        html = self.get_html(url)\n",
    "        return BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "    def close(self):\n",
    "        self.driver.quit()\n",
    "\n",
    "    def __del__(self):\n",
    "        self.close()\n",
    "\n",
    "def organize_content_by_heading(soup: BeautifulSoup):\n",
    "    \"\"\"Organize content based on headings, ensuring all relevant text is captured.\"\"\"\n",
    "    data = {}\n",
    "    current_h1, current_h2, current_h3, current_h4, current_h5, current_h6 = None, None, None, None, None, None\n",
    "\n",
    "    for tag in soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'p', 'div', 'span']):\n",
    "        if tag.name == 'h1':\n",
    "            current_h1 = tag.get_text(strip=True)\n",
    "            data[current_h1] = {'content': \"\"}\n",
    "            current_h2 = current_h3 = current_h4 = current_h5 = current_h6 = None  # Reset lower headings\n",
    "        elif tag.name == 'h2' and current_h1:\n",
    "            current_h2 = tag.get_text(strip=True)\n",
    "            data[current_h1][current_h2] = {'content': \"\"}\n",
    "            current_h3 = current_h4 = current_h5 = current_h6 = None\n",
    "        elif tag.name == 'h3' and current_h2:\n",
    "            current_h3 = tag.get_text(strip=True)\n",
    "            data[current_h1][current_h2][current_h3] = {'content': \"\"}\n",
    "            current_h4 = current_h5 = current_h6 = None\n",
    "        elif tag.name == 'h4' and current_h3:\n",
    "            current_h4 = tag.get_text(strip=True)\n",
    "            data[current_h1][current_h2][current_h3][current_h4] = {'content': \"\"}\n",
    "            current_h5 = current_h6 = None\n",
    "        elif tag.name == 'h5' and current_h4:\n",
    "            current_h5 = tag.get_text(strip=True)\n",
    "            data[current_h1][current_h2][current_h3][current_h4][current_h5] = {'content': \"\"}\n",
    "            current_h6 = None\n",
    "        elif tag.name == 'h6' and current_h5:\n",
    "            current_h6 = tag.get_text(strip=True)\n",
    "            data[current_h1][current_h2][current_h3][current_h4][current_h5][current_h6] = {'content': \"\"}\n",
    "        else:\n",
    "            content = tag.get_text(strip=True)\n",
    "            if content:\n",
    "                if current_h6:\n",
    "                    data[current_h1][current_h2][current_h3][current_h4][current_h5][current_h6]['content'] += content + \" \"\n",
    "                elif current_h5:\n",
    "                    data[current_h1][current_h2][current_h3][current_h4][current_h5]['content'] += content + \" \"\n",
    "                elif current_h4:\n",
    "                    data[current_h1][current_h2][current_h3][current_h4]['content'] += content + \" \"\n",
    "                elif current_h3:\n",
    "                    data[current_h1][current_h2][current_h3]['content'] += content + \" \"\n",
    "                elif current_h2:\n",
    "                    data[current_h1][current_h2]['content'] += content + \" \"\n",
    "                elif current_h1:\n",
    "                    data[current_h1]['content'] += content + \" \"\n",
    "\n",
    "    return data\n",
    "\n",
    "def save_to_json(data, filename):\n",
    "    \"\"\"Save data to a JSON file, ensuring no overwrite of existing content.\"\"\"\n",
    "    try:\n",
    "        if os.path.exists(filename):\n",
    "            with open(filename, 'r', encoding='utf-8') as f:\n",
    "                existing_data = json.load(f)\n",
    "            if isinstance(existing_data, list):\n",
    "                existing_data.extend(data)\n",
    "            else:\n",
    "                existing_data.update(data) \n",
    "        else:\n",
    "            existing_data = data \n",
    "\n",
    "        with open(filename, 'w', encoding='utf-8') as f:\n",
    "            json.dump(existing_data, f, ensure_ascii=False, indent=4)\n",
    "        print(f\"Data saved to {filename}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error writing to JSON file: {e}\")\n",
    "\n",
    "def scrape_links(scraper_, links):\n",
    "    \"\"\"Scrape multiple links and return organized data.\"\"\"\n",
    "    all_data = []\n",
    "    for link in links:\n",
    "        try:\n",
    "            print(f\"Fetching: {link}\")\n",
    "            soup = scraper_.get_soup(link)\n",
    "            page_data = organize_content_by_heading(soup)\n",
    "            page_data['url'] = link \n",
    "            all_data.append(page_data)\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching {link}: {e}\")\n",
    "\n",
    "    return all_data\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    scraper_ = Scraper()\n",
    "\n",
    "    links = [\n",
    "        \"https://www.pghtacofest.com/\",\n",
    "        \"https://www.pghtacofest.com/about\",\n",
    "        \"https://www.pghtacofest.com/vendors\",\n",
    "        \"https://www.pghtacofest.com/faqs\"\n",
    "       \n",
    "    ]\n",
    "\n",
    "    scraped_data = scrape_links(scraper_, links)\n",
    "    save_to_json(scraped_data, \"../raw_documents/Pittsburgh_Taco_Festival.json\")\n",
    "    scraper_.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pittsburgh Restaurant Week\n",
    "### https://pittsburghrestaurantweek.com/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching: https://pittsburghrestaurantweek.com/\n",
      "Fetching: https://pittsburghrestaurantweek.com/about/\n",
      "Fetching: https://pittsburghrestaurantweek.com/about/history/\n",
      "Fetching: https://pittsburghrestaurantweek.com/restaurants/\n",
      "Fetching: https://pittsburghrestaurantweek.com/restaurants/summer-2024-restaurants/\n",
      "Fetching: https://pittsburghrestaurantweek.com/restaurants/winter-2024-restaurants/\n",
      "Fetching: https://pittsburghrestaurantweek.com/restaurants/summer-2023-restaurants/\n",
      "Fetching: https://pittsburghrestaurantweek.com/restaurants/winter-2023-restaurants/\n",
      "Fetching: https://pittsburghrestaurantweek.com/restaurants/summer-2022-restaurants/\n",
      "Fetching: https://pittsburghrestaurantweek.com/restaurants/winter-2022-restaurants/\n",
      "Fetching: https://pittsburghrestaurantweek.com/restaurants/summer-2021-restaurants/\n",
      "Fetching: https://pittsburghrestaurantweek.com/restaurants/winter-2021-restaurants/\n",
      "Fetching: https://pittsburghrestaurantweek.com/restaurants/summer-2020-restaurants/\n",
      "Fetching: https://pittsburghrestaurantweek.com/restaurants/winter-2020-restaurants/\n",
      "Fetching: https://pittsburghrestaurantweek.com/restaurants/summer-2019-restaurants/\n",
      "Fetching: https://pittsburghrestaurantweek.com/restaurants/winter-2019-restaurants/\n",
      "Fetching: https://pittsburghrestaurantweek.com/restaurants/summer-2018-restaurants/\n",
      "Fetching: https://pittsburghrestaurantweek.com/restaurants/winter-2018-restaurants/\n",
      "Fetching: https://pittsburghrestaurantweek.com/restaurants/summer-2017-restaurants/\n",
      "Fetching: https://pittsburghrestaurantweek.com/restaurants/winter-2017-restaurants/\n",
      "Fetching: https://pittsburghrestaurantweek.com/restaurants/summer-2016-restaurants/\n",
      "Fetching: https://pittsburghrestaurantweek.com/restaurants/winter-2016-restaurants/\n",
      "Fetching: https://pittsburghrestaurantweek.com/restaurants/summer-2015-restaurants/\n",
      "Fetching: https://pittsburghrestaurantweek.com/restaurants/winter-2015-restaurants/\n",
      "Fetching: https://pittsburghrestaurantweek.com/restaurants/summer-2014-restaurants/\n",
      "Fetching: https://pittsburghrestaurantweek.com/restaurants/winter-2014-restaurants/\n",
      "Fetching: https://pittsburghrestaurantweek.com/restaurants/summer-2013-restaurants/\n",
      "Fetching: https://pittsburghrestaurantweek.com/restaurants/winter-2013-restaurants/\n",
      "Fetching: https://pittsburghrestaurantweek.com/restaurants/summer-2012-restaurants/\n",
      "Fetching: https://pittsburghrestaurantweek.com/restaurants/winter-2012-restaurants/\n",
      "Data saved to ../raw_documents/Pittsburgh_Restaurant_Week.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import os\n",
    "\n",
    "class Scraper:\n",
    "    def __init__(self):\n",
    "        self.chrome_options = Options()\n",
    "        self.chrome_options.add_argument(\"--headless\")\n",
    "        self.chrome_options.add_argument(\"--disable-gpu\")\n",
    "        self.chrome_options.add_argument(\"--no-sandbox\")\n",
    "        self.chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "        self.driver = webdriver.Chrome(options=self.chrome_options)\n",
    "        self.driver.set_page_load_timeout(10)\n",
    "\n",
    "    def get_html(self, url: str):\n",
    "        \"\"\"Get HTML.\"\"\"\n",
    "        self.driver.get(url)\n",
    "        return self.driver.page_source\n",
    "\n",
    "    def get_soup(self, url: str):\n",
    "        html = self.get_html(url)\n",
    "        return BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "    def close(self):\n",
    "        self.driver.quit()\n",
    "\n",
    "    def __del__(self):\n",
    "        self.close()\n",
    "\n",
    "def organize_content_by_heading(soup: BeautifulSoup):\n",
    "    data = {}\n",
    "    current_h1, current_h2, current_h3, current_h4, current_h5, current_h6 = None, None, None, None, None, None\n",
    "\n",
    "    for tag in soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'p', 'div', 'span']):\n",
    "        if tag.name == 'h1':\n",
    "            current_h1 = tag.get_text(strip=True)\n",
    "            data[current_h1] = {'content': \"\"}\n",
    "            current_h2 = current_h3 = current_h4 = current_h5 = current_h6 = None\n",
    "        elif tag.name == 'h2' and current_h1:\n",
    "            current_h2 = tag.get_text(strip=True)\n",
    "            data[current_h1][current_h2] = {'content': \"\"}\n",
    "            current_h3 = current_h4 = current_h5 = current_h6 = None\n",
    "        elif tag.name == 'h3' and current_h2:\n",
    "            current_h3 = tag.get_text(strip=True)\n",
    "            data[current_h1][current_h2][current_h3] = {'content': \"\"}\n",
    "            current_h4 = current_h5 = current_h6 = None\n",
    "        elif tag.name == 'h4' and current_h3:\n",
    "            current_h4 = tag.get_text(strip=True)\n",
    "            data[current_h1][current_h2][current_h3][current_h4] = {'content': \"\"}\n",
    "            current_h5 = current_h6 = None\n",
    "        elif tag.name == 'h5' and current_h4:\n",
    "            current_h5 = tag.get_text(strip=True)\n",
    "            data[current_h1][current_h2][current_h3][current_h4][current_h5] = {'content': \"\"}\n",
    "            current_h6 = None\n",
    "        elif tag.name == 'h6' and current_h5:\n",
    "            current_h6 = tag.get_text(strip=True)\n",
    "            data[current_h1][current_h2][current_h3][current_h4][current_h5][current_h6] = {'content': \"\"}\n",
    "        else:\n",
    "            content = tag.get_text(strip=True)\n",
    "            if content:\n",
    "                if current_h6:\n",
    "                    data[current_h1][current_h2][current_h3][current_h4][current_h5][current_h6]['content'] += content + \" \"\n",
    "                elif current_h5:\n",
    "                    data[current_h1][current_h2][current_h3][current_h4][current_h5]['content'] += content + \" \"\n",
    "                elif current_h4:\n",
    "                    data[current_h1][current_h2][current_h3][current_h4]['content'] += content + \" \"\n",
    "                elif current_h3:\n",
    "                    data[current_h1][current_h2][current_h3]['content'] += content + \" \"\n",
    "                elif current_h2:\n",
    "                    data[current_h1][current_h2]['content'] += content + \" \"\n",
    "                elif current_h1:\n",
    "                    data[current_h1]['content'] += content + \" \"\n",
    "\n",
    "    return data\n",
    "\n",
    "def save_to_json(data, filename):\n",
    "    \"\"\"Save data to a JSON file, ensuring no overwrite of existing content.\"\"\"\n",
    "    try:\n",
    "        if os.path.exists(filename):\n",
    "            with open(filename, 'r', encoding='utf-8') as f:\n",
    "                existing_data = json.load(f)\n",
    "            if isinstance(existing_data, list):\n",
    "                existing_data.extend(data)\n",
    "            else:\n",
    "                existing_data.update(data)  \n",
    "        else:\n",
    "            existing_data = data \n",
    "\n",
    "        with open(filename, 'w', encoding='utf-8') as f:\n",
    "            json.dump(existing_data, f, ensure_ascii=False, indent=4)\n",
    "        print(f\"Data saved to {filename}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error writing to JSON file: {e}\")\n",
    "\n",
    "def scrape_links(scraper_, links):\n",
    "    all_data = []\n",
    "    for link in links:\n",
    "        try:\n",
    "            print(f\"Fetching: {link}\")\n",
    "            soup = scraper_.get_soup(link)\n",
    "            page_data = organize_content_by_heading(soup)\n",
    "            page_data['url'] = link \n",
    "            all_data.append(page_data)\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching {link}: {e}\")\n",
    "\n",
    "    return all_data\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    scraper_ = Scraper()\n",
    "\n",
    "    links = [\n",
    "        \"https://pittsburghrestaurantweek.com/\",\n",
    "        \"https://pittsburghrestaurantweek.com/about/\",\n",
    "        \"https://pittsburghrestaurantweek.com/about/history/\",\n",
    "        \"https://pittsburghrestaurantweek.com/restaurants/\",\n",
    "        \"https://pittsburghrestaurantweek.com/restaurants/summer-2024-restaurants/\",\n",
    "        \"https://pittsburghrestaurantweek.com/restaurants/winter-2024-restaurants/\",\n",
    "        \"https://pittsburghrestaurantweek.com/restaurants/summer-2023-restaurants/\",\n",
    "        \"https://pittsburghrestaurantweek.com/restaurants/winter-2023-restaurants/\",\n",
    "        \"https://pittsburghrestaurantweek.com/restaurants/summer-2022-restaurants/\",\n",
    "        \"https://pittsburghrestaurantweek.com/restaurants/winter-2022-restaurants/\",\n",
    "        \"https://pittsburghrestaurantweek.com/restaurants/summer-2021-restaurants/\",\n",
    "        \"https://pittsburghrestaurantweek.com/restaurants/winter-2021-restaurants/\",\n",
    "        \"https://pittsburghrestaurantweek.com/restaurants/summer-2020-restaurants/\",\n",
    "        \"https://pittsburghrestaurantweek.com/restaurants/winter-2020-restaurants/\",\n",
    "        \"https://pittsburghrestaurantweek.com/restaurants/summer-2019-restaurants/\",\n",
    "        \"https://pittsburghrestaurantweek.com/restaurants/winter-2019-restaurants/\",\n",
    "        \"https://pittsburghrestaurantweek.com/restaurants/summer-2018-restaurants/\",\n",
    "        \"https://pittsburghrestaurantweek.com/restaurants/winter-2018-restaurants/\",\n",
    "        \"https://pittsburghrestaurantweek.com/restaurants/summer-2017-restaurants/\",\n",
    "        \"https://pittsburghrestaurantweek.com/restaurants/winter-2017-restaurants/\",\n",
    "        \"https://pittsburghrestaurantweek.com/restaurants/summer-2016-restaurants/\",\n",
    "        \"https://pittsburghrestaurantweek.com/restaurants/winter-2016-restaurants/\",\n",
    "        \"https://pittsburghrestaurantweek.com/restaurants/summer-2015-restaurants/\",\n",
    "        \"https://pittsburghrestaurantweek.com/restaurants/winter-2015-restaurants/\",\n",
    "        \"https://pittsburghrestaurantweek.com/restaurants/summer-2014-restaurants/\",\n",
    "        \"https://pittsburghrestaurantweek.com/restaurants/winter-2014-restaurants/\",\n",
    "        \"https://pittsburghrestaurantweek.com/restaurants/summer-2013-restaurants/\",\n",
    "        \"https://pittsburghrestaurantweek.com/restaurants/winter-2013-restaurants/\",\n",
    "        \"https://pittsburghrestaurantweek.com/restaurants/summer-2012-restaurants/\",\n",
    "        \"https://pittsburghrestaurantweek.com/restaurants/winter-2012-restaurants/\"\n",
    "       \n",
    "    ]\n",
    "\n",
    "    scraped_data = scrape_links(scraper_, links)\n",
    "    save_to_json(scraped_data, \"../raw_documents/Pittsburgh_Restaurant_Week.json\")\n",
    "    scraper_.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Little Italy Days\n",
    "### https://littleitalydays.com/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching: https://littleitalydays.com/\n",
      "Fetching: https://littleitalydays.com/entertainment-schedule/\n",
      "Fetching: https://littleitalydays.com/getting-around/\n",
      "Fetching: https://littleitalydays.com/about-us/\n",
      "Fetching: https://littleitalydays.com/faq/\n",
      "Fetching: https://littleitalydays.com/bloomfield-businesses/\n",
      "Data saved to ../raw_documents/Pittsburgh_Little_Italy_Day.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import os\n",
    "\n",
    "class Scraper:\n",
    "    def __init__(self):\n",
    "        self.chrome_options = Options()\n",
    "        self.chrome_options.add_argument(\"--headless\")\n",
    "        self.chrome_options.add_argument(\"--disable-gpu\")\n",
    "        self.chrome_options.add_argument(\"--no-sandbox\")\n",
    "        self.chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "        self.driver = webdriver.Chrome(options=self.chrome_options)\n",
    "        self.driver.set_page_load_timeout(10)\n",
    "\n",
    "    def get_html(self, url: str):\n",
    "        \"\"\"Get HTML.\"\"\"\n",
    "        self.driver.get(url)\n",
    "        return self.driver.page_source\n",
    "\n",
    "    def get_soup(self, url: str):\n",
    "        \"\"\"Parse and return BeautifulSoup object.\"\"\"\n",
    "        html = self.get_html(url)\n",
    "        return BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "    def close(self):\n",
    "        self.driver.quit()\n",
    "\n",
    "    def __del__(self):\n",
    "        self.close()\n",
    "\n",
    "def organize_content_by_heading(soup: BeautifulSoup):\n",
    "    data = {}\n",
    "    current_h1, current_h2, current_h3, current_h4, current_h5, current_h6 = None, None, None, None, None, None\n",
    "\n",
    "    for tag in soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'p', 'div', 'span']):\n",
    "        if tag.name == 'h1':\n",
    "            current_h1 = tag.get_text(strip=True)\n",
    "            data[current_h1] = {'content': \"\"}\n",
    "            current_h2 = current_h3 = current_h4 = current_h5 = current_h6 = None  # Reset lower headings\n",
    "        elif tag.name == 'h2' and current_h1:\n",
    "            current_h2 = tag.get_text(strip=True)\n",
    "            data[current_h1][current_h2] = {'content': \"\"}\n",
    "            current_h3 = current_h4 = current_h5 = current_h6 = None\n",
    "        elif tag.name == 'h3' and current_h2:\n",
    "            current_h3 = tag.get_text(strip=True)\n",
    "            data[current_h1][current_h2][current_h3] = {'content': \"\"}\n",
    "            current_h4 = current_h5 = current_h6 = None\n",
    "        elif tag.name == 'h4' and current_h3:\n",
    "            current_h4 = tag.get_text(strip=True)\n",
    "            data[current_h1][current_h2][current_h3][current_h4] = {'content': \"\"}\n",
    "            current_h5 = current_h6 = None\n",
    "        elif tag.name == 'h5' and current_h4:\n",
    "            current_h5 = tag.get_text(strip=True)\n",
    "            data[current_h1][current_h2][current_h3][current_h4][current_h5] = {'content': \"\"}\n",
    "            current_h6 = None\n",
    "        elif tag.name == 'h6' and current_h5:\n",
    "            current_h6 = tag.get_text(strip=True)\n",
    "            data[current_h1][current_h2][current_h3][current_h4][current_h5][current_h6] = {'content': \"\"}\n",
    "        else:\n",
    "            content = tag.get_text(strip=True)\n",
    "            if content:\n",
    "                if current_h6:\n",
    "                    data[current_h1][current_h2][current_h3][current_h4][current_h5][current_h6]['content'] += content + \" \"\n",
    "                elif current_h5:\n",
    "                    data[current_h1][current_h2][current_h3][current_h4][current_h5]['content'] += content + \" \"\n",
    "                elif current_h4:\n",
    "                    data[current_h1][current_h2][current_h3][current_h4]['content'] += content + \" \"\n",
    "                elif current_h3:\n",
    "                    data[current_h1][current_h2][current_h3]['content'] += content + \" \"\n",
    "                elif current_h2:\n",
    "                    data[current_h1][current_h2]['content'] += content + \" \"\n",
    "                elif current_h1:\n",
    "                    data[current_h1]['content'] += content + \" \"\n",
    "\n",
    "    return data\n",
    "\n",
    "def save_to_json(data, filename):\n",
    "    try:\n",
    "        if os.path.exists(filename):\n",
    "            with open(filename, 'r', encoding='utf-8') as f:\n",
    "                existing_data = json.load(f)\n",
    "            if isinstance(existing_data, list):\n",
    "                existing_data.extend(data) \n",
    "            else:\n",
    "                existing_data.update(data)\n",
    "        else:\n",
    "            existing_data = data \n",
    "\n",
    "        with open(filename, 'w', encoding='utf-8') as f:\n",
    "            json.dump(existing_data, f, ensure_ascii=False, indent=4)\n",
    "        print(f\"Data saved to {filename}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error writing to JSON file: {e}\")\n",
    "\n",
    "def scrape_links(scraper_, links):\n",
    "    \"\"\"Scrape multiple links and return organized data.\"\"\"\n",
    "    all_data = []\n",
    "    for link in links:\n",
    "        try:\n",
    "            print(f\"Fetching: {link}\")\n",
    "            soup = scraper_.get_soup(link)\n",
    "            page_data = organize_content_by_heading(soup)\n",
    "            page_data['url'] = link  \n",
    "            all_data.append(page_data)\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching {link}: {e}\")\n",
    "\n",
    "    return all_data\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    scraper_ = Scraper()\n",
    "    links = [\n",
    "        \"https://littleitalydays.com/\",\n",
    "        \"https://littleitalydays.com/entertainment-schedule/\",\n",
    "        \"https://littleitalydays.com/getting-around/\",\n",
    "        \"https://littleitalydays.com/about-us/\",\n",
    "        \"https://littleitalydays.com/faq/\",\n",
    "        \"https://littleitalydays.com/bloomfield-businesses/\"\n",
    "    ]\n",
    "\n",
    "    scraped_data = scrape_links(scraper_, links)\n",
    "    save_to_json(scraped_data, \"../raw_documents/Pittsburgh_Little_Italy_Day.json\")\n",
    "    scraper_.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error fetching https://littleitalydays.com/: 403 Client Error: Forbidden for url: https://littleitalydays.com/\n",
      "Error fetching https://littleitalydays.com/entertainment-schedule/: 403 Client Error: Forbidden for url: https://littleitalydays.com/entertainment-schedule/\n",
      "Error fetching https://littleitalydays.com/getting-around/: 403 Client Error: Forbidden for url: https://littleitalydays.com/getting-around/\n",
      "Error fetching https://littleitalydays.com/about-us/: 403 Client Error: Forbidden for url: https://littleitalydays.com/about-us/\n",
      "Error fetching https://littleitalydays.com/faq/: 403 Client Error: Forbidden for url: https://littleitalydays.com/faq/\n",
      "Error fetching https://littleitalydays.com/bloomfield-businesses/: 403 Client Error: Forbidden for url: https://littleitalydays.com/bloomfield-businesses/\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import sys\n",
    "import re\n",
    "\n",
    "def extract_text_from_url(url):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()  \n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        title = soup.title.string.strip() if soup.title else 'No Title'\n",
    "\n",
    "        for tag in soup([\"script\", \"style\", \"header\", \"footer\", \"nav\", \"aside\"]):\n",
    "            tag.decompose()\n",
    "        for tag in soup.find_all(class_=re.compile(\".*(nav|footer|menu|social|sidebar).*\", re.I)):\n",
    "            tag.decompose()\n",
    "        for tag in soup.find_all(id=re.compile(\".*(nav|footer|menu|social|sidebar).*\", re.I)):\n",
    "            tag.decompose()\n",
    "\n",
    "        stop_phrases = [\n",
    "            \"Follow us on\", \"Contact Us\", \"Privacy Policy\", \"©\",\n",
    "            \"BUY TICKETS\", \"GIVE NOW\", \"SUBSCRIBE\", \"Site Map\",\n",
    "            \"Board Of Directors Login\"\n",
    "        ]\n",
    "\n",
    "        content = []\n",
    "        for element in soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'p', 'li']):\n",
    "            text = ' '.join(element.stripped_strings)\n",
    "            if text:\n",
    "                if any(phrase.lower() in text.lower() for phrase in stop_phrases):\n",
    "                    continue  \n",
    "                content.append(text)\n",
    "        if not content:\n",
    "            print(f\"No content found for {url}\", file=sys.stderr)\n",
    "            return f\"URL: {url}\\nTitle: {title}\\n\\nNo relevant content found.\\n\\n{'-'*80}\\n\\n\"\n",
    "\n",
    "        formatted_paragraphs = []\n",
    "        current_paragraph = ''\n",
    "        for line in content:\n",
    "            if line.endswith(':'):\n",
    "                if current_paragraph:\n",
    "                    formatted_paragraphs.append(current_paragraph.strip('; '))\n",
    "                current_paragraph = line.rstrip(':') + ': '\n",
    "            else:\n",
    "                current_paragraph += line + '; '\n",
    "\n",
    "        if current_paragraph:\n",
    "            formatted_paragraphs.append(current_paragraph.strip('; '))\n",
    "        formatted_text = '\\n\\n'.join(formatted_paragraphs)\n",
    "\n",
    "        return f\"URL: {url}\\nTitle: {title}\\n\\n{formatted_text}\\n\\n{'-'*80}\\n\\n\"\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching {url}: {e}\", file=sys.stderr)\n",
    "        return ''\n",
    "\n",
    "def append_text_to_file(text, file_path):\n",
    "    with open(file_path, 'a', encoding='utf-8') as f:\n",
    "        f.write(text)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    links = [\n",
    "        \"https://littleitalydays.com/\",\n",
    "        \"https://littleitalydays.com/entertainment-schedule/\",\n",
    "        \"https://littleitalydays.com/getting-around/\",\n",
    "        \"https://littleitalydays.com/about-us/\",\n",
    "        \"https://littleitalydays.com/faq/\",\n",
    "        \"https://littleitalydays.com/bloomfield-businesses/\"\n",
    "    ]\n",
    "    output_file = '../raw_documents/Music_Culture/11_Little_Italy_Day.txt'\n",
    "\n",
    "    for url in links:\n",
    "        text_content = extract_text_from_url(url)\n",
    "        append_text_to_file(text_content, output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Banana Split Fest\n",
    "### https://bananasplitfest.com/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching: https://bananasplitfest.com/\n",
      "Fetching: https://bananasplitfest.com/activities/\n",
      "Fetching: https://bananasplitfest.com/events/princess-pageant/\n",
      "Fetching: https://bananasplitfest.com/activities/crafts-games-activities/\n",
      "Fetching: https://bananasplitfest.com/activities/participating-vendors/\n",
      "Fetching: https://bananasplitfest.com/activities/food/\n",
      "Fetching: https://bananasplitfest.com/activities/over-21-area/\n",
      "Fetching: https://bananasplitfest.com/events/\n",
      "Fetching: https://bananasplitfest.com/events/5k-banana-run/\n",
      "Fetching: https://bananasplitfest.com/events/great-american-banana-baking-contest/\n",
      "Fetching: https://bananasplitfest.com/events/banana-challenge/\n",
      "Fetching: https://bananasplitfest.com/events/blood-drive/\n",
      "Fetching: https://bananasplitfest.com/events/cornhole-tournament/\n",
      "Fetching: https://bananasplitfest.com/events/car-show/\n",
      "Fetching: https://bananasplitfest.com/events/yellow-tie-gala/\n",
      "Fetching: https://bananasplitfest.com/activities/entertainment/\n",
      "Fetching: https://bananasplitfest.com/schedule/\n",
      "Fetching: https://bananasplitfest.com/information/parking/\n",
      "Fetching: https://bananasplitfest.com/information/plan-your-visit/\n",
      "Fetching: https://bananasplitfest.com/history/\n",
      "Fetching: https://bananasplitfest.com/information/media/\n",
      "Data saved to ../raw_documents/Pittsburgh_Banana_Split_Festival.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import os\n",
    "\n",
    "class Scraper:\n",
    "    def __init__(self):\n",
    "        self.chrome_options = Options()\n",
    "        self.chrome_options.add_argument(\"--headless\")\n",
    "        self.chrome_options.add_argument(\"--disable-gpu\")\n",
    "        self.chrome_options.add_argument(\"--no-sandbox\")\n",
    "        self.chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "        self.driver = webdriver.Chrome(options=self.chrome_options)\n",
    "        self.driver.set_page_load_timeout(10)\n",
    "\n",
    "    def get_html(self, url: str):\n",
    "        \"\"\"Get HTML.\"\"\"\n",
    "        self.driver.get(url)\n",
    "        return self.driver.page_source\n",
    "\n",
    "    def get_soup(self, url: str):\n",
    "        \"\"\"Parse and return BeautifulSoup object.\"\"\"\n",
    "        html = self.get_html(url)\n",
    "        return BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "    def close(self):\n",
    "        self.driver.quit()\n",
    "\n",
    "    def __del__(self):\n",
    "        self.close()\n",
    "\n",
    "def organize_content_by_heading(soup: BeautifulSoup):\n",
    "    \"\"\"Organize content based on headings, ensuring all relevant text is captured.\"\"\"\n",
    "    data = {}\n",
    "    current_h1, current_h2, current_h3, current_h4, current_h5, current_h6 = None, None, None, None, None, None\n",
    "\n",
    "    for tag in soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'p', 'div', 'span']):\n",
    "        if tag.name == 'h1':\n",
    "            current_h1 = tag.get_text(strip=True)\n",
    "            data[current_h1] = {'content': \"\"}\n",
    "            current_h2 = current_h3 = current_h4 = current_h5 = current_h6 = None  # Reset lower headings\n",
    "        elif tag.name == 'h2' and current_h1:\n",
    "            current_h2 = tag.get_text(strip=True)\n",
    "            data[current_h1][current_h2] = {'content': \"\"}\n",
    "            current_h3 = current_h4 = current_h5 = current_h6 = None\n",
    "        elif tag.name == 'h3' and current_h2:\n",
    "            current_h3 = tag.get_text(strip=True)\n",
    "            data[current_h1][current_h2][current_h3] = {'content': \"\"}\n",
    "            current_h4 = current_h5 = current_h6 = None\n",
    "        elif tag.name == 'h4' and current_h3:\n",
    "            current_h4 = tag.get_text(strip=True)\n",
    "            data[current_h1][current_h2][current_h3][current_h4] = {'content': \"\"}\n",
    "            current_h5 = current_h6 = None\n",
    "        elif tag.name == 'h5' and current_h4:\n",
    "            current_h5 = tag.get_text(strip=True)\n",
    "            data[current_h1][current_h2][current_h3][current_h4][current_h5] = {'content': \"\"}\n",
    "            current_h6 = None\n",
    "        elif tag.name == 'h6' and current_h5:\n",
    "            current_h6 = tag.get_text(strip=True)\n",
    "            data[current_h1][current_h2][current_h3][current_h4][current_h5][current_h6] = {'content': \"\"}\n",
    "        else:\n",
    "            content = tag.get_text(strip=True)\n",
    "            if content:\n",
    "                if current_h6:\n",
    "                    data[current_h1][current_h2][current_h3][current_h4][current_h5][current_h6]['content'] += content + \" \"\n",
    "                elif current_h5:\n",
    "                    data[current_h1][current_h2][current_h3][current_h4][current_h5]['content'] += content + \" \"\n",
    "                elif current_h4:\n",
    "                    data[current_h1][current_h2][current_h3][current_h4]['content'] += content + \" \"\n",
    "                elif current_h3:\n",
    "                    data[current_h1][current_h2][current_h3]['content'] += content + \" \"\n",
    "                elif current_h2:\n",
    "                    data[current_h1][current_h2]['content'] += content + \" \"\n",
    "                elif current_h1:\n",
    "                    data[current_h1]['content'] += content + \" \"\n",
    "\n",
    "    return data\n",
    "\n",
    "def save_to_json(data, filename):\n",
    "    \"\"\"Save data to a JSON file, ensuring no overwrite of existing content.\"\"\"\n",
    "    try:\n",
    "        if os.path.exists(filename):\n",
    "            with open(filename, 'r', encoding='utf-8') as f:\n",
    "                existing_data = json.load(f)\n",
    "            if isinstance(existing_data, list):\n",
    "                existing_data.extend(data)  \n",
    "            else:\n",
    "                existing_data.update(data)  \n",
    "        else:\n",
    "            existing_data = data \n",
    "\n",
    "        with open(filename, 'w', encoding='utf-8') as f:\n",
    "            json.dump(existing_data, f, ensure_ascii=False, indent=4)\n",
    "        print(f\"Data saved to {filename}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error writing to JSON file: {e}\")\n",
    "\n",
    "def scrape_links(scraper_, links):\n",
    "    \"\"\"Scrape multiple links and return organized data.\"\"\"\n",
    "    all_data = []\n",
    "    for link in links:\n",
    "        try:\n",
    "            print(f\"Fetching: {link}\")\n",
    "            soup = scraper_.get_soup(link)\n",
    "            page_data = organize_content_by_heading(soup)\n",
    "            page_data['url'] = link \n",
    "            all_data.append(page_data)\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching {link}: {e}\")\n",
    "\n",
    "    return all_data\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    scraper_ = Scraper()\n",
    "\n",
    "    links = [\n",
    "        \"https://bananasplitfest.com/\",\n",
    "        \"https://bananasplitfest.com/activities/\",\n",
    "        \"https://bananasplitfest.com/events/princess-pageant/\",\n",
    "        \"https://bananasplitfest.com/activities/crafts-games-activities/\",\n",
    "        \"https://bananasplitfest.com/activities/participating-vendors/\",\n",
    "        \"https://bananasplitfest.com/activities/food/\",\n",
    "        \"https://bananasplitfest.com/activities/over-21-area/\",\n",
    "        \"https://bananasplitfest.com/events/\",\n",
    "        \"https://bananasplitfest.com/events/5k-banana-run/\",\n",
    "        \"https://bananasplitfest.com/events/great-american-banana-baking-contest/\",\n",
    "        \"https://bananasplitfest.com/events/banana-challenge/\",\n",
    "        \"https://bananasplitfest.com/events/blood-drive/\",\n",
    "        \"https://bananasplitfest.com/events/cornhole-tournament/\",\n",
    "        \"https://bananasplitfest.com/events/car-show/\",\n",
    "        \"https://bananasplitfest.com/events/yellow-tie-gala/\",\n",
    "        \"https://bananasplitfest.com/activities/entertainment/\",\n",
    "        \"https://bananasplitfest.com/schedule/\",\n",
    "        \"https://bananasplitfest.com/information/parking/\",\n",
    "        \"https://bananasplitfest.com/information/plan-your-visit/\",\n",
    "        \"https://bananasplitfest.com/history/\",\n",
    "        \"https://bananasplitfest.com/information/media/\"\n",
    "    ]\n",
    "\n",
    "    scraped_data = scrape_links(scraper_, links)\n",
    "    save_to_json(scraped_data, \"../raw_documents/Pittsburgh_Banana_Split_Festival.json\")\n",
    "    scraper_.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag_pittsburgh_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
