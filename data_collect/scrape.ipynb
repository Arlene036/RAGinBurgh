{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_file_name(url):\n",
    "    return '-'.join(url.split(\"/\")[2:]).replace('.','-') + \".json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "\n",
    "def scrape_wikipedia_to_json(url, output_file):\n",
    "    # Send request to the URL\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    # Parse the page content\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    # Find the correct div based on the updated structure\n",
    "    content = soup.find('div', {'id': 'mw-content-text'})\n",
    "\n",
    "    # Dictionary to store the result\n",
    "    data = {}\n",
    "    current_h1 = None\n",
    "    current_h2 = None\n",
    "    current_h3 = None\n",
    "    current_h4 = None\n",
    "    current_h5 = None\n",
    "    current_h6 = None\n",
    "\n",
    "    # Iterate over headings and paragraphs\n",
    "    for tag in content.find_all(['h2', 'h3', 'h4', 'h5', 'h6', 'p']):\n",
    "        if tag.name == 'h2':\n",
    "            current_h2 = tag.get_text(strip=True)\n",
    "            data[current_h2] = {}\n",
    "            current_h3 = current_h4 = current_h5 = current_h6 = None\n",
    "        elif tag.name == 'h3' and current_h2:\n",
    "            current_h3 = tag.get_text(strip=True)\n",
    "            data[current_h2][current_h3] = {}\n",
    "            current_h4 = current_h5 = current_h6 = None\n",
    "        elif tag.name == 'h4' and current_h3:\n",
    "            current_h4 = tag.get_text(strip=True)\n",
    "            data[current_h2][current_h3][current_h4] = {}\n",
    "            current_h5 = current_h6 = None\n",
    "        elif tag.name == 'h5' and current_h4:\n",
    "            current_h5 = tag.get_text(strip=True)\n",
    "            data[current_h2][current_h3][current_h4][current_h5] = {}\n",
    "            current_h6 = None\n",
    "        elif tag.name == 'h6' and current_h5:\n",
    "            current_h6 = tag.get_text(strip=True)\n",
    "            data[current_h2][current_h3][current_h4][current_h5][current_h6] = {}\n",
    "        elif tag.name == 'p':\n",
    "            if current_h6:\n",
    "                data[current_h2][current_h3][current_h4][current_h5][current_h6].setdefault('content', []).append(tag.get_text(strip=True))\n",
    "            elif current_h5:\n",
    "                data[current_h2][current_h3][current_h4][current_h5].setdefault('content', []).append(tag.get_text(strip=True))\n",
    "            elif current_h4:\n",
    "                data[current_h2][current_h3][current_h4].setdefault('content', []).append(tag.get_text(strip=True))\n",
    "            elif current_h3:\n",
    "                data[current_h2][current_h3].setdefault('content', []).append(tag.get_text(strip=True))\n",
    "            elif current_h2:\n",
    "                data[current_h2].setdefault('content', []).append(tag.get_text(strip=True))\n",
    "    \n",
    "    # Save the dictionary as a JSON file\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=4)\n",
    "    \n",
    "    print(f\"Content successfully written to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Content successfully written to Pittsburgh_Wikipedia.json\n",
      "Content successfully written to History_of_Pittsburgh_Wikipedia.json\n",
      "Content successfully written to Carnegie_Mellon_University_Wikipedia.json\n"
     ]
    }
   ],
   "source": [
    "url_list = [\"https://en.wikipedia.org/wiki/Pittsburgh\", \"https://en.wikipedia.org/wiki/History_of_Pittsburgh\",\n",
    "\"https://en.wikipedia.org/wiki/Carnegie_Mellon_University\"]\n",
    "output_file_list = [\"Pittsburgh_Wikipedia.json\", \"History_of_Pittsburgh_Wikipedia.json\", \"Carnegie_Mellon_University_Wikipedia.json\"]\n",
    "for url, output_file in zip(url_list, output_file_list):\n",
    "    scrape_wikipedia_to_json(url, output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ---- scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bs4.element.Tag"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = soup.find('div', {'id': 'bodyContent'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tag in content.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'p']):\n",
    "    print(tag.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to store the result\n",
    "data = {}\n",
    "current_h1 = None\n",
    "current_h2 = None\n",
    "current_h3 = None\n",
    "current_h4 = None\n",
    "current_h5 = None\n",
    "current_h6 = None\n",
    "\n",
    "# Iterate over headings and paragraphs\n",
    "for tag in content.find_all(['h2', 'h3', 'h4', 'h5', 'h6', 'p']):\n",
    "    if tag.name == 'h2':\n",
    "        current_h2 = tag.get_text(strip=True)\n",
    "        data[current_h2] = {}\n",
    "        current_h3 = current_h4 = current_h5 = current_h6 = None\n",
    "    elif tag.name == 'h3' and current_h2:\n",
    "        current_h3 = tag.get_text(strip=True)\n",
    "        data[current_h2][current_h3] = {}\n",
    "        current_h4 = current_h5 = current_h6 = None\n",
    "    elif tag.name == 'h4' and current_h3:\n",
    "        current_h4 = tag.get_text(strip=True)\n",
    "        data[current_h2][current_h3][current_h4] = {}\n",
    "        current_h5 = current_h6 = None\n",
    "    elif tag.name == 'h5' and current_h4:\n",
    "        current_h5 = tag.get_text(strip=True)\n",
    "        data[current_h2][current_h3][current_h4][current_h5] = {}\n",
    "        current_h6 = None\n",
    "    elif tag.name == 'h6' and current_h5:\n",
    "        current_h6 = tag.get_text(strip=True)\n",
    "        data[current_h2][current_h3][current_h4][current_h5][current_h6] = {}\n",
    "    elif tag.name == 'p':\n",
    "        if current_h6:\n",
    "            data[current_h2][current_h3][current_h4][current_h5][current_h6].setdefault('content', []).append(tag.get_text(strip=True))\n",
    "        elif current_h5:\n",
    "            data[current_h2][current_h3][current_h4][current_h5].setdefault('content', []).append(tag.get_text(strip=True))\n",
    "        elif current_h4:\n",
    "            data[current_h2][current_h3][current_h4].setdefault('content', []).append(tag.get_text(strip=True))\n",
    "        elif current_h3:\n",
    "            data[current_h2][current_h3].setdefault('content', []).append(tag.get_text(strip=True))\n",
    "        elif current_h2:\n",
    "            data[current_h2].setdefault('content', []).append(tag.get_text(strip=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pittsburghpa.gov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def get_subpage_links(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    # Find all <a> tags\n",
    "    links = []\n",
    "    for a_tag in soup.find_all('a', href=True):\n",
    "        link = a_tag['href']\n",
    "        if link.startswith('/'):  # If it's a relative link, prepend the base URL\n",
    "            link = f\"{url.rstrip('/')}{link}\"\n",
    "        links.append(link)\n",
    "    \n",
    "    return links\n",
    "\n",
    "# Example usage\n",
    "url = \"https://pittsburghpa.gov/index.html\"  # Replace with the base URL\n",
    "subpage_links = get_subpage_links(url)\n",
    "print(f\"Found {len(subpage_links)} subpage links:\")\n",
    "for link in subpage_links:\n",
    "    print(link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pittsburghpa-gov-pittsburgh-pgh-about.json\n",
      "pittsburghpa-gov-pittsburgh-pgh-sports.json\n",
      "pittsburghpa-gov-pittsburgh-cultural-activities.json\n",
      "pittsburghpa-gov-pittsburgh-flag-seal.json\n",
      "pittsburghpa-gov-mayor-pghmayors.json\n"
     ]
    }
   ],
   "source": [
    "# https://pittsburghpa.gov/pittsburgh/pgh-about\n",
    "gov_urls = [\"https://pittsburghpa.gov/pittsburgh/pgh-about\", \"https://pittsburghpa.gov/pittsburgh/pgh-sports\", \n",
    "\"https://pittsburghpa.gov/pittsburgh/cultural-activities\", \"https://pittsburghpa.gov/pittsburgh/flag-seal\", \n",
    "\"https://pittsburghpa.gov/mayor/pghmayors\"]\n",
    "\n",
    "\n",
    "gov_files = []\n",
    "for url in gov_urls:\n",
    "    gov_files.append(build_file_name(url))\n",
    "\n",
    "for url, output_file in zip(gov_urls, gov_files):\n",
    "    scrape_gov_to_json(url, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Content successfully written to Pittsburgh_Gov_About.json\n",
      "Content successfully written to Pittsburgh_Gov_Sports.json\n"
     ]
    }
   ],
   "source": [
    "def scrape_gov_to_json(url, output_file):\n",
    "    # Send request to the URL\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    # Parse the page content\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    # Find the correct div based on the updated structure\n",
    "    content = soup.find('div', {'class': 'col-md-12'})\n",
    "\n",
    "    # Dictionary to store the result\n",
    "    data = {}\n",
    "    for passage in content.find_all('p'):\n",
    "        data.setdefault('content', []).append(passage.get_text(strip=True))\n",
    "    \n",
    "    # Save the dictionary as a JSON file\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=4)\n",
    "    \n",
    "    print(f\"Content successfully written to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
